observer.py
pythonimport os
import logging
import time
import random
import asyncio
from enum import Enum
from typing import Any, Dict, Optional, List, Callable, AsyncGenerator
from functools import wraps
from retry import retry
import requests
import pandas as pd
import phoenix as px
from phoenix.evaluations import (
    HallucinationEvaluator,
    RelevanceEvaluator,
    QaEvaluator,
    ToxicityEvaluator,
)
from arize.utils.types import ModelTypes, Environments, Metrics
from arize.public_pb2 import Record
from arize.api import Client as ArizeClient
from arize.otel import register, Endpoints
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.resources import Resource
from openinference.instrumentation.openai import OpenAIInstrumentor
from openinference.instrumentation.anthropic import AnthropicInstrumentor
from openinference.instrumentation.vertexai import VertexAIInstrumentor
from presidio_analyzer import AnalyzerEngine
try:
    from google.adk.agents import Agent
except ImportError:
    Agent = None

# Structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
)
logger = logging.getLogger(__name__)

class LLMProvider(Enum):
    VERTEX_AI = "vertexai"
    OPENAI = "openai"
    GROK = "grok"
    ANTHROPIC = "anthropic"

class EnterpriseLLMObserver:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.__initialized = False
        return cls._instance

    def __init__(
        self,
        space_id: Optional[str] = None,
        api_key: Optional[str] = None,
        project_name: str = "enterprise-llm-observability",
        endpoint: Optional[str] = None,
        environment: str = "production",
        oauth_client_id: Optional[str] = None,
        oauth_client_secret: Optional[str] = None,
        oauth_token_url: Optional[str] = None,
        token_buffer_seconds: int = 60,
        default_sample_rate: float = 0.1,
        openai_proxy_url: Optional[str] = None,
        openai_jwt_token: Optional[str] = None,
        use_phoenix: bool = True,
    ):
        if self.__initialized:
            logger.info("Observer instance already initialized; reusing.")
            return
        self.space_id = space_id or os.getenv("PHOENIX_SPACE_ID")
        self.api_key = api_key or os.getenv("PHOENIX_API_KEY")
        self.project_name = project_name
        self.endpoint = endpoint or Endpoints.ARIZE_OTLP_HTTP
        self.environment = environment
        self.oauth_client_id = oauth_client_id
        self.oauth_client_secret = oauth_client_secret
        self.oauth_token_url = oauth_token_url
        self.token_buffer_seconds = token_buffer_seconds
        self.default_sample_rate = default_sample_rate
        self.openai_proxy_url = openai_proxy_url or os.getenv("OPENAI_PROXY_URL")
        self.openai_jwt_token = openai_jwt_token or os.getenv("OPENAI_JWT_TOKEN")
        self.use_phoenix = use_phoenix
        self.access_token: Optional[str] = None
        self.token_expiration: float = 0.0
        self.tracer_provider: Optional[TracerProvider] = None
        self.session = None
        self.arize_client = None if use_phoenix else ArizeClient(api_key=self.api_key, space_id=self.space_id)
        self.batch_buffer: List[Dict[str, Any]] = []
        self.eval_buffer: List[pd.DataFrame] = []
        self.pii_analyzer = AnalyzerEngine()
        self._setup()
        self.__initialized = True

    def _setup(self):
        if not self.space_id or not self.api_key:
            raise ValueError("Space ID and API Key required.")

        if self.oauth_client_id and self.oauth_client_secret and self.oauth_token_url:
            self._fetch_oauth_token()

        os.environ["PHOENIX_SPACE_ID"] = self.space_id
        os.environ["PHOENIX_API_KEY"] = self.api_key

        if self.use_phoenix:
            try:
                self.session = px.launch_app(project_name=self.project_name)
                logger.info(f"Phoenix session initialized for {self.project_name}")
            except Exception as e:
                logger.error(f"Failed to initialize Phoenix: {str(e)}")
                raise
        else:
            logger.info("Using Arize AX mode for enterprise ML observability.")

        resource = Resource(attributes={"environment": self.environment, "service.name": "llm-observability"})
        self.tracer_provider = register(
            space_id=self.space_id,
            api_key=self.api_key,
            project_name=self.project_name,
            endpoint=self.endpoint,
            resource=resource,
        )
        trace.set_tracer_provider(self.tracer_provider)
        logger.info("OpenTelemetry initialized.")

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def _fetch_oauth_token(self):
        try:
            response = requests.post(
                self.oauth_token_url,
                data={
                    "grant_type": "client_credentials",
                    "client_id": self.oauth_client_id,
                    "client_secret": self.oauth_client_secret,
                },
            )
            response.raise_for_status()
            data = response.json()
            self.access_token = data["access_token"]
            expires_in = data.get("expires_in", 3600)
            self.token_expiration = time.time() + expires_in - self.token_buffer_seconds
            os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Bearer {self.access_token}"
            logger.info("OAuth token fetched/renewed.")
        except Exception as e:
            logger.error(f"Failed to fetch OAuth token: {str(e)}")
            raise

    def _ensure_token_valid(self):
        if self.oauth_token_url and time.time() >= self.token_expiration:
            logger.info("OAuth token near expiration; renewing.")
            self._fetch_oauth_token()

    def _anonymize_data(self, text: str) -> str:
        try:
            result = self.pii_analyzer.analyze(text=text, language="en")
            return result.redact()
        except Exception as e:
            logger.error(f"Failed to anonymize data: {str(e)}")
            return text

    def instrument_llm(self, provider: LLMProvider):
        self._ensure_token_valid()
        try:
            if provider in [LLMProvider.OPENAI, LLMProvider.GROK]:
                if self.openai_proxy_url and self.openai_jwt_token:
                    config = {
                        "base_url": self.openai_proxy_url,
                        "extra_headers": {"Authorization": f"Bearer {self.openai_jwt_token}"},
                    }
                    OpenAIInstrumentor().instrument(tracer_provider=self.tracer_provider, client_config=config)
                else:
                    OpenAIInstrumentor().instrument(tracer_provider=self.tracer_provider)
                logger.info(f"{provider.name} instrumented with proxy: {bool(self.openai_proxy_url)}")
            elif provider == LLMProvider.ANTHROPIC:
                AnthropicInstrumentor().instrument(tracer_provider=self.tracer_provider)
                logger.info("Anthropic instrumented.")
            elif provider == LLMProvider.VERTEX_AI:
                VertexAIInstrumentor().instrument(tracer_provider=self.tracer_provider)
                logger.info("Vertex AI (Gemini) instrumented.")
        except Exception as e:
            logger.error(f"Failed to instrument {provider.name}: {str(e)}")
            raise

    def instrument_adk(self):
        self._ensure_token_valid()
        if not Agent:
            raise ImportError("Google ADK not installed.")
        original_run_async = Agent.run_async

        @wraps(original_run_async)
        async def traced_run_async(self, context, *args, **kwargs) -> AsyncGenerator[Any, None]:
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span(f"adk_{self.name}_run_async") as span:
                span.set_attribute("agent_name", self.name)
                span.set_attribute("agent_model", self.model)
                span.set_attribute("agent_description", self.description)
                start_time = time.time()
                try:
                    async for event in original_run_async(self, context, *args, **kwargs):
                        span.set_attribute("event_type", type(event).__name__)
                        yield event
                    span.set_attribute("execution_time_ms", (time.time() - start_time) * 1000)
                    span.set_status(trace.StatusCode.OK)
                except Exception as e:
                    span.set_status(trace.StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    logger.error(f"Error in ADK agent {self.name}: {str(e)}")
                    raise

        Agent.run_async = traced_run_async
        logger.info("Google ADK instrumented.")

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def log_interaction(
        self,
        prompt: str,
        response: str,
        model: str,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        self._ensure_token_valid()
        anonymized_prompt = self._anonymize_data(prompt)
        anonymized_response = self._anonymize_data(response)
        if self.use_phoenix:
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span("llm_interaction") as span:
                span.set_attribute("input", anonymized_prompt)
                span.set_attribute("output", anonymized_response)
                span.set_attribute("model", model)
                for key, value in (metadata or {}).items():
                    span.set_attribute(key, str(value))
                logger.info(f"Logged interaction for {model} in Phoenix mode.")
        else:
            record = Record(
                prediction_id=str(time.time()),
                prediction_label=anonymized_response,
                features={"model": model, "prompt": anonymized_prompt},
                tags=metadata or {},
            )
            self.arize_client.log(record=record, model_id="my-model", model_type=ModelTypes.GENERATIVE_LLM, environment=Environments.PRODUCTION, metric=Metrics.GENERATIVE_LLM)
            logger.info(f"Logged interaction for {model} in Arize AX mode.")

    def log_batch(self, interactions: List[Dict[str, Any]]):
        self._ensure_token_valid()
        self.batch_buffer.extend(interactions)
        if len(self.batch_buffer) >= 100:
            self._flush_batch()
        logger.debug(f"Added {len(interactions)} to batch (total: {len(self.batch_buffer)}).")

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def _flush_batch(self):
        self._ensure_token_valid()
        if not self.batch_buffer:
            return
        try:
            for interaction in self.batch_buffer:
                self.log_interaction(
                    prompt=interaction["prompt"],
                    response=interaction["response"],
                    model=interaction["model"],
                    metadata=interaction.get("metadata"),
                )
            logger.info(f"Flushed {len(self.batch_buffer)} interactions.")
            self.batch_buffer.clear()
        except Exception as e:
            logger.error(f"Failed to flush batch: {str(e)}")
            raise

    async def evaluate_and_log_async(
        self,
        trace_data: Dict[str, Any],
        evaluators: List[Callable],
        sample_rate: Optional[float] = None,
        eval_model: str = "gemini-1.5-pro",
    ):
        self._ensure_token_valid()
        sample_rate = sample_rate or self.default_sample_rate
        if random.random() > sample_rate:
            logger.debug("Skipped evaluation due to sampling.")
            return

        trace_data["prompt"] = self._anonymize_data(trace_data.get("prompt", ""))
        trace_data["response"] = self._anonymize_data(trace_data.get("response", ""))

        df = pd.DataFrame([trace_data])
        results = []
        for evaluator_cls in evaluators:
            evaluator = evaluator_cls(model=eval_model)
            try:
                result = await asyncio.to_thread(evaluator.evaluate, df)
                results.append((result, evaluator_cls.__name__))
            except Exception as e:
                logger.error(f"Failed evaluation with {evaluator_cls.__name__}: {str(e)}")

        self.eval_buffer.extend(results)
        if len(self.eval_buffer) >= 10:
            self._flush_eval_buffer()

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def _flush_eval_buffer(self):
        self._ensure_token_valid()
        if not self.eval_buffer:
            return
        try:
            for result, eval_name in self.eval_buffer:
                if self.use_phoenix:
                    px.log_evaluations(result, eval_name=eval_name)
                else:
                    for index, row in result.iterrows():
                        record = Record(
                            prediction_id=str(time.time()),
                            prediction_label=row.get("output", ""),
                            features={"eval_name": eval_name},
                            tags={"eval_score": row.get("score", 0.0), "eval_label": row.get("label", "unknown")},
                        )
                        self.arize_client.log(record=record, model_id="my-model", model_type=ModelTypes.GENERATIVE_LLM, environment=Environments.PRODUCTION, metric=Metrics.GENERATIVE_LLM)
            logger.info(f"Flushed {len(self.eval_buffer)} evaluation results in {'Phoenix' if self.use_phoenix else 'Arize AX'} mode.")
            self.eval_buffer.clear()
        except Exception as e:
            logger.error(f"Failed to flush eval buffer: {str(e)}")
            raise

    def trace_function(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            self._ensure_token_valid()
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span(func.__name__) as span:
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("execution_time_ms", (time.time() - start_time) * 1000)
                    span.set_status(trace.StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(trace.StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    logger.error(f"Error in {func.__name__}: {str(e)}")
                    raise
        return wrapper

    def shutdown(self):
        try:
            if self.batch_buffer:
                self._flush_batch()
            if self.eval_buffer:
                self._flush_eval_buffer()
            if self.tracer_provider:
                self.tracer_provider.shutdown()
            if self.session:
                self.session.close()
            logger.info("Shutdown completed.")
        except Exception as e:
            logger.error(f"Shutdown error: {str(e)}")
            raise
requirements.txt
txtarize-phoenix>=4.0.0
arize>=7.0.0
arize-otel>=0.1.0
openinference-instrumentation-openai>=0.1.3
openinference-instrumentation-anthropic>=0.1.3
openinference-instrumentation-vertexai>=0.1.4
opentelemetry-exporter-otlp-proto-http>=1.22.0
retry>=0.9.2
requests>=2.28.0
google-cloud-aiplatform>=1.38.0
presidio-analyzer>=2.2.33
pandas>=2.0.0
# Optional for ADK: google-adk
openai_grok_call.py (Demo for OpenAI/Grok with Observer)
pythonimport asyncio
from observer import EnterpriseLLMObserver, LLMProvider
from openai import OpenAI
from phoenix.evaluations import HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator

# Initialize observer (singleton)
observer = EnterpriseLLMObserver(
    space_id="your-space-id",
    api_key="your-api-key",
    endpoint="https://your-apigee-proxy/arize-otlp",
    oauth_client_id="your-client-id",
    oauth_client_secret="your-client-secret",
    oauth_token_url="https://your-oauth-token-url/oauth/token",
    openai_proxy_url="https://your-apigee-proxy/openai",
    openai_jwt_token="your-jwt-token",
    default_sample_rate=0.1,
    use_phoenix=True,
)

observer.instrument_llm(LLMProvider.GROK)

client = OpenAI(
    base_url=observer.openai_proxy_url or "https://api.x.ai/v1",
    api_key="your-xai-api-key",
)
response = client.chat.completions.create(
    model="grok-4",
    messages=[{"role": "user", "content": "Explain quantum computing."}],
)

observer.log_interaction(
    prompt="Explain quantum computing.",
    response=response.choices[0].message.content,
    model="grok-4",
)

trace_data = {"prompt": "Explain quantum computing.", "response": response.choices[0].message.content, "model": "grok-4"}
evaluators = [HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator]
asyncio.run(observer.evaluate_and_log_async(trace_data, evaluators, eval_model="gpt-4o"))

observer.shutdown()
evaluation.py (Demo for Evaluations, Including Offline with PD DataFrame)
pythonimport asyncio
import pandas as pd
from observer import EnterpriseLLMObserver, LLMProvider
from phoenix import TraceDataset
from phoenix.evaluations import HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator

# Initialize observer
observer = EnterpriseLLMObserver(
    space_id="your-space-id",
    api_key="your-api-key",
    endpoint="https://your-apigee-proxy/arize-otlp",
    oauth_client_id="your-client-id",
    oauth_client_secret="your-client-secret",
    oauth_token_url="https://your-oauth-token-url/oauth/token",
    default_sample_rate=0.1,
    use_phoenix=True,
)

# Online eval example
trace_data = {"prompt": "What is AI?", "response": "AI is machine intelligence.", "model": "gemini-1.5-pro"}
evaluators = [HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator]
asyncio.run(observer.evaluate_and_log_async(trace_data, evaluators, eval_model="gemini-1.5-pro"))

# Offline eval example with PD DataFrame
# Assume some interactions are logged or from traces
interactions = [
    {"prompt": "What is ML?", "response": "Machine learning is a subset of AI.", "model": "gemini-1.5-pro"},
    {"prompt": "Explain ethics.", "response": "Ethics in AI is important.", "model": "grok-4"},
]
df = pd.DataFrame(interactions)

# Anonymize
df['prompt'] = df['prompt'].apply(observer._anonymize_data)
df['response'] = df['response'].apply(observer._anonymize_data)

# Run all evaluators
evaluator_types = [HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator]
results = []
for evaluator_cls in evaluator_types:
    evaluator = evaluator_cls(model="gemini-1.5-pro")
    result = evaluator.evaluate(df)
    results.append(result)
    if observer.use_phoenix:
        px.log_evaluations(result, eval_name=evaluator_cls.__name__)
    else:
        for index, row in result.iterrows():
            record = Record(
                prediction_id=str(time.time()),
                prediction_label=row.get("response", ""),
                features={"eval_name": evaluator_cls.__name__},
                tags={"eval_score": row.get("score", 0.0), "eval_label": row.get("label", "unknown")},
            )
            observer.arize_client.log(record=record, model_id="my-model", model_type=ModelTypes.GENERATIVE_LLM, environment=Environments.PRODUCTION, metric=Metrics.GENERATIVE_LLM)

# Aggregate (detailed analysis)
for i, result in enumerate(results):
    avg_score = result['score'].mean()
    logger.info(f"Average score for {evaluator_types[i].__name__}: {avg_score}")
    result.to_csv(f"{evaluator_types[i].__name__}_results.csv")

# Offline from Phoenix traces
traces = TraceDataset.from_project(observer.project_name)
df = traces.to_pandas()
# Repeat evaluation process above

observer.shutdown()
This provides the files for demo. To run:

Install dependencies from requirements.txt: pip install -r requirements.txt.
Run python openai_grok_call.py for Grok demo.
Run python evaluation.py for evaluation demo.
For package, wrap the class later. Let me know if you need more details!
