import asyncio
import json
import pandas as pd
from openai import OpenAI
from observer import EnterpriseLLMObserver, LLMProvider
from google.adk.agents import Agent
from phoenix.evals import HallucinationEvaluator, RelevanceEvaluator, QAEvaluator, ToxicityEvaluator

# Initialize observer
observer = EnterpriseLLMObserver(
    space_id="your-space-id",
    api_key="your-api-key",
    project_name="demo-project",
    default_sample_rate=0.1,
    openai_proxy_url="https://your-proxy/openai",  # Optional
    openai_jwt_token="your-jwt-token",  # Optional
    use_phoenix=True,
)

# Instrument OpenAI for Grok
observer.instrument_llm(LLMProvider.GROK)

# Custom function calls (multiple)
@observer.trace_function
def preprocess_query(query: str) -> str:
    """Preprocess the query."""
    return query.lower().strip()

@observer.trace_function
def validate_query(query: str) -> bool:
    """Validate the query."""
    if len(query) < 5:
        raise ValueError("Query too short.")
    return True

@observer.trace_function
def format_prompt(query: str, prompt_template: str) -> str:
    """Format the query with a prompt template."""
    return prompt_template.format(query=query)

# OpenAI tool function
def get_weather(city: str) -> str:
    """Simulated weather tool."""
    return f"Weather in {city}: Sunny, 25°C"

# OpenAI tool schema
weather_tool = {
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get the weather for a city",
        "parameters": {
            "type": "object",
            "properties": {"city": {"type": "string", "description": "City name"}},
            "required": ["city"],
        },
    },
}

async def main():
    # Start session trace
    session_id = f"session_{int(time.time())}"
    session_span_context = observer.start_session_trace(
        session_id=session_id,
        metadata={"user_id": "demo_user", "app": "weather_chat"}
    )

    # Prompt management
    prompt_id = observer.create_prompt(
        prompt_content="Get weather for {query}.",
        metadata={"version": "v1", "use_case": "weather_query"},
        session_id=session_id
    )
    prompt = observer.read_prompt(prompt_id, session_id=session_id)
    observer.update_prompt(
        prompt_id=prompt_id,
        prompt_content="Fetch current weather for {query}.",
        metadata={"version": "v2"},
        session_id=session_id
    )

    # Multiple function calls
    query = "What's the weather in New York?"
    processed_query = preprocess_query(query)  # Traced
    validate_query(processed_query)  # Traced
    prompt_content = observer.read_prompt(prompt_id)["content"]
    formatted_prompt = format_prompt(processed_query, prompt_content)  # Traced

    # OpenAI call with tool
    client = OpenAI(
        base_url=observer.openai_proxy_url or "https://api.x.ai/v1",
        api_key="your-xai-api-key",
    )
    response = client.chat.completions.create(
        model="grok-4",
        messages=[{"role": "user", "content": formatted_prompt}],
        tools=[weather_tool],
        tool_choice="auto",
    )

    # Log interaction
    response_content = response.choices[0].message.content or ""
    if response.choices[0].message.tool_calls:
        tool_call = response.choices[0].message.tool_calls[0]
        tool_name = tool_call.function.name
        tool_input = json.loads(tool_call.function.arguments)
        tool_output = get_weather(tool_input["city"])
        observer.log_tool_call(
            tool_name=tool_name,
            tool_input=tool_input,
            tool_output=tool_output,
            model="grok-4",
            metadata={"session_id": session_id, "prompt_id": prompt_id},
            session_id=session_id
        )
        response_content = tool_output

    observer.log_interaction(
        prompt=formatted_prompt,
        response=response_content,
        model="grok-4",
        metadata={"session_id": session_id, "prompt_id": prompt_id},
        session_id=session_id
    )

    # Online evaluation (10% sampling)
    trace_data = {
        "prompt": formatted_prompt,
        "response": response_content,
        "model": "grok-4",
    }
    evaluators = [HallucinationEvaluator, RelevanceEvaluator, QAEvaluator, ToxicityEvaluator]
    await observer.evaluate_and_log_async(
        trace_data=trace_data,
        evaluators=evaluators,
        session_id=session_id,
        eval_model="gpt-4o"
    )

    # ADK agent
    agent = Agent(
        name="weather_agent",
        model="gemini-1.5-pro",
        description="Weather query agent",
    )
    async for event in observer.instrument_adk_agent(
        agent=agent,
        context={"query": formatted_prompt},
        session_id=session_id,
        metadata={"agent_task": "weather_lookup"}
    ):
        print(f"ADK event: {event}")

    # Offline evaluation and experiment with dataset
    dataset = pd.DataFrame([
        {
            "prompt": prompt_content.format(query="New York"),
            "response": "Weather in New York: Sunny, 25°C",
            "model": "grok-4",
            "ground_truth": "Weather in New York: Clear, 24°C",
            "prompt_id": prompt_id,
        },
        {
            "prompt": prompt_content.format(query="London"),
            "response": "Weather in London: Rainy, 15°C",
            "model": "grok-4",
            "ground_truth": "Weather in London: Cloudy, 16°C",
            "prompt_id": prompt_id,
        },
    ])
    observer.log_dataset(dataset, dataset_name="weather_dataset", session_id=session_id)
    await observer.run_experiment(
        dataset=dataset,
        experiment_id="weather_exp_001",
        evaluators=evaluators,
        experiment_metadata={"prompt_version": "v2", "use_case": "weather"},
        eval_model="gpt-4o",
        session_id=session_id
    )

    # Update trace with business metrics
    observer.update_trace(
        span_context=session_span_context,
        attributes={
            "user_feedback": "positive",
            "function_calls": 3,  # Count of preprocess_query, validate_query, format_prompt
            "response_time_ms": int((time.time() - time.time()) * 1000)
        }
    )

    # End session trace
    observer.end_session_trace(session_span_context, trace.StatusCode.OK)
    observer.shutdown()

if __name__ == "__main__":
    asyncio.run(main())


-------------
export PHOENIX_SPACE_ID="your-space-id"
export PHOENIX_API_KEY="your-api-key"
export OPENAI_PROXY_URL="https://your-proxy/openai"  # Optional
export OPENAI_JWT_TOKEN="your-jwt-token"  # Optional
export ARIZE_PROJECT_NAME="demo-project"
--------------------------------
arize-phoenix>=4.0.0
arize>=7.0.0
arize-otel>=0.1.0
openinference-instrumentation-openai>=0.1.3
openinference-instrumentation-anthropic>=0.1.3
openinference-instrumentation-vertexai>=0.1.4
opentelemetry-exporter-otlp-proto-http>=1.22.0
retry>=0.9.2
requests>=2.28.0
google-cloud-aiplatform>=1.38.0
presidio-analyzer>=2.2.33
pandas>=2.0.0
openai>=1.0.0
google-adk  # Required for ADK
----------------------------------------
import os
import logging
import time
import random
import asyncio
import uuid
import pandas as pd
import json
import requests
from enum import Enum
from typing import Any, Dict, Optional, List, Callable, AsyncGenerator
from functools import wraps
from retry import retry
import certifi
import phoenix as px
from phoenix import TraceDataset
from phoenix.otel import register
from phoenix.evals import (
    HallucinationEvaluator,
    RelevanceEvaluator,
    QAEvaluator,
    ToxicityEvaluator,
)
from arize.utils.types import ModelTypes, Environments, Metrics
from arize.public_pb2 import Record
from arize.api import Client as ArizeClient
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.resources import Resource
from openinference.instrumentation.openai import OpenAIInstrumentor
from openinference.instrumentation.anthropic import AnthropicInstrumentor
from openinference.instrumentation.vertexai import VertexAIInstrumentor
from presidio_analyzer import AnalyzerEngine
try:
    from google.adk.agents import Agent
except ImportError:
    Agent = None

# Structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
)
logger = logging.getLogger(__name__)

class LLMProvider(Enum):
    VERTEX_AI = "vertexai"
    OPENAI = "openai"
    GROK = "grok"
    ANTHROPIC = "anthropic"

class EnterpriseLLMObserver:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.__initialized = False
        return cls._instance

    def __init__(
        self,
        space_id: Optional[str] = None,
        api_key: Optional[str] = None,
        project_name: str = "enterprise-llm-observability",
        endpoint: Optional[str] = None,
        environment: str = "production",
        default_sample_rate: float = 0.1,
        openai_proxy_url: Optional[str] = None,
        openai_jwt_token: Optional[str] = None,
        use_phoenix: bool = True,
        fallback_log_file: str = "fallback_traces.jsonl",
    ):
        if self.__initialized:
            logger.info("Observer instance already initialized; reusing.")
            return
        self.space_id = space_id or os.getenv("PHOENIX_SPACE_ID")
        self.api_key = api_key or os.getenv("PHOENIX_API_KEY")
        self.project_name = project_name
        self.endpoint = endpoint or os.getenv("PHOENIX_COLLECTOR_ENDPOINT") or "http://localhost:6006"
        self.environment = environment
        self.default_sample_rate = default_sample_rate
        self.openai_proxy_url = openai_proxy_url or os.getenv("OPENAI_PROXY_URL")
        self.openai_jwt_token = openai_jwt_token or os.getenv("OPENAI_JWT_TOKEN")
        self.use_phoenix = use_phoenix
        self.fallback_log_file = fallback_log_file
        self.tracer_provider: Optional[TracerProvider] = None
        self.session = None
        self.arize_client = None if use_phoenix else ArizeClient(api_key=self.api_key, space_id=self.space_id)
        self.batch_buffer: List[Dict[str, Any]] = []
        self.eval_buffer: List[pd.DataFrame] = []
        self.prompt_store: Dict[str, Dict[str, Any]] = {}  # In-memory prompt store
        self.pii_analyzer = AnalyzerEngine()
        self._setup()
        self.__initialized = True

    def test_endpoint(self) -> bool:
        """Test connectivity to the Phoenix endpoint."""
        try:
            response = requests.get(f"{self.endpoint}/v1/traces", timeout=5)
            if response.status_code in [200, 404]:  # 404 may indicate endpoint exists but no data
                logger.info(f"Successfully connected to Phoenix endpoint: {self.endpoint}")
                return True
            else:
                logger.warning(f"Phoenix endpoint {self.endpoint} returned status code {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"Failed to connect to Phoenix endpoint {self.endpoint}: {str(e)}")
            return False

    def _log_fallback(self, data: Dict[str, Any]):
        """Log data to a fallback file if Arize endpoint fails."""
        try:
            with open(self.fallback_log_file, "a") as f:
                f.write(json.dumps(data) + "\n")
            logger.info(f"Logged data to fallback file: {self.fallback_log_file}")
        except Exception as e:
            logger.error(f"Failed to write to fallback file {self.fallback_log_file}: {str(e)}")

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def _setup(self):
        if not self.space_id or not self.api_key:
            raise ValueError("Space ID and API Key required.")

        os.environ["PHOENIX_SPACE_ID"] = self.space_id
        os.environ["PHOENIX_API_KEY"] = self.api_key
        os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = self.endpoint
        os.environ["GRPC_DEFAULT_SSL_ROOTS_FILE_PATH"] = certifi.where()

        if self.use_phoenix:
            try:
                self.session = px.launch_app(project_name=self.project_name)
                logger.info(f"Phoenix session initialized for {self.project_name}")
            except Exception as e:
                logger.error(f"Failed to initialize Phoenix: {str(e)}")
                self._log_fallback({"event": "phoenix_init_failure", "error": str(e)})
                raise

        try:
            if not self.test_endpoint():
                logger.warning(f"Phoenix endpoint {self.endpoint} is unreachable. Falling back to file logging.")
                self._log_fallback({"event": "endpoint_unreachable", "endpoint": self.endpoint})

            self.tracer_provider = register(
                project_name=self.project_name,
                auto_instrument=True,
            )
            trace.set_tracer_provider(self.tracer_provider)
            logger.info(f"OpenTelemetry initialized with endpoint: {self.endpoint}")
        except Exception as e:
            logger.error(f"Failed to initialize OpenTelemetry: {str(e)}")
            self._log_fallback({"event": "otlp_init_failure", "error": str(e)})
            raise

    def _anonymize_data(self, text: str) -> str:
        try:
            result = self.pii_analyzer.analyze(text=text, language="en")
            return result.redact()
        except Exception as e:
            logger.error(f"Failed to anonymize data: {str(e)}")
            return text

    def create_prompt(
        self,
        prompt_content: str,
        prompt_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
    ) -> str:
        """Create a prompt and log it to Arize."""
        prompt_id = prompt_id or str(uuid.uuid4())
        metadata = metadata or {}
        metadata["project"] = self.project_name
        metadata["prompt_id"] = prompt_id
        anonymized_content = self._anonymize_data(prompt_content)
        self.prompt_store[prompt_id] = {
            "content": anonymized_content,
            "metadata": metadata,
            "created_at": time.time(),
        }

        tracer = trace.get_tracer(__name__)
        span_context = self.start_session_trace(session_id) if session_id else None
        with tracer.start_as_current_span(
            "create_prompt",
            context=span_context
        ) if span_context else tracer.start_as_current_span("create_prompt") as span:
            span.set_attribute("prompt_id", prompt_id)
            span.set_attribute("content", anonymized_content)
            for key, value in metadata.items():
                span.set_attribute(key, str(value))
            span.set_status(trace.StatusCode.OK)

        if self.use_phoenix:
            logger.info(f"Created prompt {prompt_id} in Phoenix mode.")
        else:
            try:
                record = Record(
                    prediction_id=str(time.time()),
                    prediction_label=anonymized_content,
                    features={"prompt_id": prompt_id, "type": "prompt_create"},
                    tags=metadata,
                )
                self.arize_client.log(
                    record=record,
                    model_id=self.project_name,
                    model_type=ModelTypes.GENERATIVE_LLM,
                    environment=Environments.PRODUCTION,
                    metrics=[Metrics.GENERATIVE_LLM]
                )
                logger.info(f"Created prompt {prompt_id} in Arize AX mode.")
            except Exception as e:
                logger.error(f"Failed to log prompt {prompt_id} to Arize AX: {str(e)}")
                self._log_fallback({"event": "prompt_create", "prompt_id": prompt_id, "error": str(e)})

        if span_context:
            self.end_session_trace(span_context, trace.StatusCode.OK)
        return prompt_id

    def update_prompt(
        self,
        prompt_id: str,
        prompt_content: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
    ):
        """Update a prompt's content or metadata."""
        if prompt_id not in self.prompt_store:
            logger.error(f"Prompt {prompt_id} not found.")
            raise ValueError(f"Prompt {prompt_id} not found.")

        tracer = trace.get_tracer(__name__)
        span_context = self.start_session_trace(session_id) if session_id else None
        with tracer.start_as_current_span(
            "update_prompt",
            context=span_context
        ) if span_context else tracer.start_as_current_span("update_prompt") as span:
            try:
                if prompt_content:
                    self.prompt_store[prompt_id]["content"] = self._anonymize_data(prompt_content)
                    span.set_attribute("content", self.prompt_store[prompt_id]["content"])
                if metadata:
                    self.prompt_store[prompt_id]["metadata"].update(metadata)
                    for key, value in metadata.items():
                        span.set_attribute(key, str(value))
                self.prompt_store[prompt_id]["updated_at"] = time.time()
                span.set_attribute("prompt_id", prompt_id)
                span.set_status(trace.StatusCode.OK)

                if self.use_phoenix:
                    logger.info(f"Updated prompt {prompt_id} in Phoenix mode.")
                else:
                    try:
                        record = Record(
                            prediction_id=str(time.time()),
                            prediction_label=self.prompt_store[prompt_id]["content"],
                            features={"prompt_id": prompt_id, "type": "prompt_update"},
                            tags=self.prompt_store[prompt_id]["metadata"],
                        )
                        self.arize_client.log(
                            record=record,
                            model_id=self.project_name,
                            model_type=ModelTypes.GENERATIVE_LLM,
                            environment=Environments.PRODUCTION,
                            metrics=[Metrics.GENERATIVE_LLM]
                        )
                        logger.info(f"Updated prompt {prompt_id} in Arize AX mode.")
                    except Exception as e:
                        logger.error(f"Failed to log prompt update {prompt_id} to Arize AX: {str(e)}")
                        self._log_fallback({"event": "prompt_update", "prompt_id": prompt_id, "error": str(e)})
            except Exception as e:
                span.set_status(trace.StatusCode.ERROR, description=str(e))
                span.record_exception(e)
                logger.error(f"Failed to update prompt {prompt_id}: {str(e)}")
                self._log_fallback({"event": "prompt_update_failure", "prompt_id": prompt_id, "error": str(e)})
                raise
            finally:
                if span_context:
                    self.end_session_trace(span_context, trace.StatusCode.OK)

    def read_prompt(self, prompt_id: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """Read a prompt by ID."""
        if prompt_id not in self.prompt_store:
            logger.error(f"Prompt {prompt_id} not found.")
            raise ValueError(f"Prompt {prompt_id} not found.")

        tracer = trace.get_tracer(__name__)
        span_context = self.start_session_trace(session_id) if session_id else None
        with tracer.start_as_current_span(
            "read_prompt",
            context=span_context
        ) if span_context else tracer.start_as_current_span("read_prompt") as span:
            span.set_attribute("prompt_id", prompt_id)
            span.set_attribute("content", self.prompt_store[prompt_id]["content"])
            for key, value in self.prompt_store[prompt_id]["metadata"].items():
                span.set_attribute(key, str(value))
            span.set_status(trace.StatusCode.OK)

        if span_context:
            self.end_session_trace(span_context, trace.StatusCode.OK)
        logger.info(f"Read prompt {prompt_id}.")
        return self.prompt_store[prompt_id]

    def instrument_llm(self, provider: LLMProvider):
        try:
            if provider in [LLMProvider.OPENAI, LLMProvider.GROK]:
                if self.openai_proxy_url and self.openai_jwt_token:
                    config = {
                        "base_url": self.openai_proxy_url,
                        "extra_headers": {"Authorization": f"Bearer {self.openai_jwt_token}"},
                    }
                    OpenAIInstrumentor().instrument(tracer_provider=self.tracer_provider, client_config=config)
                else:
                    OpenAIInstrumentor().instrument(tracer_provider=self.tracer_provider)
                logger.info(f"{provider.name} instrumented with proxy: {bool(self.openai_proxy_url)}")
            elif provider == LLMProvider.ANTHROPIC:
                AnthropicInstrumentor().instrument(tracer_provider=self.tracer_provider)
                logger.info("Anthropic instrumented.")
            elif provider == LLMProvider.VERTEX_AI:
                VertexAIInstrumentor().instrument(tracer_provider=self.tracer_provider)
                logger.info("Vertex AI (Gemini) instrumented.")
        except Exception as e:
            logger.error(f"Failed to instrument {provider.name}: {str(e)}")
            self._log_fallback({"event": "llm_instrument_failure", "provider": provider.name, "error": str(e)})
            raise

    async def instrument_adk_agent(
        self,
        agent: Any,
        context: Dict[str, Any],
        session_id: str,
        metadata: Optional[Dict[str, Any]] = None,
        *args,
        **kwargs
    ) -> AsyncGenerator[Any, None]:
        if not Agent:
            raise ImportError("Google ADK not installed.")
        tracer = trace.get_tracer(__name__)
        session_span_context = self.start_session_trace(session_id, metadata)
        with tracer.start_as_current_span(
            f"adk_{agent.name}_run_async",
            context=session_span_context
        ) as span:
            span.set_attribute("agent_name", agent.name)
            span.set_attribute("agent_model", agent.model)
            span.set_attribute("agent_description", agent.description)
            start_time = time.time()
            try:
                async for event in agent.run_async(context, *args, **kwargs):
                    span.set_attribute("event_type", type(event).__name__)
                    if hasattr(event, "tool_calls"):
                        for tool_call in event.tool_calls:
                            self.log_tool_call(
                                tool_name=tool_call.name,
                                tool_input=tool_call.input,
                                tool_output=tool_call.output,
                                model=agent.model,
                                metadata={"agent_name": agent.name, "session_id": session_id}
                            )
                    yield event
                span.set_attribute("execution_time_ms", (time.time() - start_time) * 1000)
                span.set_status(trace.StatusCode.OK)
                self.end_session_trace(session_span_context, trace.StatusCode.OK)
            except Exception as e:
                span.set_status(trace.StatusCode.ERROR, description=str(e))
                span.record_exception(e)
                self.end_session_trace(session_span_context, trace.StatusCode.ERROR, str(e))
                logger.error(f"Error in ADK agent {agent.name}: {str(e)}")
                self._log_fallback({"event": "adk_agent_failure", "agent_name": agent.name, "error": str(e)})
                raise

    def instrument_model_context(self, context_processor: Callable) -> Callable:
        @wraps(context_processor)
        def wrapper(*args, **kwargs):
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span("model_context_processing") as span:
                start_time = time.time()
                try:
                    result = context_processor(*args, **kwargs)
                    span.set_attribute("context_input", str(args))
                    span.set_attribute("context_output", self._anonymize_data(str(result)))
                    span.set_attribute("execution_time_ms", (time.time() - start_time) * 1000)
                    span.set_status(trace.StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(trace.StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    logger.error(f"Error in context processing: {str(e)}")
                    self._log_fallback({"event": "context_processing_failure", "error": str(e)})
                    raise
        return wrapper

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def log_interaction(
        self,
        prompt: str,
        response: str,
        model: str,
        metadata: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
    ):
        anonymized_prompt = self._anonymize_data(prompt)
        anonymized_response = self._anonymize_data(response)
        metadata = metadata or {}
        metadata["project"] = self.project_name
        if session_id:
            metadata["session_id"] = session_id
        tracer = trace.get_tracer(__name__)
        span_context = self.start_session_trace(session_id) if session_id else None
        with tracer.start_as_current_span(
            "llm_interaction",
            context=span_context
        ) if span_context else tracer.start_as_current_span("llm_interaction") as span:
            span.set_attribute("input", anonymized_prompt)
            span.set_attribute("output", anonymized_response)
            span.set_attribute("model", model)
            for key, value in metadata.items():
                span.set_attribute(key, str(value))
            span.set_status(trace.StatusCode.OK)
            if self.use_phoenix:
                logger.info(f"Logged interaction for {model} in Phoenix mode.")
            else:
                try:
                    record = Record(
                        prediction_id=str(time.time()),
                        prediction_label=anonymized_response,
                        features={"model": model, "prompt": anarchist_prompt},
                        tags=metadata,
                    )
                    self.arize_client.log(
                        record=record,
                        model_id=self.project_name,
                        model_type=ModelTypes.GENERATIVE_LLM,
                        environment=Environments.PRODUCTION,
                        metrics=[Metrics.GENERATIVE_LLM]
                    )
                    logger.info(f"Logged interaction for {model} in Arize AX mode.")
                except Exception as e:
                    logger.error(f"Failed to log interaction for {model}: {str(e)}")
                    self._log_fallback({"event": "interaction_log_failure", "model": model, "error": str(e)})
        if span_context:
            self.end_session_trace(span_context, trace.StatusCode.OK)

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def log_tool_call(
        self,
        tool_name: str,
        tool_input: Any,
        tool_output: Any,
        model: str,
        metadata: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
    ):
        anonymized_input = self._anonymize_data(str(tool_input))
        anonymized_output = self._anonymize_data(str(tool_output))
        metadata = metadata or {}
        metadata["project"] = self.project_name
        metadata["interaction_type"] = "tool_call"
        if session_id:
            metadata["session_id"] = session_id
        tracer = trace.get_tracer(__name__)
        span_context = self.start_session_trace(session_id) if session_id else None
        with tracer.start_as_current_span(
            f"tool_call_{tool_name}",
            context=span_context
        ) if span_context else tracer.start_as_current_span(f"tool_call_{tool_name}") as span:
            span.set_attribute("tool_name", tool_name)
            span.set_attribute("tool_input", anonymized_input)
            span.set_attribute("tool_output", anonymized_output)
            span.set_attribute("model", model)
            for key, value in metadata.items():
                span.set_attribute(key, str(value))
            span.set_status(trace.StatusCode.OK)
            if self.use_phoenix:
                logger.info(f"Logged tool call {tool_name} for {model} in Phoenix mode.")
            else:
                try:
                    record = Record(
                        prediction_id=str(time.time()),
                        prediction_label=anonymized_output,
                        features={"tool_name": tool_name, "tool_input": anonymized_input, "model": model},
                        tags=metadata,
                    )
                    self.arize_client.log(
                        record=record,
                        model_id=self.project_name,
                        model_type=ModelTypes.GENERATIVE_LLM,
                        environment=Environments.PRODUCTION,
                        metrics=[Metrics.GENERATIVE_LLM]
                    )
                    logger.info(f"Logged tool call {tool_name} for {model} in Arize AX mode.")
                except Exception as e:
                    logger.error(f"Failed to log tool call {tool_name}: {str(e)}")
                    self._log_fallback({"event": "tool_call_log_failure", "tool_name": tool_name, "error": str(e)})
        if span_context:
            self.end_session_trace(span_context, trace.StatusCode.OK)

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def log_gen_ai_interaction(
        self,
        interaction_type: str,
        input_data: str,
        output_data: str,
        model: str,
        metadata: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
    ):
        anonymized_input = self._anonymize_data(input_data)
        anonymized_output = self._anonymize_data(output_data)
        metadata = metadata or {}
        metadata["project"] = self.project_name
        metadata["interaction_type"] = interaction_type
        if session_id:
            metadata["session_id"] = session_id
        tracer = trace.get_tracer(__name__)
        span_context = self.start_session_trace(session_id) if session_id else None
        with tracer.start_as_current_span(
            f"gen_ai_interaction_{interaction_type}",
            context=span_context
        ) if span_context else tracer.start_as_current_span(f"gen_ai_interaction_{interaction_type}") as span:
            span.set_attribute("input", anonymized_input)
            span.set_attribute("output", anonymized_output)
            span.set_attribute("model", model)
            for key, value in metadata.items():
                span.set_attribute(key, str(value))
            span.set_status(trace.StatusCode.OK)
            if self.use_phoenix:
                logger.info(f"Logged {interaction_type} for {model} in Phoenix mode.")
            else:
                try:
                    record = Record(
                        prediction_id=str(time.time()),
                        prediction_label=anonymized_output,
                        features={"interaction_type": interaction_type, "input": anonymized_input, "model": model},
                        tags=metadata,
                    )
                    self.arize_client.log(
                        record=record,
                        model_id=self.project_name,
                        model_type=ModelTypes.GENERATIVE_LLM,
                        environment=Environments.PRODUCTION,
                        metrics=[Metrics.GENERATIVE_LLM]
                    )
                    logger.info(f"Logged {interaction_type} for {model} in Arize AX mode.")
                except Exception as e:
                    logger.error(f"Failed to log interaction {interaction_type}: {str(e)}")
                    self._log_fallback({"event": "gen_ai_interaction_log_failure", "interaction_type": interaction_type, "error": str(e)})
        if span_context:
            self.end_session_trace(span_context, trace.StatusCode.OK)

    def log_batch(self, interactions: List[Dict[str, Any]]):
        self.batch_buffer.extend(interactions)
        if len(self.batch_buffer) >= 100:
            self._flush_batch()
        logger.debug(f"Added {len(interactions)} to batch (total: {len(self.batch_buffer)}).")

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def _flush_batch(self):
        if not self.batch_buffer:
            return
        try:
            for interaction in self.batch_buffer:
                if "interaction_type" in interaction:
                    self.log_gen_ai_interaction(
                        interaction_type=interaction["interaction_type"],
                        input_data=interaction["input_data"],
                        output_data=interaction["output_data"],
                        model=interaction["model"],
                        metadata=interaction.get("metadata"),
                        session_id=interaction.get("session_id"),
                    )
                elif "tool_name" in interaction:
                    self.log_tool_call(
                        tool_name=interaction["tool_name"],
                        tool_input=interaction["tool_input"],
                        tool_output=interaction["tool_output"],
                        model=interaction["model"],
                        metadata=interaction.get("metadata"),
                        session_id=interaction.get("session_id"),
                    )
                else:
                    self.log_interaction(
                        prompt=interaction["prompt"],
                        response=interaction["response"],
                        model=interaction["model"],
                        metadata=interaction.get("metadata"),
                        session_id=interaction.get("session_id"),
                    )
            logger.info(f"Flushed {len(self.batch_buffer)} interactions.")
            self.batch_buffer.clear()
        except Exception as e:
            logger.error(f"Failed to flush batch: {str(e)}")
            self._log_fallback({"event": "batch_flush_failure", "error": str(e)})
            raise

    async def evaluate_and_log_async(
        self,
        trace_data: Dict[str, Any],
        evaluators: List[Callable],
        session_id: Optional[str] = None,
        sample_rate: Optional[float] = None,
        eval_model: str = "gemini-1.5-pro",
    ):
        sample_rate = sample_rate or self.default_sample_rate
        if random.random() > sample_rate:
            logger.debug("Skipped evaluation due to sampling.")
            return

        trace_data["prompt"] = self._anonymize_data(trace_data.get("prompt", ""))
        trace_data["response"] = self._anonymize_data(trace_data.get("response", ""))

        df = pd.DataFrame([trace_data])
        results = []
        tracer = trace.get_tracer(__name__)
        span_context = self.start_session_trace(session_id) if session_id else None
        for evaluator_cls in evaluators:
            with tracer.start_as_current_span(
                f"eval_{evaluator_cls.__name__}",
                context=span_context
            ) if span_context else tracer.start_as_current_span(f"eval_{evaluator_cls.__name__}") as span:
                evaluator = evaluator_cls(model=eval_model)
                try:
                    result = await asyncio.to_thread(evaluator.evaluate, df)
                    span.set_attribute("eval_score", result.get("score", 0.0).iloc[0])
                    span.set_attribute("eval_label", result.get("label", "unknown").iloc[0])
                    span.set_status(trace.StatusCode.OK)
                    results.append((result, evaluator_cls.__name__))
                except Exception as e:
                    span.set_status(trace.StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    logger.error(f"Failed evaluation with {evaluator_cls.__name__}: {str(e)}")
                    self._log_fallback({"event": "evaluation_failure", "evaluator": evaluator_cls.__name__, "error": str(e)})
        if span_context:
            self.end_session_trace(span_context, trace.StatusCode.OK)

        self.eval_buffer.extend(results)
        if len(self.eval_buffer) >= 10:
            self._flush_eval_buffer()

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def _flush_eval_buffer(self):
        if not self.eval_buffer:
            return
        try:
            for result, eval_name in self.eval_buffer:
                if self.use_phoenix:
                    px.log_evaluations(result, name=eval_name)
                else:
                    for index, row in result.iterrows():
                        record = Record(
                            prediction_id=str(time.time()),
                            prediction_label=row.get("response", ""),
                            features={"eval_name": eval_name, "project": self.project_name},
                            tags={"eval_score": row.get("score", 0.0), "eval_label": row.get("label", "unknown")},
                        )
                        try:
                            self.arize_client.log(
                                record=record,
                                model_id=self.project_name,
                                model_type=ModelTypes.GENERATIVE_LLM,
                                environment=Environments.PRODUCTION,
                                metrics=[Metrics.GENERATIVE_LLM]
                            )
                        except Exception as e:
                            logger.error(f"Failed to log evaluation {eval_name}: {str(e)}")
                            self._log_fallback({"event": "eval_log_failure", "eval_name": eval_name, "error": str(e)})
            logger.info(f"Flushed {len(self.eval_buffer)} evaluation results in {'Phoenix' if self.use_phoenix else 'Arize AX'} mode.")
            self.eval_buffer.clear()
        except Exception as e:
            logger.error(f"Failed to flush eval buffer: {str(e)}")
            self._log_fallback({"event": "eval_buffer_flush_failure", "error": str(e)})
            raise

    def load_dataset(self, source: str, format: str = "csv") -> pd.DataFrame:
        try:
            if source == "phoenix_traces":
                traces = TraceDataset.from_project(self.project_name)
                df = traces.to_pandas()
            elif format == "csv":
                df = pd.read_csv(source)
            elif format == "json":
                df = pd.read_json(source)
            else:
                raise ValueError(f"Unsupported format: {format}")

            required_columns = ["prompt", "response", "model"]
            if not all(col in df.columns for col in required_columns):
                raise ValueError(f"Dataset must contain {required_columns}")

            df['prompt'] = df['prompt'].apply(self._anonymize_data)
            df['response'] = df['response'].apply(self._anonymize_data)
            if "ground_truth" in df.columns:
                df['ground_truth'] = df['ground_truth'].apply(self._anonymize_data)
            logger.info(f"Loaded dataset from {source} with {len(df)} rows.")
            return df
        except Exception as e:
            logger.error(f"Failed to load dataset: {str(e)}")
            self._log_fallback({"event": "dataset_load_failure", "source": source, "error": str(e)})
            raise

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def log_dataset(self, dataset: pd.DataFrame, dataset_name: str, session_id: Optional[str] = None):
        tracer = trace.get_tracer(__name__)
        span_context = self.start_session_trace(session_id) if session_id else None
        with tracer.start_as_current_span(
            "log_dataset",
            context=span_context
        ) if span_context else tracer.start_as_current_span("log_dataset") as span:
            try:
                span.set_attribute("dataset_name", dataset_name)
                span.set_attribute("rows", len(dataset))
                if self.use_phoenix:
                    px.log_dataset(dataset, name=dataset_name, project_name=self.project_name)
                    logger.info(f"Logged dataset {dataset_name} to Phoenix.")
                else:
                    for index, row in dataset.iterrows():
                        record = Record(
                            prediction_id=str(time.time() + index),
                            prediction_label=row.get("response", ""),
                            features={"prompt": row.get("prompt", ""), "model": row.get("model", "")},
                            tags={"dataset_name": dataset_name, "project": self.project_name},
                        )
                        try:
                            self.arize_client.log(
                                record=record,
                                model_id=self.project_name,
                                model_type=ModelTypes.GENERATIVE_LLM,
                                environment=Environments.PRODUCTION,
                                metrics=[Metrics.GENERATIVE_LLM]
                            )
                        except Exception as e:
                            logger.error(f"Failed to log dataset record {index}: {str(e)}")
                            self._log_fallback({"event": "dataset_record_log_failure", "dataset_name": dataset_name, "index": index, "error": str(e)})
                    logger.info(f"Logged dataset {dataset_name} to Arize AX.")
                span.set_status(trace.StatusCode.OK)
            except Exception as e:
                span.set_status(trace.StatusCode.ERROR, description=str(e))
                span.record_exception(e)
                logger.error(f"Failed to log dataset {dataset_name}: {str(e)}")
                self._log_fallback({"event": "dataset_log_failure", "dataset_name": dataset_name, "error": str(e)})
                raise
            finally:
                if span_context:
                    self.end_session_trace(span_context, trace.StatusCode.OK)

    async def run_experiment(
        self,
        dataset: pd.DataFrame,
        experiment_id: str,
        evaluators: List[Callable],
        experiment_metadata: Optional[Dict[str, Any]] = None,
        eval_model: str = "gemini-1.5-pro",
        session_id: Optional[str] = None,
    ):
        experiment_metadata = experiment_metadata or {}
        experiment_metadata["experiment_id"] = experiment_id
        experiment_metadata["project"] = self.project_name
        tracer = trace.get_tracer(__name__)
        span_context = self.start_session_trace(session_id or experiment_id, experiment_metadata)
        results = []
        for evaluator_cls in evaluators:
            with tracer.start_as_current_span(
                f"experiment_{evaluator_cls.__name__}",
                context=span_context
            ) as span:
                evaluator = evaluator_cls(model=eval_model)
                try:
                    result = await asyncio.to_thread(evaluator.evaluate, dataset)
                    span.set_attribute("eval_rows", len(dataset))
                    if self.use_phoenix:
                        px.log_evaluations(result, name=f"{evaluator_cls.__name__}_{experiment_id}")
                    else:
                        for index, row in result.iterrows():
                            record = Record(
                                prediction_id=str(time.time() + index),
                                prediction_label=row.get("response", ""),
                                features={"eval_name": evaluator_cls.__name__, "prompt": row.get("prompt", "")},
                                tags={
                                    "eval_score": row.get("score", 0.0),
                                    "eval_label": row.get("label", "unknown"),
                                    **experiment_metadata,
                                },
                            )
                            try:
                                self.arize_client.log(
                                    record=record,
                                    model_id=self.project_name,
                                    model_type=ModelTypes.GENERATIVE_LLM,
                                    environment=Environments.PRODUCTION,
                                    metrics=[Metrics.GENERATIVE_LLM]
                                )
                            except Exception as e:
                                logger.error(f"Failed to log experiment evaluation {evaluator_cls.__name__}: {str(e)}")
                                self._log_fallback({"event": "experiment_log_failure", "evaluator": evaluator_cls.__name__, "error": str(e)})
                    results.append((result, evaluator_cls.__name__))
                    span.set_status(trace.StatusCode.OK)
                    logger.info(f"Experiment {experiment_id}: Evaluated {evaluator_cls.__name__} on {len(dataset)} rows.")
                except Exception as e:
                    span.set_status(trace.StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    logger.error(f"Experiment {experiment_id}: Failed evaluation with {evaluator_cls.__name__}: {str(e)}")
                    self._log_fallback({"event": "experiment_evaluation_failure", "evaluator": evaluator_cls.__name__, "error": str(e)})
        self.end_session_trace(span_context, trace.StatusCode.OK)

        for result, eval_name in results:
            avg_score = result['score'].mean() if 'score' in result.columns else 0.0
            logger.info(f"Experiment {experiment_id}: Average {eval_name} score: {avg_score}")
            result.to_csv(f"experiment_{experiment_id}_{eval_name}_results.csv")

    def start_session_trace(self, session_id: str, metadata: Optional[Dict[str, Any]] = None) -> trace.SpanContext:
        tracer = trace.get_tracer(__name__)
        span = tracer.start_span(f"session_{session_id}")
        span.set_attribute("session_id", session_id)
        for key, value in (metadata or {}).items():
            span.set_attribute(key, str(value))
        logger.info(f"Started session trace for {session_id}.")
        return span.context

    def end_session_trace(self, session_span_context: trace.SpanContext, status: trace.StatusCode = trace.StatusCode.OK, description: Optional[str] = None):
        tracer = trace.get_tracer(__name__)
        span = tracer.start_span("end_session", context=session_span_context)
        span.set_status(status, description=description)
        span.end()
        logger.info("Ended session trace.")

    def update_trace(self, span_context: trace.SpanContext, attributes: Dict[str, Any]):
        tracer = trace.get_tracer(__name__)
        with tracer.start_as_current_span("update_trace", context=span_context) as span:
            for key, value in attributes.items():
                span.set_attribute(key, str(value))
            logger.info("Updated trace with new attributes.")

    def trace_function(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span(func.__name__) as span:
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("execution_time_ms", (time.time() - start_time) * 1000)
                    span.set_status(trace.StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(trace.StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    logger.error(f"Error in {func.__name__}: {str(e)}")
                    self._log_fallback({"event": "function_trace_failure", "function": func.__name__, "error": str(e)})
                    raise
        return wrapper

    def shutdown(self):
        try:
            if self.batch_buffer:
                self._flush_batch()
            if self.eval_buffer:
                self._flush_eval_buffer()
            if self.tracer_provider:
                self.tracer_provider.shutdown()
            if self.session:
                self.session.close()
            logger.info("Shutdown completed.")
        except Exception as e:
            logger.error(f"Shutdown error: {str(e)}")
            self._log_fallback({"event": "shutdown_failure", "error": str(e)})
            raise



Agentic AI Systems: Architectural Blueprint
1. Architectural Overview
This document outlines an architectural approach for developing two distinct, high-assurance AI agents: "The Guardian" (Continuous QA Agent) and "The Auditor" (Transaction Testing Agent).

The core principle of this architecture is Sovereign Systems, Shared Engineering. This model is designed to meet strict compliance and data isolation requirements while maximizing engineering efficiency and code quality. Each application operates as a completely independent, self-contained system with its own infrastructure, data stores, and security credentials.

The shared elements are a common, centrally-governed set of Python packages—the Shared Agentic Toolkit—and a standardized network protocol for tool communication, the Fast Model Context Protocol (FastMCP). This approach ensures agents can reliably communicate with a constellation of independent tool microservices.

2. Comparative Analysis
The following tables detail the similarities (the shared toolkit and protocol) and the critical distinctions (the sovereign applications).

Table 1: Similarities (The Shared Components)
These components are developed once and are used by each independent application to ensure consistency and efficiency.

Shared Component

Functional Purpose

Governance Model

agent_kernel

Provides the core reasoning "brain" (Orchestrator) and the state management protocol (MCP) for complex tasks.

Centrally developed and versioned by a core engineering team. Focus on logic, efficiency, and performance.

FastMCP & ToolClient

Defines a standardized network protocol for how the Orchestrator communicates with tools. Provides a reusable client to simplify making secure API calls to tool microservices.

The protocol is centrally governed to ensure interoperability. The client is centrally developed to ensure consistent security and reliability.

connectors

A library of pre-built, unconfigured clients for interacting with enterprise systems (e.g., FileNet, SQL Databases).

Centrally developed. Focus on robust, secure, and performant interactions with target systems.

knowledge_interface

A generic "adapter" for vector databases, allowing either application to use any vector DB vendor without changing its core code.

Centrally developed. Focus on providing a standard, efficient interface for semantic search.

Table 2: Distinctions (The Sovereign Applications)
These components are unique to each application, ensuring strict data and process isolation.

Feature

"The Guardian" (QA Agent App)

"The Auditor" (Transaction Testing App)

Infrastructure

Separate & Isolated. Runs on its own virtual machines or containers with dedicated network access rules.

Separate & Isolated. Runs on its own, different infrastructure with no shared access.

Data Stores

Dedicated. Owns and manages its private Vector DB, Knowledge Store (for QA policies), and Findings Database.

Dedicated. Owns and manages its private Vector DB, Knowledge Store (for audit guidelines), and Test Results Database.

Tool Deployment

Separate & Isolated. Owns and manages the deployment of its own set of tool microservices.

Separate & Isolated. Owns and manages its own, different set of tool microservices.

Credentials

Unique. Uses its own set of service account credentials that are scoped with the principle of least privilege for QA data only.

Unique. Uses a different set of credentials with access rights scoped only to audit-related data sources.

3. Common Components Functional Specification
This section provides a high-level documentation of the shared components.

3.1. agent_kernel Package
This is the most critical package, containing the agent's core intelligence.

Orchestrator: The "brain" of the agent. Its function is to receive a high-level goal and manage a reasoning loop to achieve it.

Model Context Protocol (MCP): The agent's "working memory," implemented as a data object to track the complete state of a task.

3.2. FastMCP (Fast Model Context Protocol) for Tools
This protocol replaces the in-process tool_sdk. It defines how the Orchestrator communicates with tools over a network, allowing tools to be independent microservices.

Purpose: To completely decouple tools from the agent's core application, allowing them to be developed in any language, and deployed, scaled, and secured independently.

Protocol Specification:

Transport: Tools MUST be exposed as RESTful microservices over HTTP/S.

Endpoint: Each tool service MUST expose a standard /execute endpoint that accepts POST requests.

Data Format: The request and response body MUST be in JSON format.

Request Schema: The JSON body of the POST request must contain the tool's name and arguments. Example:

{
  "tool_name": "GetClaimData",
  "args": {
    "claim_id": "C123-456"
  }
}

Response Schema: The JSON body of the response must contain a result on success or an error on failure. Example:

{
  "result": {
    "status": "approved",
    "amount": 25000
  }
}

Security: All communication MUST be encrypted using TLS. Each request MUST be authenticated via a mechanism like a Bearer Token (JWT) or an API Key passed in the request header.

ToolClient Class: To simplify interaction, the Shared Agentic Toolkit will provide a ToolClient class. The Orchestrator will use this client to call a tool by its logical name. The client will handle the underlying complexity of making a secure, authenticated HTTP request to the correct microservice endpoint, including service discovery.

3.3. connectors Package
This package provides the building blocks for direct system interaction where a microservice is not necessary.

Purpose: To provide a library of robust, reusable, and unconfigured clients for connecting to common enterprise systems.

Functionality: It contains classes like FileNetClient and SQLDatabaseClient.

3.4. knowledge_interface Package
This package future-proofs the agent's ability to use semantic search.

Purpose: To provide a generic "adapter" that decouples the agent's core logic from any specific vector database technology.

Functionality: It defines a standard interface with methods like search and add_document.
