# LLM Papers Cheatsheet for Beginners and Experts 📚

<div align="center">
  <a href="https://github.com/ombharatiya/FAANG-Coding-Interview-Questions">
    <img src="https://img.shields.io/badge/🤖%20Essential%20LLM%20Papers-000000?style=for-the-badge" alt="Essential LLM Papers" />
  </a>
  
  <p><em>A curated collection of the most important research papers and whitepapers for understanding Large Language Models</em></p>
  
  <p>
    <a href="https://github.com/ombharatiya/FAANG-Coding-Interview-Questions/stargazers">
      <img src="https://img.shields.io/badge/⭐%20STAR%20THIS%20REPO-yellow?style=for-the-badge" alt="Star This Repo" />
    </a>
    <a href="https://github.com/ombharatiya?tab=followers">
      <img src="https://img.shields.io/badge/👨‍💻%20FOLLOW%20@OMBHARATIYA-39D353?style=for-the-badge" alt="Follow @ombharatiya" />
    </a>
  </p>
</div>

## Must-Read Foundational Papers for LLM Beginners

| Paper | Authors | Year | Key Contribution | Link |
|-------|---------|------|-----------------|------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Vaswani et al. | 2017 | Introduced the Transformer architecture | [PDF](https://arxiv.org/pdf/1706.03762.pdf) |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) | Devlin et al. | 2018 | Bidirectional encoders and masked language modeling | [PDF](https://arxiv.org/pdf/1810.04805.pdf) |
| [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165) | Brown et al. | 2020 | Scaling to 175B parameters and few-shot capabilities | [PDF](https://arxiv.org/pdf/2005.14165.pdf) |
| [Improving Language Understanding by Generative Pre-Training (GPT)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | Radford et al. | 2018 | First GPT model with generative pre-training | [PDF](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) |
| [Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | Radford et al. | 2019 | Zero-shot task transfer and larger scale | [PDF](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) |

## Training Methods and Scaling for Advanced Engineers

| Paper | Authors | Year | Key Contribution | Link |
|-------|---------|------|-----------------|------|
| [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155) | Ouyang et al. | 2022 | RLHF for aligning models with human intent | [PDF](https://arxiv.org/pdf/2203.02155.pdf) |
| [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311) | Chowdhery et al. | 2022 | Scaling to 540B parameters with Pathways architecture | [PDF](https://arxiv.org/pdf/2204.02311.pdf) |
| [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) | Kaplan et al. | 2020 | Mathematical relationships for model scaling | [PDF](https://arxiv.org/pdf/2001.08361.pdf) |
| [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239) | Thoppilan et al. | 2022 | Dialogue-focused pre-training methods | [PDF](https://arxiv.org/pdf/2201.08239.pdf) |
| [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) | Touvron et al. | 2023 | Efficient training of competitive open models | [PDF](https://arxiv.org/pdf/2302.13971.pdf) |
| [Chinchilla: Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) | Hoffmann et al. | 2022 | Optimal allocation of compute between model size and training tokens | [PDF](https://arxiv.org/pdf/2203.15556.pdf) |

## Safety, Ethics and Alignment Techniques

| Paper | Authors | Year | Key Contribution | Link |
|-------|---------|------|-----------------|------|
| [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) | Bai et al. | 2022 | Using AI feedback for safety training | [PDF](https://arxiv.org/pdf/2212.08073.pdf) |
| [Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286) | Perez et al. | 2022 | Automated adversarial testing | [PDF](https://arxiv.org/pdf/2202.03286.pdf) |
| [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) | Ouyang et al. | 2022 | RLHF methodology | [PDF](https://arxiv.org/pdf/2203.02155.pdf) |
| [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288) | Touvron et al. | 2023 | Safety improvements via RLHF | [PDF](https://arxiv.org/pdf/2307.09288.pdf) |
| [RLHF: Reinforcement Learning from Human Feedback](https://arxiv.org/abs/1706.03741) | Christiano et al. | 2017 | Original RLHF formulation | [PDF](https://arxiv.org/pdf/1706.03741.pdf) |

## Capabilities Evaluation and Benchmarking

| Paper | Authors | Year | Key Contribution | Link |
|-------|---------|------|-----------------|------|
| [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) | Bubeck et al. | 2023 | Analysis of emergent abilities | [PDF](https://arxiv.org/pdf/2303.12712.pdf) |
| [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300) | Hendrycks et al. | 2020 | Comprehensive LLM evaluation benchmark | [PDF](https://arxiv.org/pdf/2009.03300.pdf) |
| [Holistic Evaluation of Language Models (HELM)](https://arxiv.org/abs/2211.09110) | Liang et al. | 2022 | Framework for standardized LLM evaluation | [PDF](https://arxiv.org/pdf/2211.09110.pdf) |
| [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) | Wei et al. | 2022 | Improving reasoning through prompting | [PDF](https://arxiv.org/pdf/2201.11903.pdf) |
| [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682) | Wei et al. | 2022 | Analysis of capabilities that emerge with scale | [PDF](https://arxiv.org/pdf/2206.07682.pdf) |

## Latest Breakthroughs in LLM Research

| Paper | Authors | Year | Key Contribution | Link |
|-------|---------|------|-----------------|------|
| [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) | OpenAI | 2023 | Architecture and capabilities of GPT-4 | [PDF](https://arxiv.org/pdf/2303.08774.pdf) |
| [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805) | Google | 2023 | Multimodal capabilities and architectures | [PDF](https://arxiv.org/pdf/2312.11805.pdf) |
| [Claude: Scaling Language Modeling with PathFinder](https://arxiv.org/abs/2402.05158) | Anthropic | 2024 | Constitutional AI and RLHF advancements | [PDF](https://arxiv.org/pdf/2402.05158.pdf) |
| [Mistral 7B](https://arxiv.org/abs/2310.06825) | Mistral AI | 2023 | Efficient architecture and sliding window attention | [PDF](https://arxiv.org/pdf/2310.06825.pdf) |
| [Mixture-of-Experts (MoE)](https://arxiv.org/abs/2101.03961) | Fedus et al. | 2022 | Sparse expert models for efficient scaling | [PDF](https://arxiv.org/pdf/2101.03961.pdf) |

## Multimodal Models and Visual-Language Integration

| Paper | Authors | Year | Key Contribution | Link |
|-------|---------|------|-----------------|------|
| [CLIP: Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) | Radford et al. | 2021 | Connecting vision and language | [PDF](https://arxiv.org/pdf/2103.00020.pdf) |
| [DALL·E 2: Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) | Ramesh et al. | 2022 | Text-to-image generation | [PDF](https://arxiv.org/pdf/2204.06125.pdf) |
| [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198) | Alayrac et al. | 2022 | Few-shot visual learning | [PDF](https://arxiv.org/pdf/2204.14198.pdf) |
| [GPT-4V Technical Report](https://openai.com/research/gpt-4v-system-card) | OpenAI | 2023 | Multimodal capabilities | [Link](https://openai.com/research/gpt-4v-system-card) |
| [Multimodal Foundation Models: From Specialists to General-Purpose Assistants](https://arxiv.org/abs/2309.10020) | Driess et al. | 2023 | Survey of multimodal architectures | [PDF](https://arxiv.org/pdf/2309.10020.pdf) |

## Beginner's Learning Path: Where to Start with LLMs

If you're new to LLMs, here's the recommended order to approach these papers:

1. **Transformer Architecture**: "Attention Is All You Need"
2. **Pre-training Concept**: "Improving Language Understanding by Generative Pre-Training (GPT)"
3. **Scaling Properties**: "Scaling Laws for Neural Language Models"
4. **Alignment Techniques**: "Training language models to follow instructions with human feedback"
5. **Recent Architecture**: "LLaMA: Open and Efficient Foundation Language Models"
6. **Multimodal Integration**: "CLIP: Learning Transferable Visual Models From Natural Language Supervision"

## Resources for LLM Practitioners and Researchers

- [Hugging Face Model Hub](https://huggingface.co/models) - Repository of thousands of pre-trained models
- [Papers With Code](https://paperswithcode.com/task/language-modelling) - LLM papers with implementation code
- [LLM University by Databricks](https://www.databricks.com/resources/learn/training/llm-foundations-course) - Free comprehensive LLM course
- [State of AI Report](https://www.stateof.ai/) - Annual report on AI progress
- [Stanford CS324: Large Language Models](https://stanford-cs324.github.io/winter2022/) - University course materials

---

<div align="center">

### Connect With Me

If you found this resource helpful, please consider following me:

- **GitHub**: [@ombharatiya](https://github.com/ombharatiya)
- **Twitter**: [@ombharatiya](https://twitter.com/ombharatiya)
- **LinkedIn**: [ombharatiya](https://linkedin.com/in/ombharatiya)

I regularly share ML/AI resources, paper breakdowns, and career advice for tech professionals. Your support helps keep these resources updated!

</div> 