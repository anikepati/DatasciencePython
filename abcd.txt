Thank you for clarifying that you want the results from `run_online_evals` in `gen_ai_observer.py` to be saved in the Phoenix UI (`http://localhost:6006` or `https://arize-phoenix-ocp.nonprod.we.net`) so they are visible alongside traces, child spans, and other metadata. The current code is functional for tracing with both the HTTPS OCP endpoint (`https://arize-phoenix-ocp.nonprod.we.net/v1/traces`) and localhost (`http://localhost:6006/v1/traces`), supports singletons (`GenAIObserver`, `ToolSet`, `WorkflowManager`), OpenAI function calls with a custom proxy gateway, evaluations with custom headers, and displays span kind, LLM icons, and tool icons in the Phoenix UI (after recent fixes). However, the evaluation results from `run_online_evals` (e.g., `QA`, `Hallucination`, `Relevance`, `Toxicity`, and `ROUGE` scores) may not be fully persisting or visible in the Phoenix UI, possibly due to issues with how evaluations are logged or rendered.

### Why Evaluation Results May Not Be Saved in Phoenix
The `run_online_evals` method in `gen_ai_observer.py` runs evaluations using Phoenix’s `run_evals` function and logs them via `self.log_evaluation` and `self.log_rouge_evaluation`. Potential reasons why results aren’t visible in the Phoenix UI include:
1. **Logging Issue**: The `log_evaluation` and `log_rouge_evaluation` methods may fail silently (e.g., due to network issues, incorrect endpoint, or Phoenix client misconfiguration), causing evaluations to not persist.
2. **Phoenix UI Rendering**: Phoenix may expect specific evaluation formats or attributes (e.g., `context.span_id`, `score`, `label`, `explanation`) that aren’t fully aligned, or the UI may not display evaluations unless explicitly queried.
3. **Span Association**: Evaluations are tied to spans via `context.span_id`, but if the span IDs aren’t correctly linked or the Phoenix client isn’t syncing them properly, results may not appear.
4. **Retry Logic**: The `_retry_operation` method may fail after retries, and errors may not be logged clearly, hiding issues.
5. **Evaluation Data Format**: The DataFrame format or column names passed to `phoenix_client.log_evaluations` may not match Phoenix’s expectations.

### Solution
To ensure evaluation results from `run_online_evals` are saved and visible in the Phoenix UI:
- **Enhance Logging in `gen_ai_observer.py`**:
  - Improve error handling and logging in `log_evaluation` and `log_rouge_evaluation` to detect failures.
  - Verify that `phoenix_client.log_evaluations` is called correctly with `SpanEvaluations`.
- **Add Span Attributes for Evaluations**: Attach evaluation results as span attributes or events to make them visible in the trace view.
- **Validate Phoenix Client**: Ensure the Phoenix client is properly initialized and connected to the correct endpoint.
- **Maintain Features**: Preserve singleton pattern, OpenAI function calls with proxy gateway, online (10% sampling) and offline evaluations, child spans (`search`, `calculate`, `fetch_weather`, `get_stock_price`, `send_email`), custom annotations (`custom.annotation.*`), span kind, LLM/tool icons, and SSL bypass for OCP.
- **Remove Redundant Attributes**: As per your previous request, centralize span attribute/event setting in `gen_ai_observer.py` (already done in the `workflow` decorator), so no changes are needed in `main.py` for this.

### Updated .env Files
No changes needed, as the issue is code-level. For reference:
- **Localhost**:
  ```plaintext
  # .env.local
  OPENAI_API_KEY='sk-'
  OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
  OPENAI_PROXY_TOKEN='your-bearer-token'
  OTEL_EXPORTER_OTLP_ENDPOINT='http://localhost:6006/v1/traces'
  PHOENIX_COLLECTOR_ENDPOINT='http://localhost:6006'
  USE_AX_MODE=false
  ALLOW_INSECURE_CONNECTION=true
  ONLINE_SAMPLE_RATIO=0.1
  PHOENIX_PROJECT_NAME=default
  ```
- **OCP**:
  ```plaintext
  # .env.ocp
  OPENAI_API_KEY='sk-'
  OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
  OPENAI_PROXY_TOKEN='your-bearer-token'
  OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
  PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
  USE_AX_MODE=false
  ALLOW_INSECURE_CONNECTION=true
  ONLINE_SAMPLE_RATIO=0.1
  PHOENIX_PROJECT_NAME=default
  ```
Replace `'https://your-openai-proxy-gateway/api'` and `'your-bearer-token'` with your actual proxy endpoint and token.

### Updated Code
- **gen_ai_observer.py**: Enhance `run_online_evals`, `log_evaluation`, and `log_rouge_evaluation` to log evaluation results as span attributes/events, improve error handling, and ensure results are saved in Phoenix.
- **tools.py**: Unchanged, as it correctly applies decorators dynamically.
- **main.py**: Unchanged, as it’s optimized and attribute setting is centralized in `gen_ai_observer.py`.
- **requirements.txt**: Unchanged.

```python
# gen_ai_observer.py
import os
import logging
import random
import pandas as pd
from typing import Callable
import json
import httpx
import requests
import certifi
import socket
import threading
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as OTLPSpanExporterHTTP
from opentelemetry.trace import Status, StatusCode, SpanKind

try:
    from phoenix.otel import register as phoenix_register
    import phoenix as px
    from phoenix.trace import SpanEvaluations
except ImportError:
    phoenix_register = None
    px = None
    SpanEvaluations = None

try:
    from arize.otel import register as arize_register
    from arize.pandas.logger import Client as ArizeClient
except ImportError:
    arize_register = None
    ArizeClient = None

from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
    ToxicityEvaluator,
    OpenAIModel,
    run_evals,
)

from rouge_score import rouge_scorer

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Patch requests and httpx to bypass SSL verification when insecure
insecure = os.getenv('ALLOW_INSECURE_CONNECTION', 'false').lower() == 'true'
if insecure:
    logger.warning("Globally disabling SSL verification for requests and httpx")
    orig_request = requests.Session.request
    def patched_request(self, *args, **kwargs):
        kwargs['verify'] = False
        return orig_request(self, *args, **kwargs)
    requests.Session.request = patched_request
    try:
        import openai
        orig_openai_client = openai.OpenAI.__init__
        def patched_openai_init(self, *args, **kwargs):
            kwargs['http_client'] = httpx.Client(verify=False)
            orig_openai_client(self, *args, **kwargs)
        openai.OpenAI.__init__ = patched_openai_init
    except ImportError:
        logger.warning("openai package not installed; evaluations may fail")

custom_cert = os.getenv('CUSTOM_SSL_CERT_FILE', certifi.where())
os.environ['REQUESTS_CA_BUNDLE'] = custom_cert
logger.debug(f"SSL cert set: {custom_cert}")

class GenAIObserver:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.mode = 'ax' if os.getenv('USE_AX_MODE', 'false').lower() == 'true' else 'local'
        self.ui_endpoint = os.getenv('PHOENIX_COLLECTOR_ENDPOINT', 'http://localhost:6006/')
        self.otlp_endpoint = os.getenv('OTEL_EXPORTER_OTLP_ENDPOINT', self.ui_endpoint.rstrip('/') + '/v1/traces')
        self.insecure = os.getenv('ALLOW_INSECURE_CONNECTION', 'false').lower() == 'true'
        self.sample_rate = float(os.getenv('ONLINE_SAMPLE_RATIO', 0.1))
        logger.debug(f"Env config: mode={self.mode}, ui_endpoint={self.ui_endpoint}, otlp_endpoint={self.otlp_endpoint}, insecure={self.insecure}, sample_rate={self.sample_rate}")
        self._test_endpoint(self.otlp_endpoint)
        self.tracer_provider = self.setup_tracer()
        trace.set_tracer_provider(self.tracer_provider)
        self.tracer = trace.get_tracer(__name__)
        self.phoenix_client = None
        self.arize_client = None
        self.setup_client()
        self.stored_traces = []
        self.eval_model = self.setup_eval_model()
        self.evaluators = self.setup_evaluators()

    def _test_endpoint(self, endpoint):
        """Test if the OTLP endpoint is reachable."""
        try:
            import urllib.parse
            parsed = urllib.parse.urlparse(endpoint)
            host = parsed.hostname
            port = parsed.port or (443 if parsed.scheme == 'https' else 80)
            with socket.create_connection((host, port), timeout=2) as sock:
                logger.debug(f"Successfully connected to {host}:{port}")
        except Exception as e:
            logger.error(f"Failed to connect to {endpoint}: {e}")

    def setup_eval_model(self):
        """Set up OpenAIModel with custom OpenAI endpoint and headers."""
        try:
            from openai import OpenAI
        except ImportError:
            logger.error("openai package not installed; evaluations will fail without OPENAI_API_KEY")
            raise ImportError("openai package required for evaluations")

        logger.info("Using custom OpenAI endpoint for evaluations")
        openai_endpoint = os.getenv('OPENAI_API_ENDPOINT', 'https://api.openai.com')
        proxy_token = os.getenv('OPENAI_PROXY_TOKEN', os.getenv('OPENAI_API_KEY'))
        if not proxy_token:
            raise ValueError("OPENAI_API_KEY or OPENAI_PROXY_TOKEN must be set for evaluations")

        # Create custom OpenAI client with headers
        headers = {"Authorization": f"Bearer {proxy_token}"}
        http_client = httpx.Client(verify=not self.insecure, headers=headers)
        openai_client = OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=openai_endpoint,
            http_client=http_client
        )
        return OpenAIModel(model="gpt-4-turbo-preview", openai_client=openai_client)

    def determine_mode(self):
        return self.mode

    def setup_tracer(self):
        exporter_session_kwargs = {'session': httpx.Client(verify=not self.insecure)}
        if self.insecure and self.otlp_endpoint.startswith("https"):
            logger.warning("SSL verification is disabled for the OTLP HTTP exporter.")
            exporter_session_kwargs['session'] = httpx.Client(verify=False)

        if self.mode == 'ax':
            if not arize_register:
                raise ImportError("arize-otel not installed for AX mode")
            logger.info("Using AX mode exporter")
            return arize_register(
                space_id=os.getenv('ARIZE_SPACE_KEY'),
                api_key=os.getenv('ARIZE_API_KEY'),
                endpoint=self.otlp_endpoint,
            )
        else:
            project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            logger.debug(f"Setting up tracer for project: {project_name}")
            if phoenix_register:
                logger.info("Using Phoenix register for local mode")
                return phoenix_register(endpoint=self.otlp_endpoint, protocol='http/protobuf', project_name=project_name)
            else:
                logger.info(f"Using HTTP exporter with endpoint: {self.otlp_endpoint}")
                exporter = OTLPSpanExporterHTTP(
                    endpoint=self.otlp_endpoint,
                    **exporter_session_kwargs
                )
                processor = BatchSpanProcessor(exporter)
                tracer_provider = TracerProvider()
                tracer_provider.add_span_processor(processor)
                return tracer_provider

    def setup_client(self):
        if self.mode == 'ax':
            if not ArizeClient:
                raise ImportError("arize not installed for AX mode")
            space_id = os.getenv('ARIZE_SPACE_KEY') or os.getenv('ARIZE_SPACE_ID')
            api_key = os.getenv('ARIZE_API_KEY')
            self.project_name = os.getenv('PHOENIX_PROJECT_NAME', 'GenAIObserver')
            self.arize_client = ArizeClient(api_key=api_key, space_id=space_id)
            logger.info("Using Arize client for AX mode")
        else:
            if px is None:
                raise ImportError("phoenix not installed for local mode")
            self.project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            self.phoenix_client = px.Client(endpoint=self.ui_endpoint)
            logger.info("Using Phoenix client for local mode")

    def setup_evaluators(self):
        return [
            QAEvaluator(self.eval_model),
            HallucinationEvaluator(self.eval_model),
            RelevanceEvaluator(self.eval_model),
            ToxicityEvaluator(self.eval_model),
        ]

    def _retry_operation(self, operation: Callable, max_retries: int = 3) -> bool:
        for attempt in range(max_retries):
            try:
                operation()
                logger.debug(f"Operation succeeded on attempt {attempt + 1}")
                return True
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt == max_retries - 1:
                    logger.error(f"Max retries exceeded for operation: {e}")
                    return False
        return False

    def _add_score_if_missing(self, eval_df: pd.DataFrame, eval_name: str) -> pd.DataFrame:
        eval_df = eval_df.copy()
        if 'score' not in eval_df.columns:
            if eval_name == 'QA':
                eval_df['score'] = (eval_df['label'] == 'correct').astype(float)
            elif eval_name == 'Hallucination':
                eval_df['score'] = (eval_df['label'] == 'factual').astype(float)
            elif eval_name == 'Relevance':
                eval_df['score'] = (eval_df['label'] == 'relevant').astype(float)
            elif eval_name == 'Toxicity':
                eval_df['score'] = (eval_df['label'] == 'non-toxic').astype(float)
        return eval_df

    def log_evaluation(self, eval_df: pd.DataFrame, eval_name: str, span_id: str) -> bool:
        eval_df = self._add_score_if_missing(eval_df, eval_name)
        eval_df = eval_df.rename_axis("context.span_id")
        logger.debug(f"Logging evaluation {eval_name} for span_id {span_id}: {eval_df.to_dict()}")
        if self.mode == 'local' and self.phoenix_client:
            success = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name=eval_name, dataframe=eval_df)
            ))
            if success:
                # Add evaluation results as span attributes
                span_context = trace.get_span_context(span_id)
                if span_context:
                    with self.tracer.start_as_current_span("evaluation", context=span_context) as span:
                        span.set_attribute(f"eval.{eval_name}.score", eval_df.get('score', [None])[0])
                        span.set_attribute(f"eval.{eval_name}.label", eval_df.get('label', [None])[0])
                        span.set_attribute(f"eval.{eval_name}.explanation", eval_df.get('explanation', [None])[0])
                        span.add_event(f"eval_{eval_name}", {
                            "score": eval_df.get('score', [None])[0],
                            "label": eval_df.get('label', [None])[0],
                            "explanation": eval_df.get('explanation', [None])[0]
                        })
                return success
            else:
                logger.error(f"Failed to log {eval_name} evaluation for span_id {span_id}")
                return False
        elif self.mode == 'ax' and self.arize_client:
            eval_df_log = eval_df[['label', 'score', 'explanation']].copy()
            prefix = f'eval.{eval_name}.'
            eval_df_log = eval_df_log.rename(columns={
                'label': prefix + 'label',
                'score': prefix + 'score',
                'explanation': prefix + 'explanation',
            })
            success = self._retry_operation(lambda: self.arize_client.log_evaluations_sync(eval_df_log, self.project_name))
            if success:
                # Add evaluation results as span attributes
                span_context = trace.get_span_context(span_id)
                if span_context:
                    with self.tracer.start_as_current_span("evaluation", context=span_context) as span:
                        span.set_attribute(f"eval.{eval_name}.score", eval_df.get('score', [None])[0])
                        span.set_attribute(f"eval.{eval_name}.label", eval_df.get('label', [None])[0])
                        span.set_attribute(f"eval.{eval_name}.explanation", eval_df.get('explanation', [None])[0])
                        span.add_event(f"eval_{eval_name}", {
                            "score": eval_df.get('score', [None])[0],
                            "label": eval_df.get('label', [None])[0],
                            "explanation": eval_df.get('explanation', [None])[0]
                        })
                return success
            else:
                logger.error(f"Failed to log {eval_name} evaluation for span_id {span_id}")
                return False
        else:
            logger.warning(f"Cannot log evaluation: {self.mode} client not available")
            return False

    def log_rouge_evaluation(self, rouge_df: pd.DataFrame, offline: bool, span_id: str) -> bool:
        rouge_df = rouge_df.set_index("context.span_id")
        logger.debug(f"Logging ROUGE evaluation for span_id {span_id}: {rouge_df.to_dict()}")
        logged = True
        if self.mode == 'local' and self.phoenix_client:
            rouge1_df = rouge_df[['score_rouge1']].rename(columns={'score_rouge1': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGE1", dataframe=rouge1_df)
            )) and logged
            rougeL_df = rouge_df[['score_rougeL']].rename(columns={'score_rougeL': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGEL", dataframe=rougeL_df)
            )) and logged
            if logged:
                # Add ROUGE results as span attributes
                span_context = trace.get_span_context(span_id)
                if span_context:
                    with self.tracer.start_as_current_span("rouge_evaluation", context=span_context) as span:
                        span.set_attribute("eval.ROUGE1.score", rouge_df.get('score_rouge1', [None])[0])
                        span.set_attribute("eval.ROUGEL.score", rouge_df.get('score_rougeL', [None])[0])
                        span.add_event("rouge_eval", {
                            "ROUGE1_score": rouge_df.get('score_rouge1', [None])[0],
                            "ROUGEL_score": rouge_df.get('score_rougeL', [None])[0]
                        })
            else:
                logger.error(f"Failed to log ROUGE evaluation for span_id {span_id}")
        elif self.mode == 'ax' and self.arize_client:
            for metric in ['rouge1', 'rougeL']:
                metric_df = rouge_df[[f'score_{metric}']].rename(columns={f'score_{metric}': f'eval.ROUGE_{metric.upper()}.score'})
                logged = self._retry_operation(lambda df=metric_df: self.arize_client.log_evaluations_sync(df, self.project_name)) and logged
            if logged:
                # Add ROUGE results as span attributes
                span_context = trace.get_span_context(span_id)
                if span_context:
                    with self.tracer.start_as_current_span("rouge_evaluation", context=span_context) as span:
                        span.set_attribute("eval.ROUGE1.score", rouge_df.get('score_rouge1', [None])[0])
                        span.set_attribute("eval.ROUGEL.score", rouge_df.get('score_rougeL', [None])[0])
                        span.add_event("rouge_eval", {
                            "ROUGE1_score": rouge_df.get('score_rouge1', [None])[0],
                            "ROUGEL_score": rouge_df.get('score_rougeL', [None])[0]
                        })
            else:
                logger.error(f"Failed to log ROUGE evaluation for span_id {span_id}")
        if offline:
            logger.info(f"Offline ROUGE evals: {rouge_df.reset_index().to_dict(orient='records')}")
        else:
            logger.info(f"Online ROUGE eval: {rouge_df.reset_index().to_dict(orient='records')[0]}")
        return logged

    def workflow(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span("workflow", kind=SpanKind.INTERNAL) as span:
                span.set_attribute("span_type", "llm_workflow")
                span.set_attribute("llm.model", "gpt-4-turbo-preview")
                span.set_attribute("llm.prompt", args[0] if args else "")
                span.set_attribute("input.value", args[0] if args else "")
                span.set_attribute("custom.annotation.prompt", args[0] if args else "")
                span.set_attribute("custom.annotation.reference", args[1] if len(args) > 1 else "")
                span.add_event("prompt", {"value": args[0] if args else ""})
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("llm.response", result)
                    span.set_attribute("output.value", result)
                    span.set_attribute("custom.annotation.output", result)
                    span.add_event("response", {"value": result})
                    span.set_status(Status(StatusCode.OK))
                    span_id = str(span.get_span_context().span_id)
                    self.stored_traces.append({
                        'span_id': span_id,
                        'input': args[0] if args else "",
                        'reference': args[1] if len(args) > 1 else "",
                        'output': result,
                    })
                    if random.random() < self.sample_rate:
                        self.run_online_evals(span_id, args[0] if args else "", args[1] if len(args) > 1 else "", result)
                    return result
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"Workflow error: {e}")
                    raise
        return wrapper

    def tool_span(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span(func.__name__, kind=SpanKind.INTERNAL) as span:
                span.set_attribute("span_type", "tool")
                span.set_attribute("tool.name", func.__name__)
                span.set_attribute("input.value", str(args))
                span.set_attribute("custom.annotation.tool_input", str(args))
                span.add_event("tool_input", {"value": str(args)})
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("output.value", str(result))
                    span.set_attribute("custom.annotation.tool_output", str(result))
                    span.add_event("tool_output", {"value": str(result)})
                    span.set_status(Status(StatusCode.OK))
                    return result
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"Tool error in {func.__name__}: {e}")
                    raise
        return wrapper

    def run_online_evals(self, span_id, input_text, reference, output):
        df = pd.DataFrame([{
            "input": input_text,
            "output": output,
            "reference": reference,
            "context": reference,
        }], index=[span_id])
        df.index.name = "context.span_id"
        logger.debug(f"Running online evaluations for span_id {span_id}: {df.to_dict()}")
        try:
            eval_dfs = run_evals(
                dataframe=df,
                evaluators=self.evaluators,
                provide_explanation=True,
            )
            for eval_df, evaluator in zip(eval_dfs, self.evaluators):
                eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
                if self.log_evaluation(eval_df, eval_name, span_id):
                    logger.info(f"Online {eval_name} eval logged successfully for span_id {span_id}")
                else:
                    logger.warning(f"Failed to log online {eval_name} eval for span_id {span_id}")
            self.run_rouge_eval(df, offline=False, span_id=span_id)
        except Exception as e:
            logger.error(f"Error running online evaluations for span_id {span_id}: {e}")
            raise

    def run_offline_evals(self):
        if not self.stored_traces:
            logger.info("No stored traces for offline evaluations")
            return
        df = pd.DataFrame(self.stored_traces)
        df["context"] = df["reference"]
        df = df.set_index("span_id")
        df.index.name = "context.span_id"
        logger.debug(f"Running offline evaluations: {df.to_dict()}")
        try:
            eval_dfs = run_evals(
                dataframe=df,
                evaluators=self.evaluators,
                provide_explanation=True,
            )
            for eval_df, evaluator in zip(eval_dfs, self.evaluators):
                eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
                for span_id in eval_df.index:
                    if self.log_evaluation(eval_df.loc[[span_id]], eval_name, span_id):
                        logger.info(f"Offline {eval_name} eval logged successfully for span_id {span_id}")
                    else:
                        logger.warning(f"Failed to log offline {eval_name} eval for span_id {span_id}")
            self.run_rouge_eval(df, offline=True)
        except Exception as e:
            logger.error(f"Error running offline evaluations: {e}")
            raise

    def run_rouge_eval(self, df, offline=False, span_id=None):
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
        rouge_data = []
        for span_id_row, row in df.iterrows():
            pred = row["output"]
            ref = row["reference"]
            scores = scorer.score(ref, pred)
            rouge_data.append({
                "span_id": span_id_row,
                "score_rouge1": scores["rouge1"].fmeasure,
                "score_rougeL": scores["rougeL"].fmeasure,
            })
        rouge_df = pd.DataFrame(rouge_data)
        if offline:
            for _, row in rouge_df.iterrows():
                self.log_rouge_evaluation(rouge_df[rouge_df['span_id'] == row['span_id']], offline=True, span_id=row['span_id'])
        else:
            self.log_rouge_evaluation(rouge_df, offline=False, span_id=span_id)

    def shutdown(self):
        if hasattr(self, 'tracer_provider'):
            self.tracer_provider.shutdown()
        logger.info("Tracer shutdown complete")
```

```python
# tools.py
import threading
import json
from genai_observer import GenAIObserver

class ToolSet:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.observer = GenAIObserver.get_instance()
        # Dynamically apply tool_span decorator to methods
        self.search = self.tool_span(self.search)
        self.calculate = self.tool_span(self.calculate)
        self.fetch_weather = self.tool_span(self.fetch_weather)
        self.get_stock_price = self.tool_span(self.get_stock_price)
        self.send_email = self.tool_span(self.send_email)

    def tool_span(self, func):
        """Function decorator to apply observer.tool_span."""
        return self.observer.tool_span(func)

    def search(self, query: str) -> str:
        """Simulates a web search."""
        return f"Mock search result for: {query}"

    def calculate(self, expression: str) -> float:
        """Simulates a computation."""
        try:
            return eval(expression)
        except Exception as e:
            return f"Error: {e}"

    def fetch_weather(self, city: str) -> str:
        """Simulates an API call for weather."""
        return f"Mock weather in {city}: Sunny, 75°F"

    def get_stock_price(self, ticker: str) -> str:
        """Simulates fetching a stock price for a given ticker."""
        return f"Mock stock price for {ticker}: $100.00"

    def send_email(self, recipient: str, subject: str, body: str) -> str:
        """Simulates sending an email."""
        return f"Mock email sent to {recipient} with subject '{subject}' and body '{body}'"

    def get_tool_schemas(self):
        """Returns JSON schemas for OpenAI function calling."""
        return [
            {
                "name": "search",
                "description": "Perform a web search with a given query.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "The search query"}
                    },
                    "required": ["query"]
                }
            },
            {
                "name": "calculate",
                "description": "Evaluate a mathematical expression.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "expression": {"type": "string", "description": "The mathematical expression to evaluate"}
                    },
                    "required": ["expression"]
                }
            },
            {
                "name": "fetch_weather",
                "description": "Fetch the weather for a given city.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {"type": "string", "description": "The city name"}
                    },
                    "required": ["city"]
                }
            },
            {
                "name": "get_stock_price",
                "description": "Fetch the stock price for a given ticker symbol.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "ticker": {"type": "string", "description": "The stock ticker symbol"}
                    },
                    "required": ["ticker"]
                }
            },
            {
                "name": "send_email",
                "description": "Send an email to a recipient with a subject and body.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "recipient": {"type": "string", "description": "The email address of the recipient"},
                        "subject": {"type": "string", "description": "The subject of the email"},
                        "body": {"type": "string", "description": "The body of the email"}
                    },
                    "required": ["recipient", "subject", "body"]
                }
            }
        ]
```

```python
# main.py
import json
import threading
from dotenv import load_dotenv
import openai
from genai_observer import GenAIObserver
import tools
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode, SpanKind

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

load_dotenv()

class WorkflowManager:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.observer = GenAIObserver.get_instance()
        self.tool_set = tools.ToolSet.get_instance()
        openai_endpoint = os.getenv('OPENAI_API_ENDPOINT', 'https://api.openai.com')
        proxy_token = os.getenv('OPENAI_PROXY_TOKEN', os.getenv('OPENAI_API_KEY'))
        if not proxy_token:
            raise ValueError("OPENAI_API_KEY or OPENAI_PROXY_TOKEN must be set for OpenAI client")
        headers = {"Authorization": f"Bearer {proxy_token}"}
        http_client = httpx.Client(verify=not self.observer.insecure, headers=headers)
        self.openai_client = openai.OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=openai_endpoint,
            http_client=http_client
        )
        # Dynamically apply workflow decorator
        self.run_workflow = self.workflow(self.run_workflow)

    def workflow(self, func):
        """Function decorator to apply observer.workflow."""
        return self.observer.workflow(func)

    def run_workflow(self, prompt: str, reference: str) -> str:
        """Run the LLM workflow with OpenAI function calls."""
        try:
            messages = [{"role": "user", "content": prompt}]
            tools = self.tool_set.get_tool_schemas()

            # Make OpenAI API call with function tools
            with self.observer.tracer.start_as_current_span("openai_call", kind=SpanKind.CLIENT) as span:
                span.set_attribute("span_type", "llm")
                span.set_attribute("llm.model", "gpt-4-turbo-preview")
                span.set_attribute("input.value", prompt)
                span.add_event("llm_prompt", {"value": prompt})
                try:
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4-turbo-preview",
                        messages=messages,
                        tools=[{"type": "function", "function": tool} for tool in tools],
                        tool_choice="auto"
                    )
                    span.set_attribute("output.value", str(response))
                    span.add_event("llm_response", {"value": str(response)})
                    span.set_status(Status(StatusCode.OK))
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"OpenAI API error: {e}")
                    raise

            # Process tool calls
            tool_calls = response.choices[0].message.tool_calls
            results = []
            if tool_calls:
                for tool_call in tool_calls:
                    func_name = tool_call.function.name
                    args = json.loads(tool_call.function.arguments)
                    try:
                        if func_name == "search":
                            result = self.tool_set.search(args["query"])
                        elif func_name == "calculate":
                            result = self.tool_set.calculate(args["expression"])
                        elif func_name == "fetch_weather":
                            result = self.tool_set.fetch_weather(args["city"])
                        elif func_name == "get_stock_price":
                            result = self.tool_set.get_stock_price(args["ticker"])
                        elif func_name == "send_email":
                            result = self.tool_set.send_email(args["recipient"], args["subject"], args["body"])
                        else:
                            result = f"Unknown function: {func_name}"
                        results.append(f"{func_name}: {result}")
                    except Exception as e:
                        results.append(f"{func_name}: Error - {str(e)}")
                        logger.error(f"Tool call error for {func_name}: {e}")

            # Generate final response
            response_text = f"Based on tools:\n{'\n'.join(results)}\nAnswering prompt: {prompt}"
            return response_text
        except Exception as e:
            logger.error(f"Workflow error: {e}")
            raise

    def run(self, prompt: str, reference: str):
        """Execute the workflow and evaluations."""
        result = self.run_workflow(prompt, reference)
        print("Workflow Result:")
        print(result)
        self.observer.run_offline_evals()
        self.observer.shutdown()

if __name__ == "__main__":
    # Example run with mock prompt and reference
    workflow_manager = WorkflowManager.get_instance()
    prompt = "What is the latest in AI? Also, get the stock price for AAPL, calculate 40 * 2.5, and check the weather in San Francisco."
    reference = "The latest in AI includes advancements in observability frameworks like Arize Phoenix."
    workflow_manager.run(prompt, reference)
```

```python
# requirements.txt
arize-phoenix==0.33.0
arize-otel
arize
opentelemetry-sdk
opentelemetry-exporter-otlp
evaluate
rouge_score
python-dotenv
requests
httpx
protobuf
grpcio
openai>=1.0.0
certifi
```

### Steps to Test
1. **Update .env Files**:
   - **Localhost**:
     ```plaintext
     # .env.local
     OPENAI_API_KEY='sk-'
     OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
     OPENAI_PROXY_TOKEN='your-bearer-token'
     OTEL_EXPORTER_OTLP_ENDPOINT='http://localhost:6006/v1/traces'
     PHOENIX_COLLECTOR_ENDPOINT='http://localhost:6006'
     USE_AX_MODE=false
     ALLOW_INSECURE_CONNECTION=true
     ONLINE_SAMPLE_RATIO=0.1
     PHOENIX_PROJECT_NAME=default
     ```
   - **OCP**:
     ```plaintext
     # .env.ocp
     OPENAI_API_KEY='sk-'
     OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
     OPENAI_PROXY_TOKEN='your-bearer-token'
     OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
     PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
     USE_AX_MODE=false
     ALLOW_INSECURE_CONNECTION=true
     ONLINE_SAMPLE_RATIO=0.1
     PHOENIX_PROJECT_NAME=default
     ```
   - Replace `'https://your-openai-proxy-gateway/api'` and `'your-bearer-token'` with your actual proxy endpoint and token.

2. **Install Dependencies**:
   ```
   pip install -r requirements.txt
   ```

3. **Test Localhost**:
   - Start Phoenix server:
     ```
     python -m phoenix.server.main serve
     ```
     - Confirm logs: `Server running at http://localhost:6006`.
   - Load `.env.local`:
     ```
     export $(cat .env.local | xargs)
     python main.py
     ```
   - Check logs: Look for `Online <eval_name> eval logged successfully for span_id <span_id>` and no errors like `Failed to log evaluation`.
   - Verify in Phoenix UI (`http://localhost:6006`):
     - Navigate to the Evaluations tab or Trace view.
     - Confirm evaluation results (`QA`, `Hallucination`, `Relevance`, `Toxicity`, `ROUGE1`, `ROUGEL`) are visible for spans, including `score`, `label`, and `explanation`.
     - Verify child spans (`search`, `calculate`, `fetch_weather`, `get_stock_price`, `send_email`), span kind (`INTERNAL`, `CLIENT`), LLM icons (`workflow`, `openai_call`), and tool icons.

4. **Test OCP**:
   - Load `.env.ocp`:
     ```
     export $(cat .env.ocp | xargs)
     python main.py
     ```
   - Check UI: `https://arize-phoenix-ocp.nonprod.we.net`. Confirm evaluation results, span kind, LLM icons, and tool icons.

5. **Check Logs**:
   - For localhost: Expect `Successfully connected to localhost:6006`, `Online <eval_name> eval logged successfully`, no errors.
   - For OCP: Expect `WARNING: SSL verification is disabled for the OTLP HTTP exporter.`, no `SSLCertVerificationError`.
   - Verify OpenAI function calls and evaluations use the proxy endpoint with custom headers.
   - Confirm evaluation results in the UI under the Evaluations tab or as span attributes/events.

### If Issues Persist
- **Evaluation Results Not Visible**:
  - Check Phoenix UI for an Evaluations tab or specific view for `SpanEvaluations`.
  - Share screenshots of the UI showing where evaluations should appear.
  - Verify logs for `Online <eval_name> eval logged successfully` or errors like `Failed to log evaluation`.
- **Proxy Gateway**: Test the proxy endpoint (`curl -H "Authorization: Bearer your-bearer-token" https://your-openai-proxy-gateway/api`) and share any errors.
- **Localhost**: Ensure `python -m phoenix.server.main serve` runs (`curl http://localhost:6006/v1/traces`). Check port conflicts (`lsof -i :6006`).
- **OCP**: Test with `curl -k https://arize-phoenix-ocp.nonprod.we.net/v1/traces` if issues reappear.
- **Debug**: Share the full traceback, logs (especially from `_retry_operation` or `log_evaluation`), or UI screenshots if evaluation results are missing.

For UAT/prod, use `USE_AX_MODE=true` with ARIZE keys—no SSL issues with cloud endpoints. Please share the results, any errors, or UI screenshots to help diagnose further!
