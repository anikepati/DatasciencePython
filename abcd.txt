The `TypeError: 'Property' object is not callable` error occurs in `tools.py` because the `@tool_span` decorator is applied as a property (`@property def tool_span`) in the `ToolSet` class, making `tool_span` a property object rather than a callable function. When you try to use `@tool_span` on methods like `search`, `calculate`, etc., Python attempts to call the property object, which isn’t callable, causing the error.

This issue stems from the recent change to simplify `ToolSet` by moving `observer = GenAIObserver.get_instance()` to the constructor and using `@self.observer.tool_span` directly. The `@property` decorator on `tool_span` was intended to provide access to `self.observer.tool_span`, but it prevents the decorator from being applied correctly.

### Why This Happens
- In `tools.py`, the `tool_span` property is defined as:
  ```python
  @property
  def tool_span(self):
      return self.observer.tool_span
  ```
  This makes `tool_span` a property that returns the `tool_span` method from `GenAIObserver`, but properties are not callable as decorators.
- When `@tool_span` is applied to a method (e.g., `@tool_span def search(...)`), Python tries to call the property object, resulting in the `TypeError`.
- The previous version used inner `wrapped_*` functions to apply the decorator dynamically, avoiding this issue, but we simplified it to reduce code duplication.

### Solution
To fix this, we need to make `tool_span` a callable decorator method in `ToolSet` instead of a property. We can:
- Remove the `@property` decorator and directly use `self.observer.tool_span` in method definitions.
- Alternatively, keep the property but access `self.observer.tool_span` directly in each method’s decorator.
- The simplest approach is to remove the `tool_span` property and apply `@self.observer.tool_span` directly to each method, ensuring the decorator is callable.

This change will maintain all functionality: singleton pattern for `GenAIObserver`, `ToolSet`, and `WorkflowManager`; OpenAI function calls; online (10% sampling) and offline evaluations; child spans (`search`, `calculate`, `fetch_weather`, `get_stock_price`, `send_email`) attached to the parent workflow span; custom annotations (`custom.annotation.*`); and SSL bypass for the OCP endpoint (`https://arize-phoenix-ocp.nonprod.we.net/v1/traces`). The code is already working for both localhost (`http://localhost:6006/v1/traces`) and OCP, with trace details (status, kind, input/output messages) visible in the Phoenix UI.

### Updated .env Files
No changes needed, as the issue is code-level. For reference:
- **Localhost**:
  ```plaintext
  # .env.local
  OPENAI_API_KEY='sk-'
  OTEL_EXPORTER_OTLP_ENDPOINT='http://localhost:6006/v1/traces'
  PHOENIX_COLLECTOR_ENDPOINT='http://localhost:6006'
  USE_AX_MODE=false
  ALLOW_INSECURE_CONNECTION=true
  ONLINE_SAMPLE_RATIO=0.1
  PHOENIX_PROJECT_NAME=default
  ```
- **OCP**:
  ```plaintext
  # .env.ocp
  OPENAI_API_KEY='sk-'
  OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
  PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
  USE_AX_MODE=false
  ALLOW_INSECURE_CONNECTION=true
  ONLINE_SAMPLE_RATIO=0.1
  PHOENIX_PROJECT_NAME=default
  ```

### Updated Code
- **gen_ai_observer.py**: Unchanged, as it’s working correctly.
- **tools.py**: Remove the `tool_span` property and apply `@self.observer.tool_span` directly to each method.
- **main.py**: Unchanged, as it’s already optimized with a single `run_workflow` function.
- **requirements.txt**: Unchanged.

```python
# gen_ai_observer.py
import os
import logging
import random
import pandas as pd
from typing import Callable
import json
import httpx
import requests
import certifi
import socket
import threading
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as OTLPSpanExporterHTTP
from opentelemetry.trace import Status, StatusCode

try:
    from phoenix.otel import register as phoenix_register
    import phoenix as px
    from phoenix.trace import SpanEvaluations
except ImportError:
    phoenix_register = None
    px = None
    SpanEvaluations = None

try:
    from arize.otel import register as arize_register
    from arize.pandas.logger import Client as ArizeClient
except ImportError:
    arize_register = None
    ArizeClient = None

from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
    ToxicityEvaluator,
    OpenAIModel,
    run_evals,
)

from rouge_score import rouge_scorer

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Patch requests and httpx to bypass SSL verification when insecure
insecure = os.getenv('ALLOW_INSECURE_CONNECTION', 'false').lower() == 'true'
if insecure:
    logger.warning("Globally disabling SSL verification for requests and httpx")
    orig_request = requests.Session.request
    def patched_request(self, *args, **kwargs):
        kwargs['verify'] = False
        return orig_request(self, *args, **kwargs)
    requests.Session.request = patched_request
    try:
        import openai
        orig_openai_client = openai.OpenAI.__init__
        def patched_openai_init(self, *args, **kwargs):
            kwargs['http_client'] = httpx.Client(verify=False)
            orig_openai_client(self, *args, **kwargs)
        openai.OpenAI.__init__ = patched_openai_init
    except ImportError:
        logger.warning("openai package not installed; evaluations may fail")

custom_cert = os.getenv('CUSTOM_SSL_CERT_FILE', certifi.where())
os.environ['REQUESTS_CA_BUNDLE'] = custom_cert
logger.debug(f"SSL cert set: {custom_cert}")

class GenAIObserver:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.mode = 'ax' if os.getenv('USE_AX_MODE', 'false').lower() == 'true' else 'local'
        self.ui_endpoint = os.getenv('PHOENIX_COLLECTOR_ENDPOINT', 'http://localhost:6006/')
        self.otlp_endpoint = os.getenv('OTEL_EXPORTER_OTLP_ENDPOINT', self.ui_endpoint.rstrip('/') + '/v1/traces')
        self.insecure = os.getenv('ALLOW_INSECURE_CONNECTION', 'false').lower() == 'true'
        self.sample_rate = float(os.getenv('ONLINE_SAMPLE_RATIO', 0.1))
        logger.debug(f"Env config: mode={self.mode}, ui_endpoint={self.ui_endpoint}, otlp_endpoint={self.otlp_endpoint}, insecure={self.insecure}, sample_rate={self.sample_rate}")
        self._test_endpoint(self.otlp_endpoint)
        self.tracer_provider = self.setup_tracer()
        trace.set_tracer_provider(self.tracer_provider)
        self.tracer = trace.get_tracer(__name__)
        self.phoenix_client = None
        self.arize_client = None
        self.setup_client()
        self.stored_traces = []
        self.eval_model = self.setup_eval_model()
        self.evaluators = self.setup_evaluators()

    def _test_endpoint(self, endpoint):
        """Test if the OTLP endpoint is reachable."""
        try:
            import urllib.parse
            parsed = urllib.parse.urlparse(endpoint)
            host = parsed.hostname
            port = parsed.port or (443 if parsed.scheme == 'https' else 80)
            with socket.create_connection((host, port), timeout=2) as sock:
                logger.debug(f"Successfully connected to {host}:{port}")
        except Exception as e:
            logger.error(f"Failed to connect to {endpoint}: {e}")

    def setup_eval_model(self):
        """Set up OpenAIModel with direct OpenAI API."""
        try:
            from openai import OpenAI
        except ImportError:
            logger.error("openai package not installed; evaluations will fail without OPENAI_API_KEY")
            raise ImportError("openai package required for evaluations")
        
        logger.info("Using direct OpenAI API for evaluations")
        return OpenAIModel(model="gpt-4-turbo-preview")

    def determine_mode(self):
        return self.mode

    def setup_tracer(self):
        exporter_session_kwargs = {'session': httpx.Client(verify=not self.insecure)}
        if self.insecure and self.otlp_endpoint.startswith("https"):
            logger.warning("SSL verification is disabled for the OTLP HTTP exporter.")
            exporter_session_kwargs['session'] = httpx.Client(verify=False)

        if self.mode == 'ax':
            if not arize_register:
                raise ImportError("arize-otel not installed for AX mode")
            logger.info("Using AX mode exporter")
            return arize_register(
                space_id=os.getenv('ARIZE_SPACE_KEY'),
                api_key=os.getenv('ARIZE_API_KEY'),
                endpoint=self.otlp_endpoint,
            )
        else:
            project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            logger.debug(f"Setting up tracer for project: {project_name}")
            if phoenix_register:
                logger.info("Using Phoenix register for local mode")
                return phoenix_register(endpoint=self.otlp_endpoint, protocol='http/protobuf', project_name=project_name)
            else:
                logger.info(f"Using HTTP exporter with endpoint: {self.otlp_endpoint}")
                exporter = OTLPSpanExporterHTTP(
                    endpoint=self.otlp_endpoint,
                    **exporter_session_kwargs
                )
                processor = BatchSpanProcessor(exporter)
                tracer_provider = TracerProvider()
                tracer_provider.add_span_processor(processor)
                return tracer_provider

    def setup_client(self):
        if self.mode == 'ax':
            if not ArizeClient:
                raise ImportError("arize not installed for AX mode")
            space_id = os.getenv('ARIZE_SPACE_KEY') or os.getenv('ARIZE_SPACE_ID')
            api_key = os.getenv('ARIZE_API_KEY')
            self.project_name = os.getenv('PHOENIX_PROJECT_NAME', 'GenAIObserver')
            self.arize_client = ArizeClient(api_key=api_key, space_id=space_id)
            logger.info("Using Arize client for AX mode")
        else:
            if px is None:
                raise ImportError("phoenix not installed for local mode")
            self.project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            self.phoenix_client = px.Client(endpoint=self.ui_endpoint)
            logger.info("Using Phoenix client for local mode")

    def setup_evaluators(self):
        return [
            QAEvaluator(self.eval_model),
            HallucinationEvaluator(self.eval_model),
            RelevanceEvaluator(self.eval_model),
            ToxicityEvaluator(self.eval_model),
        ]

    def _retry_operation(self, operation: Callable, max_retries: int = 3) -> bool:
        for attempt in range(max_retries):
            try:
                operation()
                return True
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt == max_retries - 1:
                    logger.error("Max retries exceeded for operation")
                    return False
        return False

    def _add_score_if_missing(self, eval_df: pd.DataFrame, eval_name: str) -> pd.DataFrame:
        eval_df = eval_df.copy()
        if 'score' not in eval_df.columns:
            if eval_name == 'QA':
                eval_df['score'] = (eval_df['label'] == 'correct').astype(float)
            elif eval_name == 'Hallucination':
                eval_df['score'] = (eval_df['label'] == 'factual').astype(float)
            elif eval_name == 'Relevance':
                eval_df['score'] = (eval_df['label'] == 'relevant').astype(float)
            elif eval_name == 'Toxicity':
                eval_df['score'] = (eval_df['label'] == 'non-toxic').astype(float)
        return eval_df

    def log_evaluation(self, eval_df: pd.DataFrame, eval_name: str) -> bool:
        eval_df = self._add_score_if_missing(eval_df, eval_name)
        eval_df = eval_df.rename_axis("context.span_id")
        if self.mode == 'local' and self.phoenix_client:
            return self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name=eval_name, dataframe=eval_df)
            ))
        elif self.mode == 'ax' and self.arize_client:
            eval_df_log = eval_df[['label', 'score', 'explanation']].copy()
            prefix = f'eval.{eval_name}.'
            eval_df_log = eval_df_log.rename(columns={
                'label': prefix + 'label',
                'score': prefix + 'score',
                'explanation': prefix + 'explanation',
            })
            return self._retry_operation(lambda: self.arize_client.log_evaluations_sync(eval_df_log, self.project_name))
        else:
            logger.warning(f"Cannot log evaluation: {self.mode} client not available")
            return False

    def log_rouge_evaluation(self, rouge_df: pd.DataFrame, offline: bool = False) -> bool:
        rouge_df = rouge_df.set_index("span_id").rename_axis("context.span_id")
        logged = True
        if self.mode == 'local' and self.phoenix_client:
            rouge1_df = rouge_df[['score_rouge1']].rename(columns={'score_rouge1': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGE1", dataframe=rouge1_df)
            )) and logged
            rougeL_df = rouge_df[['score_rougeL']].rename(columns={'score_rougeL': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGEL", dataframe=rougeL_df)
            )) and logged
        elif self.mode == 'ax' and self.arize_client:
            for metric in ['rouge1', 'rougeL']:
                metric_df = rouge_df[[f'score_{metric}']].rename(columns={f'score_{metric}': f'eval.ROUGE_{metric.upper()}.score'})
                logged = self._retry_operation(lambda df=metric_df: self.arize_client.log_evaluations_sync(df, self.project_name)) and logged
        if offline:
            logger.info(f"Offline ROUGE evals: {rouge_df.reset_index().to_dict(orient='records')}")
        else:
            logger.info(f"Online ROUGE eval: {rouge_df.reset_index().to_dict(orient='records')[0]}")
        return logged

    def workflow(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span("workflow") as span:
                span.set_attribute("span.kind", "INTERNAL")
                span.set_attribute("llm.prompt", args[0] if args else "")
                span.set_attribute("input.value", args[0] if args else "")
                span.set_attribute("custom.annotation.prompt", args[0] if args else "")
                span.set_attribute("custom.annotation.reference", args[1] if len(args) > 1 else "")
                span.add_event("prompt", {"value": args[0] if args else ""})
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("llm.response", result)
                    span.set_attribute("output.value", result)
                    span.set_attribute("custom.annotation.output", result)
                    span.add_event("response", {"value": result})
                    span.set_status(Status(StatusCode.OK))
                    span_id = str(span.get_span_context().span_id)
                    self.stored_traces.append({
                        'span_id': span_id,
                        'input': args[0] if args else "",
                        'reference': args[1] if len(args) > 1 else "",
                        'output': result,
                    })
                    if random.random() < self.sample_rate:
                        self.run_online_evals(span_id, args[0] if args else "", args[1] if len(args) > 1 else "", result)
                    return result
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"Workflow error: {e}")
                    raise
        return wrapper

    def tool_span(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span(func.__name__) as span:
                span.set_attribute("span.kind", "INTERNAL")
                span.set_attribute("input.value", str(args))
                span.set_attribute("custom.annotation.tool_input", str(args))
                span.add_event("tool_input", {"value": str(args)})
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("output.value", str(result))
                    span.set_attribute("custom.annotation.tool_output", str(result))
                    span.add_event("tool_output", {"value": str(result)})
                    span.set_status(Status(StatusCode.OK))
                    return result
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"Tool error in {func.__name__}: {e}")
                    raise
        return wrapper

    def run_online_evals(self, span_id, input_text, reference, output):
        df = pd.DataFrame([{
            "input": input_text,
            "output": output,
            "reference": reference,
            "context": reference,
        }], index=[span_id])
        df.index.name = "context.span_id"
        eval_dfs = run_evals(
            dataframe=df,
            evaluators=self.evaluators,
            provide_explanation=True,
        )
        for eval_df, evaluator in zip(eval_dfs, self.evaluators):
            eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
            if self.log_evaluation(eval_df, eval_name):
                logger.info(f"Online {eval_name} eval logged successfully")
            else:
                logger.warning(f"Failed to log online {eval_name} eval")
        self.run_rouge_eval(df)

    def run_offline_evals(self):
        if not self.stored_traces:
            return
        df = pd.DataFrame(self.stored_traces)
        df["context"] = df["reference"]
        df = df.set_index("span_id")
        df.index.name = "context.span_id"
        eval_dfs = run_evals(
            dataframe=df,
            evaluators=self.evaluators,
            provide_explanation=True,
        )
        for eval_df, evaluator in zip(eval_dfs, self.evaluators):
            eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
            if self.log_evaluation(eval_df, eval_name):
                logger.info(f"Offline {eval_name} evals logged successfully")
            else:
                logger.warning(f"Failed to log offline {eval_name} evals")
        self.run_rouge_eval(df, offline=True)

    def run_rouge_eval(self, df, offline=False):
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
        rouge_data = []
        for span_id, row in df.iterrows():
            pred = row["output"]
            ref = row["reference"]
            scores = scorer.score(ref, pred)
            rouge_data.append({
                "span_id": span_id,
                "score_rouge1": scores["rouge1"].fmeasure,
                "score_rougeL": scores["rougeL"].fmeasure,
            })
        rouge_df = pd.DataFrame(rouge_data)
        self.log_rouge_evaluation(rouge_df, offline=offline)

    def shutdown(self):
        if hasattr(self, 'tracer_provider'):
            self.tracer_provider.shutdown()
        logger.info("Tracer shutdown complete")
```

```python
# tools.py
import threading
import json
from genai_observer import GenAIObserver

class ToolSet:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.observer = GenAIObserver.get_instance()

    @self.observer.tool_span
    def search(self, query: str) -> str:
        """Simulates a web search."""
        return f"Mock search result for: {query}"

    @self.observer.tool_span
    def calculate(self, expression: str) -> float:
        """Simulates a computation."""
        try:
            return eval(expression)
        except Exception as e:
            return f"Error: {e}"

    @self.observer.tool_span
    def fetch_weather(self, city: str) -> str:
        """Simulates an API call for weather."""
        return f"Mock weather in {city}: Sunny, 75°F"

    @self.observer.tool_span
    def get_stock_price(self, ticker: str) -> str:
        """Simulates fetching a stock price for a given ticker."""
        return f"Mock stock price for {ticker}: $100.00"

    @self.observer.tool_span
    def send_email(self, recipient: str, subject: str, body: str) -> str:
        """Simulates sending an email."""
        return f"Mock email sent to {recipient} with subject '{subject}' and body '{body}'"

    def get_tool_schemas(self):
        """Returns JSON schemas for OpenAI function calling."""
        return [
            {
                "name": "search",
                "description": "Perform a web search with a given query.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "The search query"}
                    },
                    "required": ["query"]
                }
            },
            {
                "name": "calculate",
                "description": "Evaluate a mathematical expression.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "expression": {"type": "string", "description": "The mathematical expression to evaluate"}
                    },
                    "required": ["expression"]
                }
            },
            {
                "name": "fetch_weather",
                "description": "Fetch the weather for a given city.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {"type": "string", "description": "The city name"}
                    },
                    "required": ["city"]
                }
            },
            {
                "name": "get_stock_price",
                "description": "Fetch the stock price for a given ticker symbol.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "ticker": {"type": "string", "description": "The stock ticker symbol"}
                    },
                    "required": ["ticker"]
                }
            },
            {
                "name": "send_email",
                "description": "Send an email to a recipient with a subject and body.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "recipient": {"type": "string", "description": "The email address of the recipient"},
                        "subject": {"type": "string", "description": "The subject of the email"},
                        "body": {"type": "string", "description": "The body of the email"}
                    },
                    "required": ["recipient", "subject", "body"]
                }
            }
        ]
```

```python
# main.py
import json
import threading
from dotenv import load_dotenv
import openai
from genai_observer import GenAIObserver
import tools
from opentelemetry.trace import Status, StatusCode

load_dotenv()

class WorkflowManager:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.observer = GenAIObserver.get_instance()
        self.tool_set = tools.ToolSet.get_instance()
        self.openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

    @property
    def workflow(self):
        return self.observer.workflow

    @workflow
    def run_workflow(self, prompt: str, reference: str) -> str:
        """Run the LLM workflow with OpenAI function calls."""
        current_span = self.observer.tracer.get_current_span()
        current_span.set_attribute("span.kind", "INTERNAL")
        current_span.set_attribute("llm.prompt", prompt)
        current_span.set_attribute("input.value", prompt)
        current_span.set_attribute("custom.annotation.prompt", prompt)
        current_span.set_attribute("custom.annotation.reference", reference)
        current_span.add_event("prompt", {"value": prompt})

        try:
            messages = [{"role": "user", "content": prompt}]
            tools = self.tool_set.get_tool_schemas()

            # Make OpenAI API call with function tools
            with self.observer.tracer.start_as_current_span("openai_call") as span:
                span.set_attribute("span.kind", "CLIENT")
                span.set_attribute("llm.model", "gpt-4-turbo-preview")
                span.set_attribute("input.value", prompt)
                span.add_event("llm_prompt", {"value": prompt})
                try:
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4-turbo-preview",
                        messages=messages,
                        tools=[{"type": "function", "function": tool} for tool in tools],
                        tool_choice="auto"
                    )
                    span.set_attribute("output.value", str(response))
                    span.add_event("llm_response", {"value": str(response)})
                    span.set_status(Status(StatusCode.OK))
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"OpenAI API error: {e}")
                    raise

            # Process tool calls
            tool_calls = response.choices[0].message.tool_calls
            results = []
            if tool_calls:
                for tool_call in tool_calls:
                    func_name = tool_call.function.name
                    args = json.loads(tool_call.function.arguments)
                    try:
                        if func_name == "search":
                            result = self.tool_set.search(args["query"])
                        elif func_name == "calculate":
                            result = self.tool_set.calculate(args["expression"])
                        elif func_name == "fetch_weather":
                            result = self.tool_set.fetch_weather(args["city"])
                        elif func_name == "get_stock_price":
                            result = self.tool_set.get_stock_price(args["ticker"])
                        elif func_name == "send_email":
                            result = self.tool_set.send_email(args["recipient"], args["subject"], args["body"])
                        else:
                            result = f"Unknown function: {func_name}"
                        results.append(f"{func_name}: {result}")
                    except Exception as e:
                        results.append(f"{func_name}: Error - {str(e)}")
                        logger.error(f"Tool call error for {func_name}: {e}")

            # Generate final response
            response_text = f"Based on tools:\n{'\n'.join(results)}\nAnswering prompt: {prompt}"
            current_span.set_attribute("llm.response", response_text)
            current_span.set_attribute("output.value", response_text)
            current_span.set_attribute("custom.annotation.output", response_text)
            current_span.add_event("response", {"value": response_text})
            current_span.set_status(Status(StatusCode.OK))
            return response_text
        except Exception as e:
            current_span.set_status(Status(StatusCode.ERROR, str(e)))
            current_span.add_event("error", {"message": str(e)})
            logger.error(f"Workflow error: {e}")
            raise

    def run(self, prompt: str, reference: str):
        """Execute the workflow and evaluations."""
        result = self.run_workflow(prompt, reference)
        print("Workflow Result:")
        print(result)
        self.observer.run_offline_evals()
        self.observer.shutdown()

if __name__ == "__main__":
    # Example run with mock prompt and reference
    workflow_manager = WorkflowManager.get_instance()
    prompt = "What is the latest in AI? Also, get the stock price for AAPL, calculate 40 * 2.5, and check the weather in San Francisco."
    reference = "The latest in AI includes advancements in observability frameworks like Arize Phoenix."
    workflow_manager.run(prompt, reference)
```

```python
# requirements.txt
arize-phoenix==0.33.0
arize-otel
arize
opentelemetry-sdk
opentelemetry-exporter-otlp
evaluate
rouge_score
python-dotenv
requests
httpx
protobuf
grpcio
openai>=1.0.0
certifi
```

### Steps to Test
1. **Test Localhost**:
   - Start Phoenix server:
     ```
     python -m phoenix.server.main serve
     ```
     - Confirm logs: `Server running at http://localhost:6006`.
   - Use `.env.local`:
     ```plaintext
     OPENAI_API_KEY='sk-'
     OTEL_EXPORTER_OTLP_ENDPOINT='http://localhost:6006/v1/traces'
     PHOENIX_COLLECTOR_ENDPOINT='http://localhost:6006'
     USE_AX_MODE=false
     ALLOW_INSECURE_CONNECTION=true
     ONLINE_SAMPLE_RATIO=0.1
     PHOENIX_PROJECT_NAME=default
     ```
   - Load and run:
     ```
     export $(cat .env.local | xargs)
     python main.py
     ```
   - Check logs: `Using HTTP exporter with endpoint: http://localhost:6006/v1/traces`, no errors.
   - Verify traces, child spans (`search`, `calculate`, `fetch_weather`, `get_stock_price`, `send_email`), and evaluations in UI: `http://localhost:6006`. Confirm span status, kind, and input/output messages are visible.

2. **Test OCP**:
   - Use `.env.ocp`:
     ```plaintext
     OPENAI_API_KEY='sk-'
     OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
     PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
     USE_AX_MODE=false
     ALLOW_INSECURE_CONNECTION=true
     ONLINE_SAMPLE_RATIO=0.1
     PHOENIX_PROJECT_NAME=default
     ```
   - Run:
     ```
     export $(cat .env.ocp | xargs)
     python main.py
     ```
   - Check UI: `https://arize-phoenix-ocp.nonprod.we.net`. Confirm span details.

3. **Install Dependencies**:
   ```
   pip install -r requirements.txt
   ```

4. **Check Logs**:
   - For localhost: Expect `Successfully connected to localhost:6006`, no `TypeError` or other errors.
   - For OCP: Expect `WARNING: SSL verification is disabled for the OTLP HTTP exporter.`, no `SSLCertVerificationError`.
   - Verify OpenAI function calls, traces, and evaluations in the UI with status, kind, and messages.

### If Issues Persist
- **Trace Details**: If span status, kind, or input/output messages are missing, check the Phoenix UI for attribute requirements (e.g., `llm.*` prefixes) and share screenshots or logs.
- **Localhost**: Ensure `python -m phoenix.server.main serve` runs (`curl http://localhost:6006/v1/traces`). Check port conflicts (`lsof -i :6006`).
- **OCP**: Test with `curl -k https://arize-phoenix-ocp.nonprod.we.net/v1/traces` if issues reappear.
- **Debug**: Share any errors or UI screenshots if trace details are incomplete.

For UAT/prod, use `USE_AX_MODE=true` with ARIZE keys—no SSL issues with cloud endpoints. Let me know if you need further tweaks or encounter issues!
