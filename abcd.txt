Thank you for the feedback that evaluation results from `run_online_evals` are still not visible in the Arize Phoenix UI (`http://localhost:6006` or `https://arize-phoenix-ocp.nonprod.we.net`) and that token information (e.g., prompt tokens, completion tokens) for the LLM call (`openai_call` span) is missing in the Phoenix dashboard. Since the code is functional for tracing, singletons (`GenAIObserver`, `ToolSet`, `WorkflowManager`), OpenAI function calls with a custom proxy gateway, and displays span kind, LLM, and tool icons (after recent fixes), I’ll focus on:
1. Ensuring evaluation results (`QA`, `Hallucination`, `Relevance`, `Toxicity`, `ROUGE1`, `ROUGEL`) are saved and visible in the Phoenix UI.
2. Adding token information (e.g., `llm.token_count.prompt`, `llm.token_count.completion`) to the `openai_call` span for visibility in the Phoenix dashboard.
3. Maintaining all existing features: singleton pattern, OpenAI function calls with proxy gateway, online (10% sampling) and offline evaluations, child spans (`search`, `calculate`, `fetch_weather`, `get_stock_price`, `send_email`), custom annotations (`custom.annotation.*`), span kind, LLM/tool icons, and SSL bypass for OCP.

### Why Evaluations and Token Information Are Missing
1. **Evaluation Results Not Visible in Phoenix**:
   - The `log_evaluation` and `log_rouge_evaluation` methods in `gen_ai_observer.py` attempt to log evaluations using `phoenix_client.log_evaluations`, but they may fail silently due to:
     - Incorrect Phoenix client configuration (e.g., endpoint or project name mismatch).
     - DataFrame format issues (e.g., missing `context.span_id` or incorrect column names).
     - Phoenix UI not displaying evaluations unless explicitly viewed in the Evaluations tab or associated with spans correctly.
   - The current code adds evaluation results as span attributes/events, but Phoenix may require specific handling (e.g., ensuring `SpanEvaluations` is correctly formatted or synced).
   - The `run_rouge_eval` method for offline evaluations may not iterate correctly over span IDs, causing results to be missed.

2. **Token Information Missing in Phoenix Dashboard**:
   - The `openai_call` span in `main.py` doesn’t capture token usage (e.g., `prompt_tokens`, `completion_tokens`) from the OpenAI response.
   - Phoenix expects attributes like `llm.token_count.prompt`, `llm.token_count.completion`, and `llm.token_count.total` to display token metrics in the dashboard.
   - The OpenAI response object typically includes `usage` (e.g., `response.usage.prompt_tokens`, `response.usage.completion_tokens`), which needs to be extracted and set as span attributes.

### Solution
To address these issues:
1. **Ensure Evaluations Are Saved in Phoenix**:
   - Enhance error handling and logging in `log_evaluation` and `log_rouge_evaluation` to confirm successful logging.
   - Verify `phoenix_client` configuration and endpoint connectivity.
   - Fix `run_rouge_eval` to handle offline evaluations correctly by iterating over span IDs.
   - Add evaluation results as span attributes/events with Phoenix-compatible keys (e.g., `eval.<name>.score`, `eval.<name>.label`).
   - Ensure `context.span_id` is correctly set in evaluation DataFrames.
2. **Add Token Information to `openai_call` Span**:
   - Extract `prompt_tokens`, `completion_tokens`, and `total_tokens` from the OpenAI response (`response.usage`).
   - Set attributes like `llm.token_count.prompt`, `llm.token_count.completion`, and `llm.token_count.total` in the `openai_call` span.
3. **Maintain Phoenix UI Compatibility**:
   - Retain `span_type="llm"`, `span_type="tool"`, `llm.model_name`, `tool.name`, and `otel.kind` for span kind and icon rendering.
4. **Centralize Attribute Setting**:
   - Keep span attribute/event setting in `gen_ai_observer.py` for the `workflow` span, as previously streamlined.
5. **Maintain Existing Features**:
   - Preserve singleton pattern, OpenAI function calls with proxy gateway (`OPENAI_API_ENDPOINT`, `OPENAI_PROXY_TOKEN`), online/offline evaluations, child spans, custom annotations, and SSL bypass.

### Updated .env Files
No changes needed, as the issues are code-level. For reference:
- **Localhost**:
  ```plaintext
  # .env.local
  OPENAI_API_KEY='sk-'
  OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
  OPENAI_PROXY_TOKEN='your-bearer-token'
  OTEL_EXPORTER_OTLP_ENDPOINT='http://localhost:6006/v1/traces'
  PHOENIX_COLLECTOR_ENDPOINT='http://localhost:6006'
  USE_AX_MODE=false
  ALLOW_INSECURE_CONNECTION=true
  ONLINE_SAMPLE_RATIO=0.1
  PHOENIX_PROJECT_NAME=default
  ```
- **OCP**:
  ```plaintext
  # .env.ocp
  OPENAI_API_KEY='sk-'
  OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
  OPENAI_PROXY_TOKEN='your-bearer-token'
  OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
  PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
  USE_AX_MODE=false
  ALLOW_INSECURE_CONNECTION=true
  ONLINE_SAMPLE_RATIO=0.1
  PHOENIX_PROJECT_NAME=default
  ```
Replace `'https://your-openai-proxy-gateway/api'` and `'your-bearer-token'` with your actual proxy endpoint and token.

### Updated Code
- **gen_ai_observer.py**: Enhance `log_evaluation`, `log_rouge_evaluation`, `run_online_evals`, and `run_offline_evals` to ensure evaluations are saved and visible in Phoenix. Add robust logging and span attribute/event setting for evaluations.
- **tools.py**: Unchanged, as it correctly applies decorators dynamically.
- **main.py**: Update `run_workflow` to extract and set token usage attributes for the `openai_call` span.
- **requirements.txt**: Unchanged.

```python
# gen_ai_observer.py
import os
import logging
import random
import pandas as pd
from typing import Callable
import json
import httpx
import requests
import certifi
import socket
import threading
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as OTLPSpanExporterHTTP
from opentelemetry.trace import Status, StatusCode, SpanKind

try:
    from phoenix.otel import register as phoenix_register
    import phoenix as px
    from phoenix.trace import SpanEvaluations
except ImportError:
    phoenix_register = None
    px = None
    SpanEvaluations = None

try:
    from arize.otel import register as arize_register
    from arize.pandas.logger import Client as ArizeClient
except ImportError:
    arize_register = None
    ArizeClient = None

from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
    ToxicityEvaluator,
    OpenAIModel,
    run_evals,
)

from rouge_score import rouge_scorer

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Patch requests and httpx to bypass SSL verification when insecure
insecure = os.getenv('ALLOW_INSECURE_CONNECTION', 'false').lower() == 'true'
if insecure:
    logger.warning("Globally disabling SSL verification for requests and httpx")
    orig_request = requests.Session.request
    def patched_request(self, *args, **kwargs):
        kwargs['verify'] = False
        return orig_request(self, *args, **kwargs)
    requests.Session.request = patched_request
    try:
        import openai
        orig_openai_client = openai.OpenAI.__init__
        def patched_openai_init(self, *args, **kwargs):
            kwargs['http_client'] = httpx.Client(verify=False)
            orig_openai_client(self, *args, **kwargs)
        openai.OpenAI.__init__ = patched_openai_init
    except ImportError:
        logger.warning("openai package not installed; evaluations may fail")

custom_cert = os.getenv('CUSTOM_SSL_CERT_FILE', certifi.where())
os.environ['REQUESTS_CA_BUNDLE'] = custom_cert
logger.debug(f"SSL cert set: {custom_cert}")

class GenAIObserver:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.mode = 'ax' if os.getenv('USE_AX_MODE', 'false').lower() == 'true' else 'local'
        self.ui_endpoint = os.getenv('PHOENIX_COLLECTOR_ENDPOINT', 'http://localhost:6006/')
        self.otlp_endpoint = os.getenv('OTEL_EXPORTER_OTLP_ENDPOINT', self.ui_endpoint.rstrip('/') + '/v1/traces')
        self.insecure = os.getenv('ALLOW_INSECURE_CONNECTION', 'false').lower() == 'true'
        self.sample_rate = float(os.getenv('ONLINE_SAMPLE_RATIO', 0.1))
        logger.debug(f"Env config: mode={self.mode}, ui_endpoint={self.ui_endpoint}, otlp_endpoint={self.otlp_endpoint}, insecure={self.insecure}, sample_rate={self.sample_rate}")
        self._test_endpoint(self.otlp_endpoint)
        self.tracer_provider = self.setup_tracer()
        trace.set_tracer_provider(self.tracer_provider)
        self.tracer = trace.get_tracer(__name__)
        self.phoenix_client = None
        self.arize_client = None
        self.setup_client()
        self.stored_traces = []
        self.eval_model = self.setup_eval_model()
        self.evaluators = self.setup_evaluators()

    def _test_endpoint(self, endpoint):
        """Test if the OTLP endpoint is reachable."""
        try:
            import urllib.parse
            parsed = urllib.parse.urlparse(endpoint)
            host = parsed.hostname
            port = parsed.port or (443 if parsed.scheme == 'https' else 80)
            with socket.create_connection((host, port), timeout=2) as sock:
                logger.debug(f"Successfully connected to {host}:{port}")
        except Exception as e:
            logger.error(f"Failed to connect to {endpoint}: {e}")

    def setup_eval_model(self):
        """Set up OpenAIModel with custom OpenAI endpoint and headers."""
        try:
            from openai import OpenAI
        except ImportError:
            logger.error("openai package not installed; evaluations will fail without OPENAI_API_KEY")
            raise ImportError("openai package required for evaluations")

        logger.info("Using custom OpenAI endpoint for evaluations")
        openai_endpoint = os.getenv('OPENAI_API_ENDPOINT', 'https://api.openai.com')
        proxy_token = os.getenv('OPENAI_PROXY_TOKEN', os.getenv('OPENAI_API_KEY'))
        if not proxy_token:
            raise ValueError("OPENAI_API_KEY or OPENAI_PROXY_TOKEN must be set for evaluations")

        # Create custom OpenAI client with headers
        headers = {"Authorization": f"Bearer {proxy_token}"}
        http_client = httpx.Client(verify=not self.insecure, headers=headers)
        openai_client = OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=openai_endpoint,
            http_client=http_client
        )
        return OpenAIModel(model="gpt-4-turbo-preview", openai_client=openai_client)

    def determine_mode(self):
        return self.mode

    def setup_tracer(self):
        exporter_session_kwargs = {'session': httpx.Client(verify=not self.insecure)}
        if self.insecure and self.otlp_endpoint.startswith("https"):
            logger.warning("SSL verification is disabled for the OTLP HTTP exporter.")
            exporter_session_kwargs['session'] = httpx.Client(verify=False)

        if self.mode == 'ax':
            if not arize_register:
                raise ImportError("arize-otel not installed for AX mode")
            logger.info("Using AX mode exporter")
            return arize_register(
                space_id=os.getenv('ARIZE_SPACE_KEY'),
                api_key=os.getenv('ARIZE_API_KEY'),
                endpoint=self.otlp_endpoint,
            )
        else:
            project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            logger.debug(f"Setting up tracer for project: {project_name}")
            if phoenix_register:
                logger.info("Using Phoenix register for local mode")
                return phoenix_register(endpoint=self.otlp_endpoint, protocol='http/protobuf', project_name=project_name)
            else:
                logger.info(f"Using HTTP exporter with endpoint: {self.otlp_endpoint}")
                exporter = OTLPSpanExporterHTTP(
                    endpoint=self.otlp_endpoint,
                    **exporter_session_kwargs
                )
                processor = BatchSpanProcessor(exporter)
                tracer_provider = TracerProvider()
                tracer_provider.add_span_processor(processor)
                return tracer_provider

    def setup_client(self):
        if self.mode == 'ax':
            if not ArizeClient:
                raise ImportError("arize not installed for AX mode")
            space_id = os.getenv('ARIZE_SPACE_KEY') or os.getenv('ARIZE_SPACE_ID')
            api_key = os.getenv('ARIZE_API_KEY')
            self.project_name = os.getenv('PHOENIX_PROJECT_NAME', 'GenAIObserver')
            self.arize_client = ArizeClient(api_key=api_key, space_id=space_id)
            logger.info("Using Arize client for AX mode")
        else:
            if px is None:
                raise ImportError("phoenix not installed for local mode")
            self.project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            self.phoenix_client = px.Client(endpoint=self.ui_endpoint)
            logger.info("Using Phoenix client for local mode")

    def setup_evaluators(self):
        return [
            QAEvaluator(self.eval_model),
            HallucinationEvaluator(self.eval_model),
            RelevanceEvaluator(self.eval_model),
            ToxicityEvaluator(self.eval_model),
        ]

    def _retry_operation(self, operation: Callable, max_retries: int = 3) -> bool:
        for attempt in range(max_retries):
            try:
                operation()
                logger.debug(f"Operation succeeded on attempt {attempt + 1}")
                return True
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt == max_retries - 1:
                    logger.error(f"Max retries exceeded for operation: {e}")
                    return False
        return False

    def _add_score_if_missing(self, eval_df: pd.DataFrame, eval_name: str) -> pd.DataFrame:
        eval_df = eval_df.copy()
        if 'score' not in eval_df.columns:
            if eval_name == 'QA':
                eval_df['score'] = (eval_df['label'] == 'correct').astype(float)
            elif eval_name == 'Hallucination':
                eval_df['score'] = (eval_df['label'] == 'factual').astype(float)
            elif eval_name == 'Relevance':
                eval_df['score'] = (eval_df['label'] == 'relevant').astype(float)
            elif eval_name == 'Toxicity':
                eval_df['score'] = (eval_df['label'] == 'non-toxic').astype(float)
        return eval_df

    def log_evaluation(self, eval_df: pd.DataFrame, eval_name: str, span_id: str) -> bool:
        eval_df = self._add_score_if_missing(eval_df, eval_name)
        eval_df = eval_df.rename_axis("context.span_id")
        logger.debug(f"Logging evaluation {eval_name} for span_id {span_id}: {eval_df.to_dict()}")
        if self.mode == 'local' and self.phoenix_client:
            success = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name=eval_name, dataframe=eval_df)
            ))
            if success:
                # Add evaluation results as span attributes
                with self.tracer.start_as_current_span(f"eval_{eval_name}", context=trace.get_span_context(span_id)) as span:
                    span.set_attribute(f"eval.{eval_name}.score", eval_df.get('score', [None])[0])
                    span.set_attribute(f"eval.{eval_name}.label", eval_df.get('label', [None])[0])
                    span.set_attribute(f"eval.{eval_name}.explanation", eval_df.get('explanation', [None])[0])
                    span.add_event(f"eval_{eval_name}", {
                        "score": eval_df.get('score', [None])[0],
                        "label": eval_df.get('label', [None])[0],
                        "explanation": eval_df.get('explanation', [None])[0]
                    })
                logger.info(f"Successfully logged {eval_name} evaluation for span_id {span_id}")
                return True
            else:
                logger.error(f"Failed to log {eval_name} evaluation for span_id {span_id}")
                return False
        elif self.mode == 'ax' and self.arize_client:
            eval_df_log = eval_df[['label', 'score', 'explanation']].copy()
            prefix = f'eval.{eval_name}.'
            eval_df_log = eval_df_log.rename(columns={
                'label': prefix + 'label',
                'score': prefix + 'score',
                'explanation': prefix + 'explanation',
            })
            success = self._retry_operation(lambda: self.arize_client.log_evaluations_sync(eval_df_log, self.project_name))
            if success:
                # Add evaluation results as span attributes
                with self.tracer.start_as_current_span(f"eval_{eval_name}", context=trace.get_span_context(span_id)) as span:
                    span.set_attribute(f"eval.{eval_name}.score", eval_df.get('score', [None])[0])
                    span.set_attribute(f"eval.{eval_name}.label", eval_df.get('label', [None])[0])
                    span.set_attribute(f"eval.{eval_name}.explanation", eval_df.get('explanation', [None])[0])
                    span.add_event(f"eval_{eval_name}", {
                        "score": eval_df.get('score', [None])[0],
                        "label": eval_df.get('label', [None])[0],
                        "explanation": eval_df.get('explanation', [None])[0]
                    })
                logger.info(f"Successfully logged {eval_name} evaluation for span_id {span_id}")
                return True
            else:
                logger.error(f"Failed to log {eval_name} evaluation for span_id {span_id}")
                return False
        else:
            logger.warning(f"Cannot log evaluation: {self.mode} client not available")
            return False

    def log_rouge_evaluation(self, rouge_df: pd.DataFrame, offline: bool, span_id: str) -> bool:
        rouge_df = rouge_df.set_index("context.span_id")
        logger.debug(f"Logging ROUGE evaluation for span_id {span_id}: {rouge_df.to_dict()}")
        logged = True
        if self.mode == 'local' and self.phoenix_client:
            rouge1_df = rouge_df[['score_rouge1']].rename(columns={'score_rouge1': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGE1", dataframe=rouge1_df)
            )) and logged
            rougeL_df = rouge_df[['score_rougeL']].rename(columns={'score_rougeL': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGEL", dataframe=rougeL_df)
            )) and logged
            if logged:
                # Add ROUGE results as span attributes
                with self.tracer.start_as_current_span("rouge_evaluation", context=trace.get_span_context(span_id)) as span:
                    span.set_attribute("eval.ROUGE1.score", rouge_df.get('score_rouge1', [None])[0])
                    span.set_attribute("eval.ROUGEL.score", rouge_df.get('score_rougeL', [None])[0])
                    span.add_event("rouge_eval", {
                        "ROUGE1_score": rouge_df.get('score_rouge1', [None])[0],
                        "ROUGEL_score": rouge_df.get('score_rougeL', [None])[0]
                    })
                logger.info(f"Successfully logged ROUGE evaluation for span_id {span_id}")
            else:
                logger.error(f"Failed to log ROUGE evaluation for span_id {span_id}")
        elif self.mode == 'ax' and self.arize_client:
            for metric in ['rouge1', 'rougeL']:
                metric_df = rouge_df[[f'score_{metric}']].rename(columns={f'score_{metric}': f'eval.ROUGE_{metric.upper()}.score'})
                logged = self._retry_operation(lambda df=metric_df: self.arize_client.log_evaluations_sync(df, self.project_name)) and logged
            if logged:
                # Add ROUGE results as span attributes
                with self.tracer.start_as_current_span("rouge_evaluation", context=trace.get_span_context(span_id)) as span:
                    span.set_attribute("eval.ROUGE1.score", rouge_df.get('score_rouge1', [None])[0])
                    span.set_attribute("eval.ROUGEL.score", rouge_df.get('score_rougeL', [None])[0])
                    span.add_event("rouge_eval", {
                        "ROUGE1_score": rouge_df.get('score_rouge1', [None])[0],
                        "ROUGEL_score": rouge_df.get('score_rougeL', [None])[0]
                    })
                logger.info(f"Successfully logged ROUGE evaluation for span_id {span_id}")
            else:
                logger.error(f"Failed to log ROUGE evaluation for span_id {span_id}")
        if offline:
            logger.info(f"Offline ROUGE evals: {rouge_df.reset_index().to_dict(orient='records')}")
        else:
            logger.info(f"Online ROUGE eval: {rouge_df.reset_index().to_dict(orient='records')[0]}")
        return logged

    def workflow(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span("workflow", kind=SpanKind.INTERNAL) as span:
                span.set_attribute("span_type", "llm_workflow")
                span.set_attribute("otel.kind", "INTERNAL")
                span.set_attribute("llm.model_name", "gpt-4-turbo-preview")
                span.set_attribute("llm.prompt", args[0] if args else "")
                span.set_attribute("custom.annotation.prompt", args[0] if args else "")
                span.set_attribute("custom.annotation.reference", args[1] if len(args) > 1 else "")
                span.add_event("prompt", {"value": args[0] if args else ""})
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("llm.completions", result)
                    span.set_attribute("custom.annotation.output", result)
                    span.add_event("completion", {"value": result})
                    span.set_status(Status(StatusCode.OK))
                    span_id = str(span.get_span_context().span_id)
                    self.stored_traces.append({
                        'span_id': span_id,
                        'input': args[0] if args else "",
                        'reference': args[1] if len(args) > 1 else "",
                        'output': result,
                    })
                    if random.random() < self.sample_rate:
                        self.run_online_evals(span_id, args[0] if args else "", args[1] if len(args) > 1 else "", result)
                    return result
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"Workflow error: {e}")
                    raise
        return wrapper

    def tool_span(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span(func.__name__, kind=SpanKind.INTERNAL) as span:
                span.set_attribute("span_type", "tool")
                span.set_attribute("otel.kind", "INTERNAL")
                span.set_attribute("tool.name", func.__name__)
                span.set_attribute("tool.description", getattr(func, '__doc__', '').strip())
                span.set_attribute("tool.input", str(args))
                span.set_attribute("custom.annotation.tool_input", str(args))
                span.add_event("tool_input", {"value": str(args)})
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("tool.output", str(result))
                    span.set_attribute("custom.annotation.tool_output", str(result))
                    span.add_event("tool_output", {"value": str(result)})
                    span.set_status(Status(StatusCode.OK))
                    return result
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"Tool error in {func.__name__}: {e}")
                    raise
        return wrapper

    def run_online_evals(self, span_id, input_text, reference, output):
        df = pd.DataFrame([{
            "input": input_text,
            "output": output,
            "reference": reference,
            "context": reference,
        }], index=[span_id])
        df.index.name = "context.span_id"
        logger.debug(f"Running online evaluations for span_id {span_id}: {df.to_dict()}")
        try:
            eval_dfs = run_evals(
                dataframe=df,
                evaluators=self.evaluators,
                provide_explanation=True,
            )
            for eval_df, evaluator in zip(eval_dfs, self.evaluators):
                eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
                if self.log_evaluation(eval_df, eval_name, span_id):
                    logger.info(f"Online {eval_name} eval logged successfully for span_id {span_id}")
                else:
                    logger.warning(f"Failed to log online {eval_name} eval for span_id {span_id}")
            self.log_rouge_evaluation(df, offline=False, span_id=span_id)
        except Exception as e:
            logger.error(f"Error running online evaluations for span_id {span_id}: {e}")
            raise

    def run_offline_evals(self):
        if not self.stored_traces:
            logger.info("No stored traces for offline evaluations")
            return
        df = pd.DataFrame(self.stored_traces)
        df["context"] = df["reference"]
        df = df.set_index("span_id")
        df.index.name = "context.span_id"
        logger.debug(f"Running offline evaluations: {df.to_dict()}")
        try:
            eval_dfs = run_evals(
                dataframe=df,
                evaluators=self.evaluators,
                provide_explanation=True,
            )
            for eval_df, evaluator in zip(eval_dfs, self.evaluators):
                eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
                for span_id in eval_df.index:
                    if self.log_evaluation(eval_df.loc[[span_id]], eval_name, span_id):
                        logger.info(f"Offline {eval_name} eval logged successfully for span_id {span_id}")
                    else:
                        logger.warning(f"Failed to log offline {eval_name} eval for span_id {span_id}")
            for span_id in df.index:
                self.log_rouge_evaluation(df.loc[[span_id]], offline=True, span_id=span_id)
        except Exception as e:
            logger.error(f"Error running offline evaluations: {e}")
            raise

    def run_rouge_eval(self, df, offline=False, span_id=None):
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
        rouge_data = []
        for span_id_row, row in df.iterrows():
            pred = row["output"]
            ref = row["reference"]
            scores = scorer.score(ref, pred)
            rouge_data.append({
                "span_id": span_id_row,
                "score_rouge1": scores["rouge1"].fmeasure,
                "score_rougeL": scores["rougeL"].fmeasure,
            })
        rouge_df = pd.DataFrame(rouge_data)
        if offline:
            for _, row in rouge_df.iterrows():
                self.log_rouge_evaluation(pd.DataFrame([row]).set_index("span_id"), offline=True, span_id=row['span_id'])
        else:
            self.log_rouge_evaluation(rouge_df, offline=False, span_id=span_id)

    def shutdown(self):
        if hasattr(self, 'tracer_provider'):
            self.tracer_provider.shutdown()
        logger.info("Tracer shutdown complete")
```

```python
# tools.py
import threading
import json
from genai_observer import GenAIObserver

class ToolSet:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.observer = GenAIObserver.get_instance()
        # Dynamically apply tool_span decorator to methods
        self.search = self.tool_span(self.search)
        self.calculate = self.tool_span(self.calculate)
        self.fetch_weather = self.tool_span(self.fetch_weather)
        self.get_stock_price = self.tool_span(self.get_stock_price)
        self.send_email = self.tool_span(self.send_email)

    def tool_span(self, func):
        """Function decorator to apply observer.tool_span."""
        return self.observer.tool_span(func)

    def search(self, query: str) -> str:
        """Simulates a web search."""
        return f"Mock search result for: {query}"

    def calculate(self, expression: str) -> float:
        """Simulates a computation."""
        try:
            return eval(expression)
        except Exception as e:
            return f"Error: {e}"

    def fetch_weather(self, city: str) -> str:
        """Simulates an API call for weather."""
        return f"Mock weather in {city}: Sunny, 75°F"

    def get_stock_price(self, ticker: str) -> str:
        """Simulates fetching a stock price for a given ticker."""
        return f"Mock stock price for {ticker}: $100.00"

    def send_email(self, recipient: str, subject: str, body: str) -> str:
        """Simulates sending an email."""
        return f"Mock email sent to {recipient} with subject '{subject}' and body '{body}'"

    def get_tool_schemas(self):
        """Returns JSON schemas for OpenAI function calling."""
        return [
            {
                "name": "search",
                "description": "Perform a web search with a given query.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "The search query"}
                    },
                    "required": ["query"]
                }
            },
            {
                "name": "calculate",
                "description": "Evaluate a mathematical expression.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "expression": {"type": "string", "description": "The mathematical expression to evaluate"}
                    },
                    "required": ["expression"]
                }
            },
            {
                "name": "fetch_weather",
                "description": "Fetch the weather for a given city.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {"type": "string", "description": "The city name"}
                    },
                    "required": ["city"]
                }
            },
            {
                "name": "get_stock_price",
                "description": "Fetch the stock price for a given ticker symbol.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "ticker": {"type": "string", "description": "The stock ticker symbol"}
                    },
                    "required": ["ticker"]
                }
            },
            {
                "name": "send_email",
                "description": "Send an email to a recipient with a subject and body.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "recipient": {"type": "string", "description": "The email address of the recipient"},
                        "subject": {"type": "string", "description": "The subject of the email"},
                        "body": {"type": "string", "description": "The body of the email"}
                    },
                    "required": ["recipient", "subject", "body"]
                }
            }
        ]
```

```python
# main.py
import json
import threading
from dotenv import load_dotenv
import openai
from genai_observer import GenAIObserver
import tools
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode, SpanKind

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

load_dotenv()

class WorkflowManager:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.observer = GenAIObserver.get_instance()
        self.tool_set = tools.ToolSet.get_instance()
        openai_endpoint = os.getenv('OPENAI_API_ENDPOINT', 'https://api.openai.com')
        proxy_token = os.getenv('OPENAI_PROXY_TOKEN', os.getenv('OPENAI_API_KEY'))
        if not proxy_token:
            raise ValueError("OPENAI_API_KEY or OPENAI_PROXY_TOKEN must be set for OpenAI client")
        headers = {"Authorization": f"Bearer {proxy_token}"}
        http_client = httpx.Client(verify=not self.observer.insecure, headers=headers)
        self.openai_client = openai.OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=openai_endpoint,
            http_client=http_client
        )
        # Dynamically apply workflow decorator
        self.run_workflow = self.workflow(self.run_workflow)

    def workflow(self, func):
        """Function decorator to apply observer.workflow."""
        return self.observer.workflow(func)

    def run_workflow(self, prompt: str, reference: str) -> str:
        """Run the LLM workflow with OpenAI function calls."""
        try:
            messages = [{"role": "user", "content": prompt}]
            tools = self.tool_set.get_tool_schemas()

            # Make OpenAI API call with function tools
            with self.observer.tracer.start_as_current_span("openai_call", kind=SpanKind.CLIENT) as span:
                span.set_attribute("span_type", "llm")
                span.set_attribute("otel.kind", "CLIENT")
                span.set_attribute("llm.model_name", "gpt-4-turbo-preview")
                span.set_attribute("llm.prompt", prompt)
                span.add_event("llm_prompt", {"value": prompt})
                try:
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4-turbo-preview",
                        messages=messages,
                        tools=[{"type": "function", "function": tool} for tool in tools],
                        tool_choice="auto"
                    )
                    # Extract token usage
                    if hasattr(response, 'usage') and response.usage:
                        span.set_attribute("llm.token_count.prompt", response.usage.prompt_tokens)
                        span.set_attribute("llm.token_count.completion", response.usage.completion_tokens)
                        span.set_attribute("llm.token_count.total", response.usage.total_tokens)
                        span.add_event("token_usage", {
                            "prompt_tokens": response.usage.prompt_tokens,
                            "completion_tokens": response.usage.completion_tokens,
                            "total_tokens": response.usage.total_tokens
                        })
                    span.set_attribute("llm.completions", str(response))
                    span.add_event("llm_completion", {"value": str(response)})
                    span.set_status(Status(StatusCode.OK))
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"OpenAI API error: {e}")
                    raise

            # Process tool calls
            tool_calls = response.choices[0].message.tool_calls
            results = []
            if tool_calls:
                for tool_call in tool_calls:
                    func_name = tool_call.function.name
                    args = json.loads(tool_call.function.arguments)
                    try:
                        if func_name == "search":
                            result = self.tool_set.search(args["query"])
                        elif func_name == "calculate":
                            result = self.tool_set.calculate(args["expression"])
                        elif func_name == "fetch_weather":
                            result = self.tool_set.fetch_weather(args["city"])
                        elif func_name == "get_stock_price":
                            result = self.tool_set.get_stock_price(args["ticker"])
                        elif func_name == "send_email":
                            result = self.tool_set.send_email(args["recipient"], args["subject"], args["body"])
                        else:
                            result = f"Unknown function: {func_name}"
                        results.append(f"{func_name}: {result}")
                    except Exception as e:
                        results.append(f"{func_name}: Error - {str(e)}")
                        logger.error(f"Tool call error for {func_name}: {e}")

            # Generate final response
            response_text = f"Based on tools:\n{'\n'.join(results)}\nAnswering prompt: {prompt}"
            return response_text
        except Exception as e:
            logger.error(f"Workflow error: {e}")
            raise

    def run(self, prompt: str, reference: str):
        """Execute the workflow and evaluations."""
        result = self.run_workflow(prompt, reference)
        print("Workflow Result:")
        print(result)
        self.observer.run_offline_evals()
        self.observer.shutdown()

if __name__ == "__main__":
    # Example run with mock prompt and reference
    workflow_manager = WorkflowManager.get_instance()
    prompt = "What is the latest in AI? Also, get the stock price for AAPL, calculate 40 * 2.5, and check the weather in San Francisco."
    reference = "The latest in AI includes advancements in observability frameworks like Arize Phoenix."
    workflow_manager.run(prompt, reference)
```

```python
# requirements.txt
arize-phoenix==0.33.0
arize-otel
arize
opentelemetry-sdk
opentelemetry-exporter-otlp
evaluate
rouge_score
python-dotenv
requests
httpx
protobuf
grpcio
openai>=1.0.0
certifi
```

### Steps to Test
1. **Update .env Files**:
   - **Localhost**:
     ```plaintext
     # .env.local
     OPENAI_API_KEY='sk-'
     OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
     OPENAI_PROXY_TOKEN='your-bearer-token'
     OTEL_EXPORTER_OTLP_ENDPOINT='http://localhost:6006/v1/traces'
     PHOENIX_COLLECTOR_ENDPOINT='http://localhost:6006'
     USE_AX_MODE=false
     ALLOW_INSECURE_CONNECTION=true
     ONLINE_SAMPLE_RATIO=0.1
     PHOENIX_PROJECT_NAME=default
     ```
   - **OCP**:
     ```plaintext
     # .env.ocp
     OPENAI_API_KEY='sk-'
     OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
     OPENAI_PROXY_TOKEN='your-bearer-token'
     OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
     PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
     USE_AX_MODE=false
     ALLOW_INSECURE_CONNECTION=true
     ONLINE_SAMPLE_RATIO=0.1
     PHOENIX_PROJECT_NAME=default
     ```
   - Replace `'https://your-openai-proxy-gateway/api'` and `'your-bearer-token'` with your actual proxy endpoint and token.

2. **Install Dependencies**:
   ```
   pip install -r requirements.txt
   ```

3. **Test Localhost**:
   - Start Phoenix server:
     ```
     python -m phoenix.server.main serve
     ```
     - Confirm logs: `Server running at http://localhost:6006`.
   - Load `.env.local`:
     ```
     export $(cat .env.local | xargs)
     python main.py
     ```
   - Check logs: Look for `Successfully logged <eval_name> evaluation for span_id <span_id>`, `Successfully logged ROUGE evaluation for span_id <span_id>`, and no errors like `Failed to log evaluation`.
   - Verify in Phoenix UI (`http://localhost:6006`):
     - Navigate to the **Evaluations** tab to confirm `QA`, `Hallucination`, `Relevance`, `Toxicity`, `ROUGE1`, and `ROUGEL` results are visible.
     - In the **Trace** tab, check the `workflow` span for evaluation attributes (`eval.<name>.score`, `eval.<name>.label`, `eval.<name>.explanation`) and the `openai_call` span for token attributes (`llm.token_count.prompt`, `llm.token_count.completion`, `llm.token_count.total`).
     - Confirm span kind (`INTERNAL`, `CLIENT`), LLM icons (`workflow`, `openai_call`), and tool icons.

4. **Test OCP**:
   - Load `.env.ocp`:
     ```
     export $(cat .env.ocp | xargs)
     python main.py
     ```
   - Check UI: `https://arize-phoenix-ocp.nonprod.we.net`. Confirm evaluation results and token information.

5. **Check Logs**:
   - For localhost: Expect `Successfully connected to localhost:6006`, `Successfully logged <eval_name> evaluation`, no errors.
   - For OCP: Expect `WARNING: SSL verification is disabled for the OTLP HTTP exporter.`, no `SSLCertVerificationError`.
   - Verify OpenAI function calls and evaluations use the proxy endpoint with custom headers.
   - Confirm evaluation results and token information in the UI.

### If Issues Persist
- **Evaluations Not Visible**:
  - Check the Phoenix UI’s **Evaluations** tab or **Trace** tab for `SpanEvaluations`.
  - Share screenshots of the UI showing where evaluations should appear.
  - Verify logs for `Successfully logged <eval_name> evaluation` or errors like `Failed to log evaluation`.
- **Token Information Missing**:
  - Confirm the OpenAI response includes `usage` (e.g., `response.usage.prompt_tokens`). Share logs if `usage` is missing.
  - Check the `openai_call` span in the UI for `llm.token_count.*` attributes.
- **Proxy Gateway**: Test the proxy endpoint (`curl -H "Authorization: Bearer your-bearer-token" https://your-openai-proxy-gateway/api`) and share any errors.
- **Localhost**: Ensure `python -m phoenix.server.main serve` runs (`curl http://localhost:6006/v1/traces`). Check port conflicts (`lsof -i :6006`).
- **OCP**: Test with `curl -k https://arize-phoenix-ocp.nonprod.we.net/v1/traces` if issues reappear.
- **Debug**: Share the full traceback, logs (especially from `_retry_operation`, `log_evaluation`, or `log_rouge_evaluation`), or UI screenshots if evaluations or tokens are missing.

For UAT/prod, use `USE_AX_MODE=true` with ARIZE keys if needed. Please share the results, any errors, or UI screenshots to help diagnose further!
