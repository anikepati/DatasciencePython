Thank you for your question about why we’re setting span attributes and events (e.g., `current_span.set_attribute("span_type", "llm_workflow")`, `llm.prompt`, etc.) in both `main.py` and `gen_ai_observer.py`, and why we’re manually setting attributes like the model name and prompt instead of extracting them from the OpenAI response. Since the code is working for tracing with both the HTTPS OCP endpoint (`https://arize-phoenix-ocp.nonprod.we.net/v1/traces`) and localhost (`http://localhost:6006/v1/traces`), supports singletons (`GenAIObserver`, `ToolSet`, `WorkflowManager`), OpenAI function calls with a proxy gateway, evaluations with custom headers, and displays most trace details in the Phoenix UI (though span kind, LLM, and tool icons were missing), I’ll address your concerns and streamline the attribute setting to avoid duplication and leverage the OpenAI response where possible.

### Why Attributes Are Set in Two Places
The code currently sets span attributes and events (e.g., `span_type`, `llm.prompt`, `input.value`, `custom.annotation.prompt`, `custom.annotation.reference`, `prompt` event) in both:
1. **`gen_ai_observer.py` (in the `workflow` decorator)**: This applies to the `run_workflow` method in `main.py`, wrapping it to create a `workflow` span with attributes and events for tracing the entire workflow.
2. **`main.py` (in `run_workflow`)**: This redundantly sets the same attributes and events on the same span, duplicating the effort.

**Reason for Duplication**:
- The `workflow` decorator in `gen_ai_observer.py` is designed to handle tracing generically for any function it wraps, setting standard attributes (`llm.prompt`, `input.value`, etc.) to ensure all workflow spans have consistent metadata.
- In `main.py`, `run_workflow` manually sets these attributes again to ensure Phoenix-specific attributes (`span_type="llm_workflow"`, etc.) are present for UI rendering, especially since we added `span_type` to address missing LLM icons.
- This duplication occurred because the initial implementation in `gen_ai_observer.py` was generic, and `main.py` added Phoenix-specific attributes later to fix UI issues (span kind, LLM/tool icons).

**Why This Is Problematic**:
- **Redundancy**: Setting the same attributes in both places is unnecessary and increases maintenance overhead.
- **Consistency Risk**: If attributes are set differently in `gen_ai_observer.py` and `main.py`, it could lead to inconsistent tracing or UI rendering.
- **Performance**: Repeated attribute setting may slightly increase overhead, though minimal.

**Solution**: Move all span attribute and event setting to the `workflow` decorator in `gen_ai_observer.py`, removing the redundant code in `main.py`. This centralizes tracing logic, ensures consistency, and leverages the decorator’s purpose.

### Why Manually Set Model Name and Prompt
The code manually sets the model name (`llm.model`) and prompt (`llm.prompt`, `input.value`) instead of extracting them from the OpenAI response because:
1. **Prompt Availability**: The prompt is passed as an input to `run_workflow` and sent to OpenAI in the `messages` list. Setting it explicitly as `llm.prompt` and `input.value` ensures it’s logged in the span before the OpenAI call, which is useful for debugging if the API call fails.
2. **Model Name**: The model name (`gpt-4-turbo-preview`) is hardcoded in the `chat.completions.create` call. Setting it as `llm.model` ensures it’s recorded in the span, as the OpenAI response doesn’t always include the model name in a directly accessible field (e.g., `response.model` may not be reliable or present).
3. **Phoenix UI Requirements**: Phoenix expects specific attributes (`llm.prompt`, `llm.model`, `llm.response`) to render LLM spans and icons correctly. Manually setting these ensures compatibility, especially since the response’s structure may vary (e.g., tool calls vs. text).
4. **Early Logging**: Setting attributes before the OpenAI call allows partial tracing if the call fails, improving observability.

**Can We Extract from Response?**:
- **Prompt**: The prompt is in `messages`, not the response, so we must set it manually from the input.
- **Model Name**: The OpenAI response may include `model` in some cases (e.g., `response.model`), but it’s not guaranteed across all API versions or configurations. Hardcoding it ensures consistency.
- **Response**: The response content (e.g., `response.choices[0].message.content` or tool calls) is complex and processed into `response_text`. We set `llm.response` and `output.value` manually to capture the final output consistently.

**Solution**: Continue setting `llm.prompt` and `llm.model` manually for reliability, but extract `llm.response` from the processed `response_text`. Centralize this in `gen_ai_observer.py` to avoid duplication.

### Updated Solution
- **Remove Redundant Attributes in `main.py`**:
  - Eliminate `current_span.set_attribute` and `current_span.add_event` calls in `run_workflow`, as they’re handled by the `workflow` decorator in `gen_ai_observer.py`.
- **Enhance `workflow` Decorator**:
  - Set `span_type="llm_workflow"`, `llm.prompt`, `llm.model`, and other attributes in `gen_ai_observer.py`.
  - Use `SpanKind.INTERNAL` for the workflow span and `SpanKind.CLIENT` for the OpenAI call span.
- **Enhance `tool_span` Decorator**:
  - Ensure `span_type="tool"` and `tool.name` are set for tool spans to display tool icons.
- **Keep Phoenix Attributes**:
  - Retain `span_type`, `llm.*`, and `tool.*` attributes for Phoenix UI compatibility.
- **Maintain Features**:
  - Preserve singleton pattern, OpenAI function calls with proxy gateway, evaluations with custom headers, online (10% sampling) and offline evaluations, child spans (`search`, `calculate`, `fetch_weather`, `get_stock_price`, `send_email`), custom annotations (`custom.annotation.*`), and SSL bypass.
- **Update `main.py`, `gen_ai_observer.py`, `tools.py`**:
  - Simplify `main.py` by removing redundant span attribute setting.
  - Update `gen_ai_observer.py` to handle all span attributes and events.
  - Keep `tools.py` as is, since it correctly applies decorators dynamically.

### Updated .env Files
No changes needed, as the issue is code-level. For reference:
- **Localhost**:
  ```plaintext
  # .env.local
  OPENAI_API_KEY='sk-'
  OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
  OPENAI_PROXY_TOKEN='your-bearer-token'
  OTEL_EXPORTER_OTLP_ENDPOINT='http://localhost:6006/v1/traces'
  PHOENIX_COLLECTOR_ENDPOINT='http://localhost:6006'
  USE_AX_MODE=false
  ALLOW_INSECURE_CONNECTION=true
  ONLINE_SAMPLE_RATIO=0.1
  PHOENIX_PROJECT_NAME=default
  ```
- **OCP**:
  ```plaintext
  # .env.ocp
  OPENAI_API_KEY='sk-'
  OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
  OPENAI_PROXY_TOKEN='your-bearer-token'
  OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
  PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
  USE_AX_MODE=false
  ALLOW_INSECURE_CONNECTION=true
  ONLINE_SAMPLE_RATIO=0.1
  PHOENIX_PROJECT_NAME=default
  ```
Replace `'https://your-openai-proxy-gateway/api'` and `'your-bearer-token'` with your actual proxy endpoint and token.

### Updated Code
- **gen_ai_observer.py**: Enhance `workflow` and `tool_span` to set all attributes and events, including `span_type`, `llm.*`, and `tool.*`.
- **tools.py**: Unchanged, as it correctly applies decorators dynamically.
- **main.py**: Remove redundant attribute/event setting, keep OpenAI call and tool processing.
- **requirements.txt**: Unchanged.

```python
# gen_ai_observer.py
import os
import logging
import random
import pandas as pd
from typing import Callable
import json
import httpx
import requests
import certifi
import socket
import threading
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as OTLPSpanExporterHTTP
from opentelemetry.trace import Status, StatusCode, SpanKind

try:
    from phoenix.otel import register as phoenix_register
    import phoenix as px
    from phoenix.trace import SpanEvaluations
except ImportError:
    phoenix_register = None
    px = None
    SpanEvaluations = None

try:
    from arize.otel import register as arize_register
    from arize.pandas.logger import Client as ArizeClient
except ImportError:
    arize_register = None
    ArizeClient = None

from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
    ToxicityEvaluator,
    OpenAIModel,
    run_evals,
)

from rouge_score import rouge_scorer

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Patch requests and httpx to bypass SSL verification when insecure
insecure = os.getenv('ALLOW_INSECURE_CONNECTION', 'false').lower() == 'true'
if insecure:
    logger.warning("Globally disabling SSL verification for requests and httpx")
    orig_request = requests.Session.request
    def patched_request(self, *args, **kwargs):
        kwargs['verify'] = False
        return orig_request(self, *args, **kwargs)
    requests.Session.request = patched_request
    try:
        import openai
        orig_openai_client = openai.OpenAI.__init__
        def patched_openai_init(self, *args, **kwargs):
            kwargs['http_client'] = httpx.Client(verify=False)
            orig_openai_client(self, *args, **kwargs)
        openai.OpenAI.__init__ = patched_openai_init
    except ImportError:
        logger.warning("openai package not installed; evaluations may fail")

custom_cert = os.getenv('CUSTOM_SSL_CERT_FILE', certifi.where())
os.environ['REQUESTS_CA_BUNDLE'] = custom_cert
logger.debug(f"SSL cert set: {custom_cert}")

class GenAIObserver:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.mode = 'ax' if os.getenv('USE_AX_MODE', 'false').lower() == 'true' else 'local'
        self.ui_endpoint = os.getenv('PHOENIX_COLLECTOR_ENDPOINT', 'http://localhost:6006/')
        self.otlp_endpoint = os.getenv('OTEL_EXPORTER_OTLP_ENDPOINT', self.ui_endpoint.rstrip('/') + '/v1/traces')
        self.insecure = os.getenv('ALLOW_INSECURE_CONNECTION', 'false').lower() == 'true'
        self.sample_rate = float(os.getenv('ONLINE_SAMPLE_RATIO', 0.1))
        logger.debug(f"Env config: mode={self.mode}, ui_endpoint={self.ui_endpoint}, otlp_endpoint={self.otlp_endpoint}, insecure={self.insecure}, sample_rate={self.sample_rate}")
        self._test_endpoint(self.otlp_endpoint)
        self.tracer_provider = self.setup_tracer()
        trace.set_tracer_provider(self.tracer_provider)
        self.tracer = trace.get_tracer(__name__)
        self.phoenix_client = None
        self.arize_client = None
        self.setup_client()
        self.stored_traces = []
        self.eval_model = self.setup_eval_model()
        self.evaluators = self.setup_evaluators()

    def _test_endpoint(self, endpoint):
        """Test if the OTLP endpoint is reachable."""
        try:
            import urllib.parse
            parsed = urllib.parse.urlparse(endpoint)
            host = parsed.hostname
            port = parsed.port or (443 if parsed.scheme == 'https' else 80)
            with socket.create_connection((host, port), timeout=2) as sock:
                logger.debug(f"Successfully connected to {host}:{port}")
        except Exception as e:
            logger.error(f"Failed to connect to {endpoint}: {e}")

    def setup_eval_model(self):
        """Set up OpenAIModel with custom OpenAI endpoint and headers."""
        try:
            from openai import OpenAI
        except ImportError:
            logger.error("openai package not installed; evaluations will fail without OPENAI_API_KEY")
            raise ImportError("openai package required for evaluations")

        logger.info("Using custom OpenAI endpoint for evaluations")
        openai_endpoint = os.getenv('OPENAI_API_ENDPOINT', 'https://api.openai.com')
        proxy_token = os.getenv('OPENAI_PROXY_TOKEN', os.getenv('OPENAI_API_KEY'))
        if not proxy_token:
            raise ValueError("OPENAI_API_KEY or OPENAI_PROXY_TOKEN must be set for evaluations")

        # Create custom OpenAI client with headers
        headers = {"Authorization": f"Bearer {proxy_token}"}
        http_client = httpx.Client(verify=not self.insecure, headers=headers)
        openai_client = OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=openai_endpoint,
            http_client=http_client
        )
        return OpenAIModel(model="gpt-4-turbo-preview", openai_client=openai_client)

    def determine_mode(self):
        return self.mode

    def setup_tracer(self):
        exporter_session_kwargs = {'session': httpx.Client(verify=not self.insecure)}
        if self.insecure and self.otlp_endpoint.startswith("https"):
            logger.warning("SSL verification is disabled for the OTLP HTTP exporter.")
            exporter_session_kwargs['session'] = httpx.Client(verify=False)

        if self.mode == 'ax':
            if not arize_register:
                raise ImportError("arize-otel not installed for AX mode")
            logger.info("Using AX mode exporter")
            return arize_register(
                space_id=os.getenv('ARIZE_SPACE_KEY'),
                api_key=os.getenv('ARIZE_API_KEY'),
                endpoint=self.otlp_endpoint,
            )
        else:
            project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            logger.debug(f"Setting up tracer for project: {project_name}")
            if phoenix_register:
                logger.info("Using Phoenix register for local mode")
                return phoenix_register(endpoint=self.otlp_endpoint, protocol='http/protobuf', project_name=project_name)
            else:
                logger.info(f"Using HTTP exporter with endpoint: {self.otlp_endpoint}")
                exporter = OTLPSpanExporterHTTP(
                    endpoint=self.otlp_endpoint,
                    **exporter_session_kwargs
                )
                processor = BatchSpanProcessor(exporter)
                tracer_provider = TracerProvider()
                tracer_provider.add_span_processor(processor)
                return tracer_provider

    def setup_client(self):
        if self.mode == 'ax':
            if not ArizeClient:
                raise ImportError("arize not installed for AX mode")
            space_id = os.getenv('ARIZE_SPACE_KEY') or os.getenv('ARIZE_SPACE_ID')
            api_key = os.getenv('ARIZE_API_KEY')
            self.project_name = os.getenv('PHOENIX_PROJECT_NAME', 'GenAIObserver')
            self.arize_client = ArizeClient(api_key=api_key, space_id=space_id)
            logger.info("Using Arize client for AX mode")
        else:
            if px is None:
                raise ImportError("phoenix not installed for local mode")
            self.project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            self.phoenix_client = px.Client(endpoint=self.ui_endpoint)
            logger.info("Using Phoenix client for local mode")

    def setup_evaluators(self):
        return [
            QAEvaluator(self.eval_model),
            HallucinationEvaluator(self.eval_model),
            RelevanceEvaluator(self.eval_model),
            ToxicityEvaluator(self.eval_model),
        ]

    def _retry_operation(self, operation: Callable, max_retries: int = 3) -> bool:
        for attempt in range(max_retries):
            try:
                operation()
                return True
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt == max_retries - 1:
                    logger.error("Max retries exceeded for operation")
                    return False
        return False

    def _add_score_if_missing(self, eval_df: pd.DataFrame, eval_name: str) -> pd.DataFrame:
        eval_df = eval_df.copy()
        if 'score' not in eval_df.columns:
            if eval_name == 'QA':
                eval_df['score'] = (eval_df['label'] == 'correct').astype(float)
            elif eval_name == 'Hallucination':
                eval_df['score'] = (eval_df['label'] == 'factual').astype(float)
            elif eval_name == 'Relevance':
                eval_df['score'] = (eval_df['label'] == 'relevant').astype(float)
            elif eval_name == 'Toxicity':
                eval_df['score'] = (eval_df['label'] == 'non-toxic').astype(float)
        return eval_df

    def log_evaluation(self, eval_df: pd.DataFrame, eval_name: str) -> bool:
        eval_df = self._add_score_if_missing(eval_df, eval_name)
        eval_df = eval_df.rename_axis("context.span_id")
        if self.mode == 'local' and self.phoenix_client:
            return self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name=eval_name, dataframe=eval_df)
            ))
        elif self.mode == 'ax' and self.arize_client:
            eval_df_log = eval_df[['label', 'score', 'explanation']].copy()
            prefix = f'eval.{eval_name}.'
            eval_df_log = eval_df_log.rename(columns={
                'label': prefix + 'label',
                'score': prefix + 'score',
                'explanation': prefix + 'explanation',
            })
            return self._retry_operation(lambda: self.arize_client.log_evaluations_sync(eval_df_log, self.project_name))
        else:
            logger.warning(f"Cannot log evaluation: {self.mode} client not available")
            return False

    def log_rouge_evaluation(self, rouge_df: pd.DataFrame, offline: bool = False) -> bool:
        rouge_df = rouge_df.set_index("span_id").rename_axis("context.span_id")
        logged = True
        if self.mode == 'local' and self.phoenix_client:
            rouge1_df = rouge_df[['score_rouge1']].rename(columns={'score_rouge1': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGE1", dataframe=rouge1_df)
            )) and logged
            rougeL_df = rouge_df[['score_rougeL']].rename(columns={'score_rougeL': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGEL", dataframe=rougeL_df)
            )) and logged
        elif self.mode == 'ax' and self.arize_client:
            for metric in ['rouge1', 'rougeL']:
                metric_df = rouge_df[[f'score_{metric}']].rename(columns={f'score_{metric}': f'eval.ROUGE_{metric.upper()}.score'})
                logged = self._retry_operation(lambda df=metric_df: self.arize_client.log_evaluations_sync(df, self.project_name)) and logged
        if offline:
            logger.info(f"Offline ROUGE evals: {rouge_df.reset_index().to_dict(orient='records')}")
        else:
            logger.info(f"Online ROUGE eval: {rouge_df.reset_index().to_dict(orient='records')[0]}")
        return logged

    def workflow(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span("workflow", kind=SpanKind.INTERNAL) as span:
                span.set_attribute("span_type", "llm_workflow")
                span.set_attribute("llm.model", "gpt-4-turbo-preview")
                span.set_attribute("llm.prompt", args[0] if args else "")
                span.set_attribute("input.value", args[0] if args else "")
                span.set_attribute("custom.annotation.prompt", args[0] if args else "")
                span.set_attribute("custom.annotation.reference", args[1] if len(args) > 1 else "")
                span.add_event("prompt", {"value": args[0] if args else ""})
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("llm.response", result)
                    span.set_attribute("output.value", result)
                    span.set_attribute("custom.annotation.output", result)
                    span.add_event("response", {"value": result})
                    span.set_status(Status(StatusCode.OK))
                    span_id = str(span.get_span_context().span_id)
                    self.stored_traces.append({
                        'span_id': span_id,
                        'input': args[0] if args else "",
                        'reference': args[1] if len(args) > 1 else "",
                        'output': result,
                    })
                    if random.random() < self.sample_rate:
                        self.run_online_evals(span_id, args[0] if args else "", args[1] if len(args) > 1 else "", result)
                    return result
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"Workflow error: {e}")
                    raise
        return wrapper

    def tool_span(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span(func.__name__, kind=SpanKind.INTERNAL) as span:
                span.set_attribute("span_type", "tool")
                span.set_attribute("tool.name", func.__name__)
                span.set_attribute("input.value", str(args))
                span.set_attribute("custom.annotation.tool_input", str(args))
                span.add_event("tool_input", {"value": str(args)})
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("output.value", str(result))
                    span.set_attribute("custom.annotation.tool_output", str(result))
                    span.add_event("tool_output", {"value": str(result)})
                    span.set_status(Status(StatusCode.OK))
                    return result
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"Tool error in {func.__name__}: {e}")
                    raise
        return wrapper

    def run_online_evals(self, span_id, input_text, reference, output):
        df = pd.DataFrame([{
            "input": input_text,
            "output": output,
            "reference": reference,
            "context": reference,
        }], index=[span_id])
        df.index.name = "context.span_id"
        eval_dfs = run_evals(
            dataframe=df,
            evaluators=self.evaluators,
            provide_explanation=True,
        )
        for eval_df, evaluator in zip(eval_dfs, self.evaluators):
            eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
            if self.log_evaluation(eval_df, eval_name):
                logger.info(f"Online {eval_name} eval logged successfully")
            else:
                logger.warning(f"Failed to log online {eval_name} eval")
        self.run_rouge_eval(df)

    def run_offline_evals(self):
        if not self.stored_traces:
            return
        df = pd.DataFrame(self.stored_traces)
        df["context"] = df["reference"]
        df = df.set_index("span_id")
        df.index.name = "context.span_id"
        eval_dfs = run_evals(
            dataframe=df,
            evaluators=self.evaluators,
            provide_explanation=True,
        )
        for eval_df, evaluator in zip(eval_dfs, self.evaluators):
            eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
            if self.log_evaluation(eval_df, eval_name):
                logger.info(f"Offline {eval_name} evals logged successfully")
            else:
                logger.warning(f"Failed to log offline {eval_name} evals")
        self.run_rouge_eval(df, offline=True)

    def run_rouge_eval(self, df, offline=False):
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
        rouge_data = []
        for span_id, row in df.iterrows():
            pred = row["output"]
            ref = row["reference"]
            scores = scorer.score(ref, pred)
            rouge_data.append({
                "span_id": span_id,
                "score_rouge1": scores["rouge1"].fmeasure,
                "score_rougeL": scores["rougeL"].fmeasure,
            })
        rouge_df = pd.DataFrame(rouge_data)
        self.log_rouge_evaluation(rouge_df, offline=offline)

    def shutdown(self):
        if hasattr(self, 'tracer_provider'):
            self.tracer_provider.shutdown()
        logger.info("Tracer shutdown complete")
```

```python
# tools.py
import threading
import json
from genai_observer import GenAIObserver

class ToolSet:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.observer = GenAIObserver.get_instance()
        # Dynamically apply tool_span decorator to methods
        self.search = self.tool_span(self.search)
        self.calculate = self.tool_span(self.calculate)
        self.fetch_weather = self.tool_span(self.fetch_weather)
        self.get_stock_price = self.tool_span(self.get_stock_price)
        self.send_email = self.tool_span(self.send_email)

    def tool_span(self, func):
        """Function decorator to apply observer.tool_span."""
        return self.observer.tool_span(func)

    def search(self, query: str) -> str:
        """Simulates a web search."""
        return f"Mock search result for: {query}"

    def calculate(self, expression: str) -> float:
        """Simulates a computation."""
        try:
            return eval(expression)
        except Exception as e:
            return f"Error: {e}"

    def fetch_weather(self, city: str) -> str:
        """Simulates an API call for weather."""
        return f"Mock weather in {city}: Sunny, 75°F"

    def get_stock_price(self, ticker: str) -> str:
        """Simulates fetching a stock price for a given ticker."""
        return f"Mock stock price for {ticker}: $100.00"

    def send_email(self, recipient: str, subject: str, body: str) -> str:
        """Simulates sending an email."""
        return f"Mock email sent to {recipient} with subject '{subject}' and body '{body}'"

    def get_tool_schemas(self):
        """Returns JSON schemas for OpenAI function calling."""
        return [
            {
                "name": "search",
                "description": "Perform a web search with a given query.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "The search query"}
                    },
                    "required": ["query"]
                }
            },
            {
                "name": "calculate",
                "description": "Evaluate a mathematical expression.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "expression": {"type": "string", "description": "The mathematical expression to evaluate"}
                    },
                    "required": ["expression"]
                }
            },
            {
                "name": "fetch_weather",
                "description": "Fetch the weather for a given city.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {"type": "string", "description": "The city name"}
                    },
                    "required": ["city"]
                }
            },
            {
                "name": "get_stock_price",
                "description": "Fetch the stock price for a given ticker symbol.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "ticker": {"type": "string", "description": "The stock ticker symbol"}
                    },
                    "required": ["ticker"]
                }
            },
            {
                "name": "send_email",
                "description": "Send an email to a recipient with a subject and body.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "recipient": {"type": "string", "description": "The email address of the recipient"},
                        "subject": {"type": "string", "description": "The subject of the email"},
                        "body": {"type": "string", "description": "The body of the email"}
                    },
                    "required": ["recipient", "subject", "body"]
                }
            }
        ]
```

```python
# main.py
import json
import threading
from dotenv import load_dotenv
import openai
from genai_observer import GenAIObserver
import tools
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode, SpanKind

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

load_dotenv()

class WorkflowManager:
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Use get_instance() to access the singleton instance")
        self.observer = GenAIObserver.get_instance()
        self.tool_set = tools.ToolSet.get_instance()
        openai_endpoint = os.getenv('OPENAI_API_ENDPOINT', 'https://api.openai.com')
        proxy_token = os.getenv('OPENAI_PROXY_TOKEN', os.getenv('OPENAI_API_KEY'))
        if not proxy_token:
            raise ValueError("OPENAI_API_KEY or OPENAI_PROXY_TOKEN must be set for OpenAI client")
        headers = {"Authorization": f"Bearer {proxy_token}"}
        http_client = httpx.Client(verify=not self.observer.insecure, headers=headers)
        self.openai_client = openai.OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=openai_endpoint,
            http_client=http_client
        )
        # Dynamically apply workflow decorator
        self.run_workflow = self.workflow(self.run_workflow)

    def workflow(self, func):
        """Function decorator to apply observer.workflow."""
        return self.observer.workflow(func)

    def run_workflow(self, prompt: str, reference: str) -> str:
        """Run the LLM workflow with OpenAI function calls."""
        try:
            messages = [{"role": "user", "content": prompt}]
            tools = self.tool_set.get_tool_schemas()

            # Make OpenAI API call with function tools
            with self.observer.tracer.start_as_current_span("openai_call", kind=SpanKind.CLIENT) as span:
                span.set_attribute("span_type", "llm")
                span.set_attribute("llm.model", "gpt-4-turbo-preview")
                span.set_attribute("input.value", prompt)
                span.add_event("llm_prompt", {"value": prompt})
                try:
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4-turbo-preview",
                        messages=messages,
                        tools=[{"type": "function", "function": tool} for tool in tools],
                        tool_choice="auto"
                    )
                    span.set_attribute("output.value", str(response))
                    span.add_event("llm_response", {"value": str(response)})
                    span.set_status(Status(StatusCode.OK))
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR, str(e)))
                    span.add_event("error", {"message": str(e)})
                    logger.error(f"OpenAI API error: {e}")
                    raise

            # Process tool calls
            tool_calls = response.choices[0].message.tool_calls
            results = []
            if tool_calls:
                for tool_call in tool_calls:
                    func_name = tool_call.function.name
                    args = json.loads(tool_call.function.arguments)
                    try:
                        if func_name == "search":
                            result = self.tool_set.search(args["query"])
                        elif func_name == "calculate":
                            result = self.tool_set.calculate(args["expression"])
                        elif func_name == "fetch_weather":
                            result = self.tool_set.fetch_weather(args["city"])
                        elif func_name == "get_stock_price":
                            result = self.tool_set.get_stock_price(args["ticker"])
                        elif func_name == "send_email":
                            result = self.tool_set.send_email(args["recipient"], args["subject"], args["body"])
                        else:
                            result = f"Unknown function: {func_name}"
                        results.append(f"{func_name}: {result}")
                    except Exception as e:
                        results.append(f"{func_name}: Error - {str(e)}")
                        logger.error(f"Tool call error for {func_name}: {e}")

            # Generate final response
            response_text = f"Based on tools:\n{'\n'.join(results)}\nAnswering prompt: {prompt}"
            return response_text
        except Exception as e:
            logger.error(f"Workflow error: {e}")
            raise

    def run(self, prompt: str, reference: str):
        """Execute the workflow and evaluations."""
        result = self.run_workflow(prompt, reference)
        print("Workflow Result:")
        print(result)
        self.observer.run_offline_evals()
        self.observer.shutdown()

if __name__ == "__main__":
    # Example run with mock prompt and reference
    workflow_manager = WorkflowManager.get_instance()
    prompt = "What is the latest in AI? Also, get the stock price for AAPL, calculate 40 * 2.5, and check the weather in San Francisco."
    reference = "The latest in AI includes advancements in observability frameworks like Arize Phoenix."
    workflow_manager.run(prompt, reference)
```

```python
# requirements.txt
arize-phoenix==0.33.0
arize-otel
arize
opentelemetry-sdk
opentelemetry-exporter-otlp
evaluate
rouge_score
python-dotenv
requests
httpx
protobuf
grpcio
openai>=1.0.0
certifi
```

### Steps to Test
1. **Update .env Files**:
   - **Localhost**:
     ```plaintext
     # .env.local
     OPENAI_API_KEY='sk-'
     OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
     OPENAI_PROXY_TOKEN='your-bearer-token'
     OTEL_EXPORTER_OTLP_ENDPOINT='http://localhost:6006/v1/traces'
     PHOENIX_COLLECTOR_ENDPOINT='http://localhost:6006'
     USE_AX_MODE=false
     ALLOW_INSECURE_CONNECTION=true
     ONLINE_SAMPLE_RATIO=0.1
     PHOENIX_PROJECT_NAME=default
     ```
   - **OCP**:
     ```plaintext
     # .env.ocp
     OPENAI_API_KEY='sk-'
     OPENAI_API_ENDPOINT='https://your-openai-proxy-gateway/api'
     OPENAI_PROXY_TOKEN='your-bearer-token'
     OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
     PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
     USE_AX_MODE=false
     ALLOW_INSECURE_CONNECTION=true
     ONLINE_SAMPLE_RATIO=0.1
     PHOENIX_PROJECT_NAME=default
     ```
   - Replace `'https://your-openai-proxy-gateway/api'` and `'your-bearer-token'` with your actual proxy endpoint and token.

2. **Install Dependencies**:
   ```
   pip install -r requirements.txt
   ```

3. **Test Localhost**:
   - Start Phoenix server:
     ```
     python -m phoenix.server.main serve
     ```
     - Confirm logs: `Server running at http://localhost:6006`.
   - Load `.env.local`:
     ```
     export $(cat .env.local | xargs)
     python main.py
     ```
   - Check logs: `Using HTTP exporter with endpoint: http://localhost:6006/v1/traces`, no errors.
   - Verify traces, child spans (`search`, `calculate`, `fetch_weather`, `get_stock_price`, `send_email`), and evaluations in UI: `http://localhost:6006`. Confirm span kind (`INTERNAL`, `CLIENT`), LLM icons (for `openai_call`, `workflow`), and tool icons (for tool spans).

4. **Test OCP**:
   - Load `.env.ocp`:
     ```
     export $(cat .env.ocp | xargs)
     python main.py
     ```
   - Check UI: `https://arize-phoenix-ocp.nonprod.we.net`. Confirm span kind, LLM icons, and tool icons.

5. **Check Logs**:
   - For localhost: Expect `Successfully connected to localhost:6006`, no errors.
   - For OCP: Expect `WARNING: SSL verification is disabled for the OTLP HTTP exporter.`, no `SSLCertVerificationError`.
   - Verify OpenAI function calls and evaluations use the proxy endpoint with custom headers.
   - Confirm span kind, LLM icons, and tool icons in the UI.

### If Issues Persist
- **Span Kind/LLM/Tool Icons**: If still missing, check the Phoenix UI for specific attribute requirements (e.g., `span_type`, `otel.kind`). Share UI screenshots or logs showing which spans lack icons.
- **Proxy Gateway**: Test the proxy endpoint (`curl -H "Authorization: Bearer your-bearer-token" https://your-openai-proxy-gateway/api`) and share any errors.
- **Localhost**: Ensure `python -m phoenix.server.main serve` runs (`curl http://localhost:6006/v1/traces`). Check port conflicts (`lsof -i :6006`).
- **OCP**: Test with `curl -k https://arize-phoenix-ocp.nonprod.we.net/v1/traces` if issues reappear.
- **Debug**: Share the full traceback or UI screenshots if icons or span details are missing.

For UAT/prod, use `USE_AX_MODE=true` with ARIZE keys—no SSL issues with cloud endpoints. Let me know the results or any new errors!
