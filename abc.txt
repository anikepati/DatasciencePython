Apologies for the incomplete code in the previous response—it seems the `gen_ai_observer.py` file was cut off. The error you're encountering (`SSLCertVerificationError`) despite `ALLOW_INSECURE_CONNECTION=true` suggests that the SSL bypass is not being applied consistently, possibly due to the Phoenix client or OpenAI client not honoring the `verify=False` setting. Since your Java application connects to `https://arize-phoenix-ocp.nonprod.we.net/v1/traces` without issues using `otlpHttpSpanExporter`, and you don’t have a proxy API key for OpenAI, we’ll simplify the Python setup to mimic the Java behavior by:
- Using a single `OTEL_EXPORTER_OTLP_ENDPOINT` for trace exports.
- Explicitly bypassing SSL verification with `httpx.Client(verify=False)` for all HTTPS requests (OTLP, Phoenix client, and OpenAI).
- Removing all proxy-related logic (`HTTP_PROXY`, `HTTPS_PROXY`, `PROXY_BASE_URL`, `PROXY_API_KEY`) since you don’t need them.
- Keeping online (10% sampling) and offline evaluations, child spans attached to the parent workflow span, and custom annotations.
- Using `certifi.where()` as a fallback for trusted CAs, but prioritizing `verify=False` to avoid cert errors.

### Updated .env
Simplified to remove proxy settings and focus on SSL bypass for the Phoenix endpoint.

```plaintext
# .env
OPENAI_API_KEY='sk-'
OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
USE_AX_MODE=false
ALLOW_INSECURE_CONNECTION=true
ONLINE_SAMPLE_RATIO=0.1
PHOENIX_PROJECT_NAME=default
# CUSTOM_HEADERS_JSON='{"X-Custom-Header": "value"}'  # Uncomment if needed
```

### Updated Code
The main changes are in `gen_ai_observer.py` to:
- Remove all proxy-related code (`proxies`, `HTTP_PROXY`, `HTTPS_PROXY`).
- Use `httpx.Client(verify=False)` for OTLP exports when `insecure=True`.
- Create a custom `requests.Session` with `verify=False` for the Phoenix client.
- Ensure OpenAI client uses `httpx.Client(verify=False)` for direct API calls.
- Retain evaluations, child spans, and annotations.

```python
# gen_ai_observer.py
import os
import logging
import random
import pandas as pd
from typing import Callable
import json
import httpx
import requests
import certifi

from dotenv import load_dotenv

load_dotenv()

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as OTLPSpanExporterHTTP

try:
    from phoenix.otel import register as phoenix_register
    import phoenix as px
    from phoenix.trace import SpanEvaluations
except ImportError:
    phoenix_register = None
    px = None
    SpanEvaluations = None

try:
    from arize.otel import register as arize_register
    from arize.pandas.logger import Client as ArizeClient
except ImportError:
    arize_register = None
    ArizeClient = None

from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
    ToxicityEvaluator,
    OpenAIModel,
    run_evals,
)

from rouge_score import rouge_scorer

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

custom_cert = os.getenv('CUSTOM_SSL_CERT_FILE', certifi.where())
os.environ['REQUESTS_CA_BUNDLE'] = custom_cert
logger.debug(f"SSL cert set: {custom_cert}")

class GenAIObserver:
    def __init__(self):
        self.mode = 'ax' if os.getenv('USE_AX_MODE', 'false').lower() == 'true' else 'local'
        self.ui_endpoint = os.getenv('PHOENIX_COLLECTOR_ENDPOINT', 'http://localhost:6006/')
        self.otlp_endpoint = os.getenv('OTEL_EXPORTER_OTLP_ENDPOINT')
        if not self.otlp_endpoint:
            raise ValueError("OTEL_EXPORTER_OTLP_ENDPOINT is not set")
        self.insecure = os.getenv('ALLOW_INSECURE_CONNECTION', 'false').lower() == 'true'
        self.sample_rate = float(os.getenv('ONLINE_SAMPLE_RATIO', 0.1))
        logger.debug(f"Env config: mode={self.mode}, ui_endpoint={self.ui_endpoint}, otlp_endpoint={self.otlp_endpoint}, insecure={self.insecure}, sample_rate={self.sample_rate}")
        self.tracer_provider = self.setup_tracer()
        trace.set_tracer_provider(self.tracer_provider)
        self.tracer = trace.get_tracer(__name__)
        self.phoenix_client = None
        self.arize_client = None
        self.setup_client()
        self.stored_traces = []
        self.eval_model = self.setup_eval_model()
        self.evaluators = self.setup_evaluators()

    def setup_eval_model(self):
        """Set up OpenAIModel with direct OpenAI API."""
        try:
            from openai import OpenAI
        except ImportError:
            logger.error("openai package not installed; evaluations will fail without OPENAI_API_KEY")
            raise ImportError("openai package required for evaluations")
        
        logger.info("Using direct OpenAI API for evaluations")
        openai_client = OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            http_client=httpx.Client(verify=not self.insecure)
        )
        return OpenAIModel(model="gpt-4-turbo-preview", openai_client=openai_client)

    def determine_mode(self):
        return self.mode

    def setup_tracer(self):
        exporter_session_kwargs = {'session': httpx.Client(verify=not self.insecure)}
        if self.insecure and self.otlp_endpoint.startswith("https"):
            logger.warning("SSL verification is disabled for the OTLP HTTP exporter.")
            exporter_session_kwargs['session'] = httpx.Client(verify=False)

        if self.mode == 'ax':
            if not arize_register:
                raise ImportError("arize-otel not installed for AX mode")
            logger.info("Using AX mode exporter")
            return arize_register(
                space_id=os.getenv('ARIZE_SPACE_KEY'),
                api_key=os.getenv('ARIZE_API_KEY'),
                endpoint=self.otlp_endpoint,
            )
        else:
            project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            logger.debug(f"Setting up tracer for project: {project_name}")
            if phoenix_register:
                logger.info("Using Phoenix register for local mode")
                return phoenix_register(endpoint=self.otlp_endpoint, protocol='http/protobuf', project_name=project_name)
            else:
                logger.info(f"Using HTTP exporter with endpoint: {self.otlp_endpoint}")
                exporter = OTLPSpanExporterHTTP(
                    endpoint=self.otlp_endpoint,
                    **exporter_session_kwargs
                )
                processor = BatchSpanProcessor(exporter)
                tracer_provider = TracerProvider()
                tracer_provider.add_span_processor(processor)
                return tracer_provider

    def setup_client(self):
        if self.mode == 'ax':
            if not ArizeClient:
                raise ImportError("arize not installed for AX mode")
            space_id = os.getenv('ARIZE_SPACE_KEY') or os.getenv('ARIZE_SPACE_ID')
            api_key = os.getenv('ARIZE_API_KEY')
            self.project_name = os.getenv('ARIZE_PROJECT_NAME', 'GenAIObserver')
            self.arize_client = ArizeClient(api_key=api_key, space_id=space_id)
            logger.info("Using Arize client for AX mode")
        else:
            if px is None:
                raise ImportError("phoenix not installed for local mode")
            self.project_name = os.getenv('PHOENIX_PROJECT_NAME', 'default')
            session = requests.Session()
            session.verify = not self.insecure
            logger.debug(f"Phoenix client session: verify={session.verify}")
            self.phoenix_client = px.Client(endpoint=self.ui_endpoint, session=session)
            logger.info("Using Phoenix client for local mode")

    def setup_evaluators(self):
        return [
            QAEvaluator(self.eval_model),
            HallucinationEvaluator(self.eval_model),
            RelevanceEvaluator(self.eval_model),
            ToxicityEvaluator(self.eval_model),
        ]

    def _retry_operation(self, operation: Callable, max_retries: int = 3) -> bool:
        for attempt in range(max_retries):
            try:
                operation()
                return True
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt == max_retries - 1:
                    logger.error("Max retries exceeded for operation")
                    return False
        return False

    def _add_score_if_missing(self, eval_df: pd.DataFrame, eval_name: str) -> pd.DataFrame:
        eval_df = eval_df.copy()
        if 'score' not in eval_df.columns:
            if eval_name == 'QA':
                eval_df['score'] = (eval_df['label'] == 'correct').astype(float)
            elif eval_name == 'Hallucination':
                eval_df['score'] = (eval_df['label'] == 'factual').astype(float)
            elif eval_name == 'Relevance':
                eval_df['score'] = (eval_df['label'] == 'relevant').astype(float)
            elif eval_name == 'Toxicity':
                eval_df['score'] = (eval_df['label'] == 'non-toxic').astype(float)
        return eval_df

    def log_evaluation(self, eval_df: pd.DataFrame, eval_name: str) -> bool:
        eval_df = self._add_score_if_missing(eval_df, eval_name)
        eval_df = eval_df.rename_axis("context.span_id")
        if self.mode == 'local' and self.phoenix_client:
            return self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name=eval_name, dataframe=eval_df)
            ))
        elif self.mode == 'ax' and self.arize_client:
            eval_df_log = eval_df[['label', 'score', 'explanation']].copy()
            prefix = f'eval.{eval_name}.'
            eval_df_log = eval_df_log.rename(columns={
                'label': prefix + 'label',
                'score': prefix + 'score',
                'explanation': prefix + 'explanation',
            })
            return self._retry_operation(lambda: self.arize_client.log_evaluations_sync(eval_df_log, self.project_name))
        else:
            logger.warning(f"Cannot log evaluation: {self.mode} client not available")
            return False

    def log_rouge_evaluation(self, rouge_df: pd.DataFrame, offline: bool = False) -> bool:
        rouge_df = rouge_df.set_index("span_id").rename_axis("context.span_id")
        logged = True
        if self.mode == 'local' and self.phoenix_client:
            rouge1_df = rouge_df[['score_rouge1']].rename(columns={'score_rouge1': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGE1", dataframe=rouge1_df)
            )) and logged
            rougeL_df = rouge_df[['score_rougeL']].rename(columns={'score_rougeL': 'score'})
            logged = self._retry_operation(lambda: self.phoenix_client.log_evaluations(
                SpanEvaluations(eval_name="ROUGEL", dataframe=rougeL_df)
            )) and logged
        elif self.mode == 'ax' and self.arize_client:
            for metric in ['rouge1', 'rougeL']:
                metric_df = rouge_df[[f'score_{metric}']].rename(columns={f'score_{metric}': f'eval.ROUGE_{metric.upper()}.score'})
                logged = self._retry_operation(lambda df=metric_df: self.arize_client.log_evaluations_sync(df, self.project_name)) and logged
        if offline:
            logger.info(f"Offline ROUGE evals: {rouge_df.reset_index().to_dict(orient='records')}")
        else:
            logger.info(f"Online ROUGE eval: {rouge_df.reset_index().to_dict(orient='records')[0]}")
        return logged

    def workflow(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span("workflow") as span:
                span.set_attribute("custom.annotation.prompt", args[0] if args else "")
                span.set_attribute("custom.annotation.reference", args[1] if len(args) > 1 else "")
                result = func(*args, **kwargs)
                span.set_attribute("custom.annotation.output", result)
                span_id = str(span.get_span_context().span_id)
                self.stored_traces.append({
                    'span_id': span_id,
                    'input': args[0] if args else "",
                    'reference': args[1] if len(args) > 1 else "",
                    'output': result,
                })
                if random.random() < self.sample_rate:
                    self.run_online_evals(span_id, args[0] if args else "", args[1] if len(args) > 1 else "", result)
            return result
        return wrapper

    def tool_span(self, func):
        def wrapper(*args, **kwargs):
            with self.tracer.start_as_current_span(func.__name__) as span:
                span.set_attribute("custom.annotation.tool_input", str(args))
                result = func(*args, **kwargs)
                span.set_attribute("custom.annotation.tool_output", str(result))
                return result
        return wrapper

    def run_online_evals(self, span_id, input_text, reference, output):
        df = pd.DataFrame([{
            "input": input_text,
            "output": output,
            "reference": reference,
            "context": reference,
        }], index=[span_id])
        df.index.name = "context.span_id"
        eval_dfs = run_evals(
            dataframe=df,
            evaluators=self.evaluators,
            provide_explanation=True,
        )
        for eval_df, evaluator in zip(eval_dfs, self.evaluators):
            eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
            if self.log_evaluation(eval_df, eval_name):
                logger.info(f"Online {eval_name} eval logged successfully")
            else:
                logger.warning(f"Failed to log online {eval_name} eval")
        self.run_rouge_eval(df)

    def run_offline_evals(self):
        if not self.stored_traces:
            return
        df = pd.DataFrame(self.stored_traces)
        df["context"] = df["reference"]
        df = df.set_index("span_id")
        df.index.name = "context.span_id"
        eval_dfs = run_evals(
            dataframe=df,
            evaluators=self.evaluators,
            provide_explanation=True,
        )
        for eval_df, evaluator in zip(eval_dfs, self.evaluators):
            eval_name = evaluator.__class__.__name__.replace("Evaluator", "")
            if self.log_evaluation(eval_df, eval_name):
                logger.info(f"Offline {eval_name} evals logged successfully")
            else:
                logger.warning(f"Failed to log offline {eval_name} evals")
        self.run_rouge_eval(df, offline=True)

    def run_rouge_eval(self, df, offline=False):
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
        rouge_data = []
        for span_id, row in df.iterrows():
            pred = row["output"]
            ref = row["reference"]
            scores = scorer.score(ref, pred)
            rouge_data.append({
                "span_id": span_id,
                "score_rouge1": scores["rouge1"].fmeasure,
                "score_rougeL": scores["rougeL"].fmeasure,
            })
        rouge_df = pd.DataFrame(rouge_data)
        self.log_rouge_evaluation(rouge_df, offline=offline)

    def shutdown(self):
        if hasattr(self, 'tracer_provider'):
            self.tracer_provider.shutdown()
        logger.info("Tracer shutdown complete")
```

```python
# tools.py
def get_tools(observer):
    @observer.tool_span
    def search_tool(query: str) -> str:
        """Simulates a web search."""
        return f"Mock search result for: {query}"

    @observer.tool_span
    def calculator_tool(expression: str) -> float:
        """Simulates a computation."""
        try:
            return eval(expression)
        except Exception as e:
            return f"Error: {e}"

    @observer.tool_span
    def fetch_weather_tool(city: str) -> str:
        """Simulates an API call for weather."""
        return f"Mock weather in {city}: Sunny, 75°F"

    return search_tool, calculator_tool, fetch_weather_tool
```

```python
# main.py
from dotenv import load_dotenv

load_dotenv()

from genai_observer import GenAIObserver
import tools

observer = GenAIObserver()
search_tool, calculator_tool, fetch_weather_tool = tools.get_tools(observer)

@observer.workflow
def llm_workflow(prompt: str, reference: str) -> str:
    # Mock multi-tool workflow
    search_result = search_tool("latest AI news")
    calc_result = calculator_tool("40 * 2.5")
    weather_result = fetch_weather_tool("San Francisco")
    
    # Mock LLM response generation
    response = f"Based on tools:\n- Search: {search_result}\n- Calc: {calc_result}\n- Weather: {weather_result}\nAnswering prompt: {prompt}"
    
    return response

if __name__ == "__main__":
    # Example run with mock prompt and reference for evaluations
    prompt = "What is the latest in AI?"
    reference = "The latest in AI includes advancements in observability frameworks like Arize Phoenix."
    result = llm_workflow(prompt, reference)
    print("Workflow Result:")
    print(result)
    observer.run_offline_evals()
    observer.shutdown()
```

```python
# requirements.txt
arize-phoenix==0.33.0
arize-otel
arize
opentelemetry-sdk
opentelemetry-exporter-otlp
evaluate
rouge_score
python-dotenv
requests
httpx
protobuf
grpcio
openai>=1.0.0
certifi
```

### Steps to Test
1. **Update .env**:
   ```plaintext
   OPENAI_API_KEY='sk-'
   OTEL_EXPORTER_OTLP_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net/v1/traces'
   PHOENIX_COLLECTOR_ENDPOINT='https://arize-phoenix-ocp.nonprod.we.net'
   USE_AX_MODE=false
   ALLOW_INSECURE_CONNECTION=true
   ONLINE_SAMPLE_RATIO=0.1
   PHOENIX_PROJECT_NAME=default
   ```

2. **Install Dependencies**:
   ```
   pip install -r requirements.txt
   ```

3. **Run Phoenix Server** (if local testing):
   ```
   python -m phoenix.server.main serve
   ```

4. **Run Script**:
   ```
   python main.py
   ```

5. **Check Logs**:
   - Expect: `WARNING: SSL verification is disabled for the OTLP HTTP exporter.` and no `SSLCertVerificationError`.
   - Look for successful trace exports and eval logs.
   - Verify traces, child spans (`search_tool`, `calculator_tool`, `fetch_weather_tool`), and annotations in the Phoenix UI (`https://arize-phoenix-ocp.nonprod.we.net`).

### If It Still Fails
- **Verify Server**: Ensure the Phoenix server is running on OCP (`oc logs <phoenix-pod>`) and `/v1/traces` is accessible (`curl -k https://arize-phoenix-ocp.nonprod.we.net/v1/traces`).
- **Java Config**: Share the full Java OTLP setup (e.g., any custom SSL or trust settings) for precise comparison.
- **Debug**: If errors persist, share the full traceback—look for SSL, connection, or timeout messages.
- **Network**: Check if your corporate network requires a proxy (despite your preference to avoid it). If needed, add `HTTP_PROXY`/`HTTPS_PROXY` back to `.env`.

For UAT/prod with AX mode, use `USE_AX_MODE=true` with ARIZE keys—cloud endpoints have trusted certs, avoiding SSL issues. Let me know the results or any new errors!
