graph TD
    A[User] -->|Upload Documents| B[QAA S3 Bucket]
    B --> C[TTA Application UI]
    C -->|Post via QAA API| D[QAA API]
    D -->|Check Extraction Readiness| E[Extraction Agent]
    E --> F[Data Aggregator]
    F --> G[Answering & Reasoning Agent]
    G --> H[Human in the Loop - TTA]
    H -->|Validate Results| I[Target System]
    I --> J[Completed QA Review]
    J --> K[Supervisory Review - QA Agent]
    K --> L[Resolved Disputes]
    L --> M[Final Output]

    subgraph QA_Agent_Process
        E --> F
        F --> G
        K --> L
    end

    subgraph TTA_Process
        C --> D
        D --> E
        H --> I
    end


sequenceDiagram
    participant U as User
    participant T as TTA Application UI
    participant S as QAA S3 Bucket
    participant A as QAA API
    participant E as Extraction Agent
    participant D as Data Aggregator
    participant R as Answering & Reasoning Agent
    participant H as Human in the Loop
    participant TS as Target System
    participant SR as Supervisory Review

    U->>T: Upload Documents
    T->>S: Store Documents
    T->>A: Post Document (Batch ID)
    A->>E: Check Extraction Readiness
    E->>D: Aggregate Data
    D->>R: Process and Reason
    R->>H: Send Results for Validation
    H->>TS: Validate and Send to Target System
    TS->>SR: Submit for Supervisory Review
    SR->>TS: Resolve Disputes
    TS->>U: Return Completed QA Review


To compare the application design and architecture of the QA Agent system (as detailed in the provided images) with another application (e.g., the TTA Application you described), and to provide insights for upper management on similarities, reusable components, integration possibilities, and other relevant features, let’s break this down step by step. The goal is to analyze the QA Agent architecture, align it with the proposed TTA Application workflow, and suggest a process for data extraction and analysis, potentially using Streamlit for document handling.

### Analysis of QA Agent Architecture
The QA Agent system, as outlined in the executive summary and related slides, is an AI-driven solution aimed at automating quality assurance (QA) processes across multiple business units (e.g., BRES IMU - Retail Mortgage, Fraud & Claims Modernization, Wholesale Operations). Key features include:
- **Automation with Agentic AI**: Utilizes intelligent agents for real-time data validation, exception handling, and monitoring.
- **Scalability**: Designed to handle large volumes (e.g., ~1,400 to ~242,000 reviews monthly) with reduced manual effort.
- **Integration**: Leverages existing systems (e.g., SAS, iCMP, CORE Portal) and data sources (e.g., Filenet, SharePoint).
- **Human-in-the-Loop (HITL)**: Incorporates supervisory review to resolve disputes and ensure accuracy.
- **Modular Design**: Includes layers like Agentic Orchestration, Onboarding & Business Logic, and data aggregation agents.
- **Non-Functional Requirements**: Emphasizes performance (sub-30s response), scalability, availability (99.9% uptime), security, adoptability, and observability.
- **Timeline and Cost**: Structured with MVPs, JAD pre-requirements, and a funding ask of ~$1.5MM, with phased delivery from April 2025 to Q1'26.

### Proposed TTA Application Workflow
Based on your description, the TTA Application is a UI-focused system with the following process:
- **UI Application (TTA)**: Serves as the user interface for uploading documents.
- **Storage**: Documents are stored in a QAA S3 Bucket.
- **API Integration**: Uses the QAA API to post documents and check extraction readiness via a batch ID.
- **Validation**: Employs System of Record (SOR) validation with uploaded documents.
- **HITL**: Human-in-the-loop validation of results within the TTA Application.

### Comparison of Architectures
#### Similarities
1. **AI-Driven Automation**: Both QA Agent and TTA rely on intelligent agents or AI for automating data extraction and validation processes.
2. **HITL Integration**: Both incorporate human oversight to handle exceptions and validate results, ensuring quality and compliance.
3. **Data Storage and Retrieval**: QA Agent uses existing data stores (e.g., Filenet, SharePoint), while TTA uses an S3 Bucket, indicating a shared reliance on scalable storage solutions.
4. **Real-Time Processing**: Both aim for efficient, real-time data handling and validation.
5. **Modular Design**: QA Agent’s layered architecture (Orchestration, Onboarding) aligns with TTA’s separation of UI, storage, and API layers.

#### Reusable Components
1. **Agentic AI Framework**: The QA Agent’s intelligent agents (e.g., Data Aggregator, Answering & Reasoning Agent) could be adapted for TTA’s extraction and validation tasks.
2. **Data Extraction Logic**: The QA Agent’s ability to extract data from multiple sources could be reused in TTA for processing documents from the S3 Bucket.
3. **HITL Module**: The supervisory review process in QA Agent can be integrated into TTA for result validation.
4. **API Integration Layer**: QA Agent’s use of APIs (e.g., with iCMP) can be extended to support TTA’s QAA API interactions.
5. **Monitoring and Analytics**: The observability features (logging, auditing) from QA Agent can enhance TTA’s result validation tracking.

#### Integration Possibilities
1. **S3 Bucket Integration**: QA Agent can be enhanced to store and retrieve documents from the QAA S3 Bucket, aligning with TTA’s storage model.
2. **QAA API Connectivity**: Extend QA Agent’s orchestration layer to interface with the QAA API for posting documents and retrieving batch IDs.
3. **SOR Validation**: Integrate QA Agent’s validation logic with TTA’s SOR validation process, leveraging existing data aggregation agents.
4. **UI Enhancement**: Incorporate TTA’s UI into QA Agent’s workflow, allowing users to upload documents directly and monitor HITL validation within a unified interface.
5. **Scalability Sync**: Align TTA’s batch processing with QA Agent’s scalable architecture to handle varying document volumes.

#### Additional Features for Management Decision-Making
- **Cost Efficiency**: Combining QA Agent’s $1.5MM funding model with TTA’s development could optimize resource allocation, potentially reducing overall costs.
- **Compliance and Security**: Both systems’ focus on security and compliance (e.g., role-based access, audit logging) can be standardized across applications.
- **Timeline Alignment**: Integrating TTA into QA Agent’s timeline (April 2025 - Q1'26) could accelerate TTA’s deployment, leveraging existing MVPs.
- **Performance Metrics**: Share QA Agent’s sub-30s response time and 99.9% uptime as benchmarks for TTA’s performance goals.
- **Adoptability**: QA Agent’s phased rollout (MVP1, MVP2) can guide TTA’s adoption strategy, ensuring smooth transition for users.

### Process for Data Extraction and Analysis
1. **Document Upload**: Use Streamlit to create a UI where users can upload PDF/PPX files for both QA Agent and TTA workflows.
2. **Data Extraction**: Extract text and images from uploaded documents using Python libraries (e.g., PyPDF2, pdf2image) or leverage QA Agent’s extraction agents.
3. **Comparison Analysis**: Compare the extracted data against predefined criteria (e.g., QA Agent’s validation rules, TTA’s SOR validation) to identify similarities and differences.
4. **Insight Generation**: Analyze metrics (e.g., review duration, dispute resolution) to provide insights on integration benefits and reusable components.
5. **HITL Validation**: Implement a HITL step in Streamlit to allow supervisors to review and validate results, aligning with both systems’ workflows.
6. **Reporting**: Generate a summary report for upper management, highlighting cost savings, scalability, and integration feasibility.

### Next Steps
Before proceeding with code generation (e.g., Streamlit implementation), I recommend confirming the following:
- Are you okay with using Streamlit for this process?
- Do you have specific document samples or additional TTA details to refine the comparison?
- Should I focus on a particular aspect (e.g., API integration, HITL) for the initial code?

Let me know your preferences, and I can proceed with generating the appropriate code or further tailoring the analysis!


1. U (User)

Role: The end-user or operator interacting with the system, typically a QA analyst, supervisor, or business unit owner.
Actions:

Initiates the process by uploading documents (e.g., PDF or PPX files) to the TTA Application UI.
Receives the final output (Completed QA Review) after the process is finalized.


Context: Represents human input and oversight, aligning with the Human-in-the-Loop (HITL) concept in both QA Agent and TTA workflows.

2. T (TTA Application UI)

Role: The user interface component of the TTA Application, serving as the front-end where users upload documents and monitor the process.
Actions:

Receives document uploads from the User.
Stores documents in the QAA S3 Bucket.
Posts the documents to the QAA API with a batch ID for processing.


Context: Acts as the entry point for the TTA workflow, facilitating user interaction and integrating with backend services like the QAA API.

3. S (QAA S3 Bucket)

Role: A cloud-based storage service (e.g., Amazon S3) used to store uploaded documents securely.
Actions:

Accepts and stores documents uploaded via the TTA Application UI.
Serves as a data repository accessible by subsequent components (e.g., Extraction Agent).


Context: Provides scalable, persistent storage, aligning with the QA Agent’s use of existing data stores (e.g., Filenet, SharePoint) and supporting the TTA’s storage needs.

4. A (QAA API)

Role: An application programming interface that facilitates communication between the TTA Application UI and backend processing components.
Actions:

Receives document posts and batch IDs from the TTA Application UI.
Checks the extraction readiness with the Extraction Agent.


Context: Acts as a middleware layer, similar to how QA Agent integrates with systems like iCMP, enabling seamless data flow and process orchestration.

5. E (Extraction Agent)

Role: An intelligent agent (part of the QA Agent system) responsible for extracting data from uploaded documents.
Actions:

Processes the documents from the QAA S3 Bucket based on the batch ID.
Passes extracted data to the Data Aggregator.


Context: Leverages AI capabilities (e.g., LLM calls) to perform initial data extraction, a key feature in both QA Agent and TTA workflows for automating manual reviews.

6. D (Data Aggregator)

Role: An agent within the QA Agent system that aggregates and organizes extracted data from multiple sources.
Actions:

Receives data from the Extraction Agent.
Prepares and sends the aggregated data to the Answering & Reasoning Agent.


Context: Ensures data consistency and completeness, a reusable component that can support TTA’s SOR validation process.

7. R (Answering & Reasoning Agent)

Role: An intelligent agent that processes the aggregated data, performs reasoning, and generates results for validation.
Actions:

Analyzes the data and applies logic or rules.
Sends the processed results to the Human in the Loop for validation.


Context: Represents the core AI-driven decision-making in QA Agent, adaptable for TTA’s validation needs with potential integration of TTA-specific rules.

8. H (Human in the Loop)

Role: A human supervisor or analyst who reviews and validates the results generated by the Answering & Reasoning Agent.
Actions:

Receives results from the Answering & Reasoning Agent.
Validates the output and sends it to the Target System.


Context: Critical for both QA Agent (supervisory review) and TTA (HITL validation), ensuring accuracy and resolving disputes.

9. TS (Target System)

Role: The destination system where validated data is stored or processed after HITL review.
Actions:

Receives validated data from the Human in the Loop.
Submits it for Supervisory Review if needed.
Returns the Completed QA Review to the User.


Context: Aligns with QA Agent’s Target System concept, serving as the final integration point for processed data.

10. SR (Supervisory Review)

Role: An additional layer of human oversight within the QA Agent process to resolve disputes or finalize the review.
Actions:

Reviews data submitted by the Target System.
Resolves any disputes and finalizes the output.


Context: Enhances the QA Agent’s robustness, ensuring compliance and quality, and can be integrated into TTA for complex cases.

Preparation and Upload as a Single Batch:

Bundle the 10 documents into a single batch (e.g., via a ZIP archive or batch ID assignment) to treat them as one cohesive unit for processing. This aligns with QA Agent's batch-oriented design for Global Operations.
Upload the batch to the QAA S3 Bucket through the TTA Application UI or directly via API. Assign a unique batch ID for tracking.
Rationale for Batch Handling: QA Agent is built for labor-intensive, error-prone processes that don't scale with volume growth (as noted in the executive summary). Batching ensures efficient orchestration without sequential delays.


Data Extraction (Extraction Agent Focus):

Trigger the Extraction Agent via the QAA API once the batch is posted and extraction readiness is confirmed (e.g., using the batch ID).
Handling Large Files: Each 200MB document (potentially thousands of pages) should be split into smaller chunks (e.g., by page or section) to manage size and avoid timeouts. Use Tachyon ADK/Google ADK (as in QA Agent's MVP1) or LLM-based tools to convert PDFs to text/images if needed. For scanned or complex PDFs, apply OCR (Optical Character Recognition) integration if available in Tachyon LLMs. Parallelize extraction across documents/chunks to reduce processing time—e.g., process 2-3 documents simultaneously on distributed infrastructure.
Field Extraction: Prompt the Extraction Agent (powered by LLMs) to scan chunks for the 200 fields. Use structured prompts like: "Extract borrower name, address, phone number, and [list remaining 197 fields] from this page/section as JSON." This yields raw data per chunk, focusing on key-value pairs for accuracy.
Expected Benefits: Reduces manual effort (similar to QA Agent's goal of >$10M annual savings) and handles variability in document layouts.


Data Aggregation (Data Aggregator Focus):

Once extraction is complete per document/chunk, route outputs to the Data Aggregator Agent.
Aggregation Logic: This agent compiles data from all 10 documents into a unified structure (e.g., a pandas DataFrame or JSON array in memory/database). It merges duplicates, resolves inconsistencies (e.g., if a borrower's name appears variably across docs), and normalizes formats (e.g., standardize phone numbers).
Scalability for Batch: Leverage MCP (Multi-Source Collection Protocol, as illustrated in QA Agent's target state) to pull from multiple sources (chunks/docs) in parallel. Store interim aggregated data in a temporary database (e.g., CORE Portal or Mongo DB, per QA Agent dependencies) to handle the volume without overload.
Custom Handling: Define aggregation rules based on your 200 fields—e.g., group by borrower ID if present—to create a single batch output file (CSV/JSON) for easy review.


Answering & Reasoning (Answering & Reasoning Agent Focus):

Feed the aggregated data into the Answering & Reasoning Agent for final processing.
Reasoning Over Fields: Use LLM prompts to reason and refine the 200 fields across the batch. For example: "From this aggregated data, validate and output the 200 fields (borrower name, address, etc.) per document, flagging any anomalies or missing values." This agent applies logic for context-specific tasks, such as cross-referencing fields between documents (e.g., matching addresses) or inferring missing data based on patterns.
Batch Optimization: Process in a single pass over the aggregated dataset to minimize LLM calls (e.g., 1-4 calls per QA Agent demo). Incorporate error handling for large inputs by chunking the aggregated data if it exceeds token limits.
Integration with QA Agent: Aligns with MVP1's in-house solutions (e.g., BRES for retail mortgage data), ensuring real-time validation and monitoring.
