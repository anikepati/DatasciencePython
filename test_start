import os
import logging
import time
import json
import uuid
import pandas as pd
import atexit
from typing import Any, Dict, Optional, List, Callable
from functools import wraps
from retry import retry
import certifi
import phoenix as px
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
# This is the explicit HTTP exporter, which will solve the gRPC errors.
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.trace import SpanKind, StatusCode, get_current_span
from openinference.instrumentation.openai import OpenAIInstrumentor
from rouge_score import rouge_scorer
import inspect

from phoenix.trace import SpanEvaluations
from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
)

# Structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
)
logger = logging.getLogger(__name__)


class RogueEvaluator:
    """Custom evaluator for ROUGE score that aligns with Phoenix's conventions."""
    def __init__(self, model: Optional[str] = None):
        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    def evaluate(self, df: pd.DataFrame) -> pd.DataFrame:
        results = []
        for _, row in df.iterrows():
            if 'output' in row and 'reference' in row:
                scores = self.scorer.score(str(row['reference']), str(row['output']))
                avg_score = (scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3
                result = {
                    'rouge1': scores['rouge1'].fmeasure,
                    'rouge2': scores['rouge2'].fmeasure,
                    'rougeL': scores['rougeL'].fmeasure,
                    'score': avg_score,
                    'label': 'pass' if avg_score > 0.5 else 'fail',
                }
                results.append(result)
        return df.assign(**pd.DataFrame(results))


class GenAIObserver:
    _instance = None
    _instrumented = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.__initialized = False
        return cls._instance

    def __init__(
        self,
        project_name: str = "gen-ai-observability",
        endpoint: Optional[str] = None,
        use_ax_mode: bool = False,
    ):
        if self.__initialized:
            return

        self.project_name = project_name
        self.use_ax_mode = use_ax_mode
        
        if use_ax_mode:
            default_arize_endpoint = "https://otlp.arize.com"
            self.endpoint = endpoint or os.getenv("PHOENIX_COLLECTOR_ENDPOINT", default_arize_endpoint)
        else:
            self.endpoint = endpoint or os.getenv("PHOENIX_COLLECTOR_ENDPOINT", "http://localhost:6006")

        self.tracer = trace.get_tracer(__name__)
        self.tracer_provider: Optional[TracerProvider] = None

        self._setup()
        self.__initialized = True
        
        if not hasattr(GenAIObserver, '_shutdown_registered'):
            atexit.register(self.shutdown)
            GenAIObserver._shutdown_registered = True

    def _setup(self):
        """
        Manually configures OpenTelemetry to export traces directly via HTTPS,
        bypassing the problematic gRPC-defaulting helper functions.
        """
        if GenAIObserver._instrumented:
            return

        try:
            # Set the project name for Phoenix context
            os.environ["PHOENIX_PROJECT_NAME"] = self.project_name
            
            # The full URL for OTLP/HTTP traces is typically at the /v1/traces path
            traces_endpoint = f"{self.endpoint.rstrip('/')}/v1/traces"
            logger.info(f"Trace exporter configured for HTTPS endpoint: {traces_endpoint}")

            # Prepare headers for authentication
            headers = {}
            if self.use_ax_mode:
                api_key, space_key = os.getenv("ARIZE_API_KEY"), os.getenv("ARIZE_SPACE_KEY")
                if not api_key or not space_key:
                    raise ValueError("AX Mode requires ARIZE_API_KEY and ARIZE_SPACE_KEY.")
                headers["x-arize-api-key"] = api_key
                headers["x-arize-space-key"] = space_key
                logger.info("Enterprise AX Mode configured with API and Space Keys.")

            # Create the explicit OTLP/HTTP Span Exporter
            otlp_exporter = OTLPSpanExporter(endpoint=traces_endpoint, headers=tuple(headers.items()))

            # Create a TracerProvider and add a BatchSpanProcessor with the HTTP exporter
            self.tracer_provider = TracerProvider()
            self.tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
            trace.set_tracer_provider(self.tracer_provider)
            
            OpenAIInstrumentor().instrument()
            GenAIObserver._instrumented = True
            logger.info("âœ… Manual HTTPS client configured and OpenAI instrumented successfully.")

        except Exception as e:
            logger.error(f"âŒ Failed to configure Phoenix tracer: {e}")

    def shutdown(self):
        if getattr(self, '_shutdown_called', False):
            return
        self._shutdown_called = True
        try:
            if self.tracer_provider and hasattr(self.tracer_provider, 'shutdown'):
                logger.info("Shutting down OpenTelemetry tracer provider (flushing spans)...")
                self.tracer_provider.shutdown()
                logger.info("âœ… Tracer provider shut down successfully.")
        except Exception as e:
            logger.error(f"âŒ Error during shutdown: {e}")

    def trace_workflow(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            session_id = str(uuid.uuid4())
            with self.tracer.start_as_current_span(
                func.__name__,
                attributes={
                    "openinference.span.kind": "workflow",
                    "session.id": session_id,
                }
            ) as span:
                logger.info(f"Starting workflow '{func.__name__}' with session_id: {session_id}")
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise
        return wrapper

    def _trace_common(self, span_name: str, span_kind: SpanKind, attributes: Dict[str, Any]):
        parent_span = get_current_span()
        session_id = parent_span.attributes.get("session.id") if parent_span.attributes else None
        if session_id:
            attributes["session.id"] = session_id
        return self.tracer.start_as_current_span(span_name, kind=span_kind, attributes=attributes)

    def trace_function(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            with self._trace_common(func.__name__, SpanKind.INTERNAL, {}) as span:
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    raise
        return wrapper

    def trace_tool_call(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            attributes = {
                "openinference.span.kind": "tool",
                "tool.name": func.__name__,
            }
            with self._trace_common(f"tool.{func.__name__}", SpanKind.CLIENT, attributes) as span:
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    raise
        return wrapper

class EvaluationManager:
    def __init__(self, observer: GenAIObserver):
        self.observer = observer

    def log_evaluations(self, eval_name: str, eval_df: pd.DataFrame):
        import phoenix as px
        from phoenix.trace import SpanEvaluations
        try:
            # For evaluations, we still use the Phoenix client, which uses standard HTTPS
            px.Client(endpoint=self.observer.endpoint).log_evaluations(
                SpanEvaluations(eval_name=eval_name, dataframe=eval_df)
            )
            logger.info(f"ðŸ“Š Successfully logged evaluations for '{eval_name}' to Phoenix.")
        except Exception as e:
            logger.error(f"Failed to log evaluations for '{eval_name}' to Phoenix: {e}")

    def create_sample_dataset(self) -> pd.DataFrame:
        data = {
            'input': [ "What is the capital of France?", "Explain machine learning."],
            'reference': ["Paris is the capital of France.", "Machine learning is a subfield of AI where systems learn from data."]
        }
        return pd.DataFrame(data)

    def run_offline_experiment(
        self,
        dataset: pd.DataFrame,
        evaluators: List[Callable],
        eval_model: str,
        eval_name: str = "Offline Experiment",
    ) -> Dict[str, pd.DataFrame]:
        from phoenix.evals import QAEvaluator
        
        tracer = trace.get_tracer(__name__)
        with tracer.start_as_current_span(f"offline_experiment.{eval_name}") as span:
            span.set_attribute("experiment.name", eval_name)
            
            results = {}
            for evaluator_cls in evaluators:
                evaluator_name = evaluator_cls.__name__
                with tracer.start_as_current_span(f"eval.{evaluator_name}") as eval_span:
                    try:
                        evaluator_instance = evaluator_cls(model=eval_model)
                        eval_df = evaluator_instance.evaluate(dataset)
                        results[evaluator_name] = eval_df
                        self.log_evaluations(f"{eval_name} - {evaluator_name}", eval_df)
                        eval_span.set_status(StatusCode.OK)
                    except Exception as e:
                        eval_span.set_status(StatusCode.ERROR, description=str(e))
                        logger.error(f"Failed to run offline eval {evaluator_name}: {e}")
            return results

