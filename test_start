import os
import logging
import uuid
import pandas as pd
import atexit
from typing import Any, Dict, Optional, List, Callable
from functools import wraps
import certifi
import phoenix as px

from opentelemetry import trace
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.trace import SpanKind, StatusCode, get_current_span
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter as OTLPGrpcSpanExporter
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as OTLPHttpSpanExporter
from openinference.instrumentation.openai import OpenAIInstrumentor
from rouge_score import rouge_scorer

from phoenix.trace import SpanEvaluations
from phoenix.evals import HallucinationEvaluator, QAEvaluator, RelevanceEvaluator


# Structured logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger(__name__)


class RogueEvaluator:
    """Custom evaluator for ROUGE score aligned with Phoenix."""

    def __init__(self, model: Optional[str] = None):
        self.scorer = rouge_scorer.RougeScorer(
            ["rouge1", "rouge2", "rougeL"], use_stemmer=True
        )

    def evaluate(self, df: pd.DataFrame) -> pd.DataFrame:
        results = []
        for _, row in df.iterrows():
            if "output" in row and "reference" in row:
                scores = self.scorer.score(str(row["reference"]), str(row["output"]))
                avg_score = (
                    scores["rouge1"].fmeasure
                    + scores["rouge2"].fmeasure
                    + scores["rougeL"].fmeasure
                ) / 3
                result = {
                    "rouge1": scores["rouge1"].fmeasure,
                    "rouge2": scores["rouge2"].fmeasure,
                    "rougeL": scores["rougeL"].fmeasure,
                    "score": avg_score,
                    "label": "pass" if avg_score > 0.5 else "fail",
                }
                results.append(result)
        return df.assign(**pd.DataFrame(results))


class GenAIObserver:
    _instance = None
    _instrumented = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.__initialized = False
        return cls._instance

    def __init__(
        self,
        project_name: str = "gen-ai-observability",
        endpoint: Optional[str] = None,
        use_ax_mode: bool = False,
    ):
        if getattr(self, "__initialized", False):
            if (
                self.project_name != project_name
                or self.endpoint != endpoint
                or self.use_ax_mode != use_ax_mode
            ):
                logger.warning(
                    "Configuration change detected. Re-initializing observer with new settings."
                )
                GenAIObserver._instrumented = False
            else:
                return

        self.project_name = project_name
        self.use_ax_mode = use_ax_mode

        # Endpoint handling
        if self.use_ax_mode:
            self.endpoint = endpoint or "https://otlp.arize.com"
        else:
            self.endpoint = endpoint or os.getenv(
                "PHOENIX_COLLECTOR_ENDPOINT", "http://localhost:6006"
            )

        self.tracer = trace.get_tracer(__name__)
        self.tracer_provider: Optional[TracerProvider] = None
        self.is_local = "localhost" in self.endpoint

        self._setup()
        self.__initialized = True

        if not hasattr(GenAIObserver, "_shutdown_registered"):
            atexit.register(self.shutdown)
            GenAIObserver._shutdown_registered = True

    def _setup(self):
        if GenAIObserver._instrumented:
            logger.info("Instrumentation already applied. Skipping setup.")
            return

        # Local Phoenix mode → skip OTLP
        if self.is_local and not self.use_ax_mode:
            logger.info("Configured for Local Phoenix client. Skipping OTLP setup.")
            GenAIObserver._instrumented = True
            return

        try:
            resource = Resource(attributes={"phoenix.project.name": self.project_name})
            traces_endpoint = f"{self.endpoint.rstrip('/')}/v1/traces"
            logger.info(f"Configuring OTLP exporter at: {traces_endpoint}")

            headers = {}
            if self.use_ax_mode:
                api_key, space_key = os.getenv("ARIZE_API_KEY"), os.getenv(
                    "ARIZE_SPACE_KEY"
                )
                if not api_key or not space_key:
                    raise ValueError(
                        "AX Mode requires ARIZE_API_KEY and ARIZE_SPACE_KEY."
                    )
                headers["x-arize-api-key"] = api_key
                headers["x-arize-space-key"] = space_key
                logger.info("Enterprise AX Mode with API/Space keys enabled.")

            exporter_args = {"endpoint": traces_endpoint, "headers": tuple(headers.items())}

            custom_cert_file = os.getenv("CUSTOM_SSL_CERT_FILE")
            allow_insecure = (
                os.getenv("ALLOW_INSECURE_CONNECTION", "false").lower() == "true"
            )

            if self.endpoint.startswith("https"):
                if custom_cert_file and os.path.exists(custom_cert_file):
                    exporter_args["certificate_file"] = custom_cert_file
                    logger.info(f"Using custom SSL cert: {custom_cert_file}")
                elif not allow_insecure:
                    exporter_args["certificate_file"] = certifi.where()
                    logger.info("Using default certifi CA bundle.")
                else:
                    logger.warning(
                        "ALLOW_INSECURE_CONNECTION=true → skipping certificate validation."
                    )
                    # Do not set certificate_file, OTLP HTTP exporter will skip validation

            # ---- Force HTTP if insecure or Azure hosted ----
            force_http = allow_insecure or ("azure" in self.endpoint.lower())
            if force_http:
                logger.info(f"Forcing HTTP exporter (endpoint={self.endpoint})")
                otlp_exporter = OTLPHttpSpanExporter(**exporter_args)
            else:
                try:
                    otlp_exporter = OTLPGrpcSpanExporter(**exporter_args)
                    logger.info("Using gRPC exporter.")
                except Exception as grpc_err:
                    logger.warning(
                        f"gRPC exporter failed ({grpc_err}), falling back to HTTP exporter."
                    )
                    otlp_exporter = OTLPHttpSpanExporter(**exporter_args)

            self.tracer_provider = TracerProvider(resource=resource)
            self.tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
            trace.set_tracer_provider(self.tracer_provider)

            OpenAIInstrumentor().instrument()
            GenAIObserver._instrumented = True
            logger.info("Tracer and OpenAI instrumentation configured.")

        except Exception as e:
            logger.error(f"Failed to configure tracer: {e}")

    def shutdown(self):
        if getattr(self, "_shutdown_called", False):
            return
        self._shutdown_called = True
        try:
            if self.tracer_provider and hasattr(self.tracer_provider, "shutdown"):
                logger.info("Shutting down tracer provider (flushing spans).")
                self.tracer_provider.shutdown()
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")

    # ---- Decorators ----
    def _trace_common(
        self, span_name: str, span_kind: SpanKind, attributes: Dict[str, Any]
    ):
        parent_span = get_current_span()
        session_id = None
        if parent_span and hasattr(parent_span, "attributes"):
            session_id = parent_span.attributes.get("session.id")
        if session_id:
            attributes["session.id"] = session_id
        return self.tracer.start_as_current_span(
            span_name, kind=span_kind, attributes=attributes
        )

    def trace_workflow(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            session_id = str(uuid.uuid4())
            with self.tracer.start_as_current_span(
                func.__name__,
                attributes={"openinference.span.kind": "workflow", "session.id": session_id},
            ) as span:
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise

        return wrapper

    def trace_function(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            with self._trace_common(func.__name__, SpanKind.INTERNAL, {}) as span:
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise

        return wrapper

    def trace_tool_call(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            attributes = {"openinference.span.kind": "tool", "tool.name": func.__name__}
            with self._trace_common(f"tool.{func.__name__}", SpanKind.CLIENT, attributes) as span:
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise

        return wrapper


class EvaluationManager:
    def __init__(self, observer: GenAIObserver):
        self.observer = observer

    def log_evaluations(self, eval_name: str, eval_df: pd.DataFrame):
        try:
            px.Client(endpoint=self.observer.endpoint).log_evaluations(
                SpanEvaluations(eval_name=eval_name, dataframe=eval_df)
            )
            logger.info(
                f"Logged evaluations to {'local' if self.observer.is_local else 'remote'} Phoenix: {eval_name}"
            )
        except Exception as e:
            logger.error(f"Failed to log evaluations: {e}")

    def create_sample_dataset(self) -> pd.DataFrame:
        return pd.DataFrame(
            {
                "input": ["What is the capital of France?", "Explain machine learning."],
                "reference": [
                    "Paris is the capital of France.",
                    "Machine learning is a subfield of AI where systems learn from data.",
                ],
            }
        )

    def run_offline_experiment(
        self,
        dataset: pd.DataFrame,
        evaluators: List[Callable],
        eval_model: str,
        eval_name: str = "Offline Experiment",
    ) -> Dict[str, pd.DataFrame]:
        tracer = trace.get_tracer(__name__)
        with tracer.start_as_current_span(f"offline_experiment.{eval_name}") as span:
            span.set_attribute("experiment.name", eval_name)
            results = {}
            for evaluator_cls in evaluators:
                evaluator_name = evaluator_cls.__name__
                with tracer.start_as_current_span(f"eval.{evaluator_name}") as eval_span:
                    try:
                        evaluator_instance = evaluator_cls(model=eval_model)
                        eval_df = evaluator_instance.evaluate(dataset)
                        results[evaluator_name] = eval_df
                        self.log_evaluations(f"{eval_name} - {evaluator_name}", eval_df)
                        eval_span.set_status(StatusCode.OK)
                    except Exception as e:
                        eval_span.set_status(StatusCode.ERROR, description=str(e))
                        logger.error(f"Failed eval {evaluator_name}: {e}")
            return results
