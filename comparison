```python
# project structure:
# .
# ├── app.py                # Main Streamlit app
# ├── utils.py              # Utility functions for extraction, aggregation, etc.
# ├── llm_insights.py       # LLM-related functions
# ├── report_generator.py   # PDF report generation
# └── requirements.txt      # Dependencies

# --- utils.py ---
import pdfplumber
from pptx import Presentation
import numpy as np
from PIL import Image
import easyocr
import io
import re
import json
import pandas as pd  # Moved here for global access

reader = easyocr.Reader(['en'])  # English language for OCR

def extract_from_pdf_page_by_page(file):
    """Extract text, tables, and OCR from images page by page in PDF."""
    pages_content = []
    with pdfplumber.open(file) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            text = page.extract_text() or ""
            tables = [pd.DataFrame(t) for t in page.extract_tables() if t]
            images_text = ""
            for img in page.images:
                try:
                    img_bytes = page.get_image(img['stream']).as_bytes()
                    img_pil = Image.open(io.BytesIO(img_bytes))
                    ocr_result = reader.readtext(np.array(img_pil))
                    images_text += " ".join([res[1] for res in ocr_result]) + "\n"
                except Exception:
                    pass
            page_content = f"Page {page_num}:\n{text}\n{images_text}"
            pages_content.append((page_content, tables))
    return pages_content

def extract_from_pptx_slide_by_slide(file):
    """Extract text and tables slide by slide in PPTX."""
    pages_content = []
    prs = Presentation(file)
    for slide_num, slide in enumerate(prs.slides, start=1):
        text = ""
        tables = []
        for shape in slide.shapes:
            if shape.has_text_frame:
                for paragraph in shape.text_frame.paragraphs:
                    for run in paragraph.runs:
                        text += run.text + " "
            if shape.has_table:
                table_data = [[cell.text for cell in row.cells] for row in shape.table.rows]
                if table_data:
                    tables.append(pd.DataFrame(table_data))
        page_content = f"Slide {slide_num}:\n{text}"
        pages_content.append((page_content, tables))
    return pages_content

def process_document_page_by_page(file, file_type):
    """Process PDF or PPTX page/slide by page."""
    if file_type == 'pdf':
        return extract_from_pdf_page_by_page(file)
    elif file_type == 'pptx':
        return extract_from_pptx_slide_by_slide(file)
    else:
        raise ValueError("Unsupported file type")

def aggregate_insights(pages_insights):
    """Aggregate page insights into document-level sections."""
    aggregated = {
        'objectives': [],
        'architecture': [],
        'integrations': [],
        'performance': [],
        'costs': [],
        'other': []
    }
    for insight in pages_insights:
        if isinstance(insight, dict) and "error" not in insight:
            for key in aggregated:
                if insight.get(key):
                    aggregated[key].append(insight[key])
    for key in aggregated:
        aggregated[key] = " ".join(aggregated[key]) if aggregated[key] else ""
    return aggregated

# --- llm_insights.py ---
import openai
import json

def get_llm_page_insight(client, page_text):
    """Use OpenAI LLM to get insights from a single page/slide."""
    prompt = f"""
    Analyze this page/slide content from an application design document:
    {page_text[:4000]}

    Extract and summarize key information in JSON:
    - summary: Brief overview of the page.
    - objectives: Goals or background mentioned.
    - architecture: Design or structure details.
    - integrations: Systems or integrations listed.
    - performance: Metrics or non-functional requirements.
    - costs: Funding, benefits, or costs.
    - other: Any other notable features, timelines, risks.
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    try:
        return json.loads(response.choices[0].message.content)
    except json.JSONDecodeError:
        return {"error": "JSON parsing failed", "raw": response.choices[0].message.content}

def get_llm_comparison(client, doc1_agg, doc2_agg):
    """Use OpenAI LLM for comprehensive comparison using aggregated insights."""
    prompt = f"""
    Compare these two application designs based on aggregated insights:

    Document 1 Aggregated:
    {json.dumps(doc1_agg)}

    Document 2 Aggregated:
    {json.dumps(doc2_agg)}

    Provide a structured JSON response:
    - relevance: Score (0-100) and explanation.
    - reusable_components: List with details.
    - architecture_design: Similarities/differences.
    - integration_possibilities: Opportunities/challenges.
    - performance: Comparison.
    - other_insights: Timelines, costs, recommendations.
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    try:
        return json.loads(response.choices[0].message.content)
    except json.JSONDecodeError:
        return {"error": "JSON parsing failed", "raw": response.choices[0].message.content}

def get_llm_single_analysis(client, doc_agg):
    """Use OpenAI LLM for single document analysis using aggregated insights."""
    prompt = f"""
    Analyze this application design document based on aggregated insights:

    Document Aggregated:
    {json.dumps(doc_agg)}

    Provide a structured JSON response:
    - summary: Overall summary of the document.
    - reusable_components: List of potential reusable elements (e.g., modules, agents).
    - architecture_design: Description of the architecture.
    - integration_possibilities: Potential integrations with other systems.
    - other_insights: Additional insights like timelines, costs, benefits, recommendations.
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    try:
        return json.loads(response.choices[0].message.content)
    except json.JSONDecodeError:
        return {"error": "JSON parsing failed", "raw": response.choices[0].message.content}

# --- report_generator.py ---
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib import colors
from io import BytesIO

def generate_pdf_report(insights, doc_name, is_comparison=False, doc2_name=None):
    """Generate a PDF report using ReportLab for single or comparison."""
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=letter)
    styles = getSampleStyleSheet()
    elements = []

    # Title
    if is_comparison:
        title = f"Document Comparison Report: {doc_name} vs {doc2_name}"
    else:
        title = f"Document Analysis Report: {doc_name}"
    elements.append(Paragraph(title, styles['Title']))
    elements.append(Spacer(1, 12))

    # Sections
    if is_comparison:
        sections = [
            ("Relevance", insights.get('relevance', {})),
            ("Reusable Components", insights.get('reusable_components', 'N/A')),
            ("Architecture Design", insights.get('architecture_design', 'N/A')),
            ("Integration Possibilities", insights.get('integration_possibilities', 'N/A')),
            ("Performance Comparison", insights.get('performance', 'N/A')),
            ("Other Insights", insights.get('other_insights', 'N/A'))
        ]
    else:
        sections = [
            ("Summary", insights.get('summary', 'N/A')),
            ("Reusable Components", insights.get('reusable_components', 'N/A')),
            ("Architecture Design", insights.get('architecture_design', 'N/A')),
            ("Integration Possibilities", insights.get('integration_possibilities', 'N/A')),
            ("Other Insights", insights.get('other_insights', 'N/A'))
        ]

    for title, content in sections:
        elements.append(Paragraph(title, styles['Heading2']))
        if isinstance(content, dict):
            if 'score' in content:
                elements.append(Paragraph(f"Score: {content['score']}", styles['Normal']))
            if 'explanation' in content:
                elements.append(Paragraph(content['explanation'], styles['Normal']))
        elif isinstance(content, list):
            for item in content:
                elements.append(Paragraph(f"- {item}", styles['Normal']))
        else:
            elements.append(Paragraph(str(content), styles['Normal']))
        elements.append(Spacer(1, 12))

    doc.build(elements)
    buffer.seek(0)
    return buffer

# --- app.py ---
import streamlit as st
import openai
from utils import process_document_page_by_page, aggregate_insights
from llm_insights import get_llm_page_insight, get_llm_comparison, get_llm_single_analysis
from report_generator import generate_pdf_report

st.title("Document Analysis and Comparison Tool")

# OpenAI API Key
api_key = st.text_input("Enter your OpenAI API Key", type="password")
if api_key:
    client = openai.OpenAI(api_key=api_key)
else:
    st.warning("Please enter your OpenAI API Key.")
    st.stop()

# Upload files
doc1 = st.file_uploader("Upload Document 1 (PDF or PPTX)", type=['pdf', 'pptx'])
doc2 = st.file_uploader("Upload Document 2 (PDF or PPTX, optional for comparison)", type=['pdf', 'pptx'])

if doc1:
    doc1_type = doc1.name.split('.')[-1]
    doc1_name = doc1.name
    
    with st.spinner("Analyzing Document 1 page by page..."):
        doc1_pages = process_document_page_by_page(doc1, doc1_type)
        doc1_insights = [get_llm_page_insight(client, page_content) for page_content, _ in doc1_pages]
        doc1_agg = aggregate_insights(doc1_insights)
    
    if doc2:
        doc2_type = doc2.name.split('.')[-1]
        doc2_name = doc2.name
        
        with st.spinner("Analyzing Document 2 page by page..."):
            doc2_pages = process_document_page_by_page(doc2, doc2_type)
            doc2_insights = [get_llm_page_insight(client, page_content) for page_content, _ in doc2_pages]
            doc2_agg = aggregate_insights(doc2_insights)
        
        with st.spinner("Generating Comparison..."):
            analysis = get_llm_comparison(client, doc1_agg, doc2_agg)
        
        is_comparison = True
        report_title = "Comparison Report"
    else:
        with st.spinner("Generating Single Document Analysis..."):
            analysis = get_llm_single_analysis(client, doc1_agg)
        
        is_comparison = False
        report_title = "Analysis Report"
        doc2_name = None
    
    # Display Results
    st.header(report_title)
    if "error" in analysis:
        st.error(analysis["error"])
        st.write(analysis["raw"])
    else:
        if is_comparison:
            st.markdown("### Relevance")
            rel = analysis.get('relevance', {})
            st.markdown(f"**Score:** {rel.get('score', 'N/A')}")
            st.markdown(rel.get('explanation', 'N/A'))
            
            st.markdown("### Reusable Components")
            for comp in analysis.get('reusable_components', []):
                st.markdown(f"- {comp}")
            
            st.markdown("### Architecture Design")
            st.markdown(analysis.get('architecture_design', 'N/A'))
            
            st.markdown("### Integration Possibilities")
            st.markdown(analysis.get('integration_possibilities', 'N/A'))
            
            st.markdown("### Performance Comparison")
            st.markdown(analysis.get('performance', 'N/A'))
            
            st.markdown("### Other Insights")
            st.markdown(analysis.get('other_insights', 'N/A'))
        else:
            st.markdown("### Summary")
            st.markdown(analysis.get('summary', 'N/A'))
            
            st.markdown("### Reusable Components")
            for comp in analysis.get('reusable_components', []):
                st.markdown(f"- {comp}")
            
            st.markdown("### Architecture Design")
            st.markdown(analysis.get('architecture_design', 'N/A'))
            
            st.markdown("### Integration Possibilities")
            st.markdown(analysis.get('integration_possibilities', 'N/A'))
            
            st.markdown("### Other Insights")
            st.markdown(analysis.get('other_insights', 'N/A'))
    
    # PDF Download
    pdf_buffer = generate_pdf_report(analysis, doc1_name, is_comparison, doc2_name)
    st.download_button(
        label="Download PDF Report",
        data=pdf_buffer,
        file_name="document_report.pdf",
        mime="application/pdf"
    )

# --- requirements.txt ---
streamlit
pdfplumber
python-pptx
openai
pandas
numpy
pillow
easyocr
reportlab
```

graph TD
    subgraph "TTA App Process"
        A["User Uploads Batch <br/> - 10 PDFs (200MB each) <br/> - Assign Batch ID"] --> B["TTA App UI: Prompt Configuration <br/> - Select Field Groups (e.g., 10 groups of 20 fields) <br/> - Add Few-Shot Examples <br/> - Serialize All 3 Agent Prompts as JSON/YAML <br/> - Toggle SOR Verification"]
        
        B -->|Upload Batch + All 3 Prompts JSON + Batch ID| C["QAA S3 Bucket: Storage <br/> - Store PDFs with Batch ID <br/> - Generate Presigned URLs <br/> - Metadata: Doc Names, Sizes"]
        
        J --> K["Final Output <br/> - Export: Single JSON/CSV (Batch ID, 200 Fields/Doc) <br/> - Metrics: Accuracy 98%, Time 15-30min <br/> - Notify User: Email/TTA UI <br/> - Sample: {&quot;txn_id&quot;: &quot;TX123&quot;, &quot;match&quot;: true, &quot;reason&quot;: &quot;SOR Verified&quot; }"]
    end
    
    subgraph "QAA Process"
        C --> D["QAA API: Orchestration <br/> - Endpoint: /submit-batch-with-prompts <br/> - Receive All 3 Prompts w/ Batch ID <br/> - Queue Job (e.g., UiPath) <br/> - Poll Readiness Every 10s"]
        
        D --> OR["Orchestration Agent <br/> - Central Coordinator <br/> - Route Prompts/Data by Batch ID <br/> - Chain Agents (e.g., Pass Outputs) <br/> - Handle Errors/Retries <br/> - Monitor: Logs, Audits"]
        
        subgraph "Extraction Agent Task"
            OR --> E1["Sub-Task 1: Chunking Large Files <br/> - Split PDFs by Pages/Sections (500-1000 tokens/chunk) <br/> - Parallelize Across 10 Docs (5 Workers) <br/> - Apply OCR if Scanned <br/> - Output: 100-200 Chunks Total"]
            E1 --> E2["Sub-Task 2: Field Extraction <br/> - Use Routed Prompt: Few-Shot for Grouped Fields (10-20/chunk) <br/> - Chain: {prev_chunk_results} <br/> - LLM Call: Extract as JSON (e.g., name, address) <br/> - Handle: 200 Fields Across Groups <br/> - Output: Per-Chunk JSONs"]
            E2 --> OR
        end
        
        subgraph "Aggregation Agent Task"
            OR --> F1["Sub-Task 1: Merge Outputs <br/> - Collect All Chunk JSONs <br/> - Use Pandas/Spark for Batch Merge <br/> - Dedupe (e.g., Same Borrower ID) <br/> - Normalize (e.g., Phone Format)"]
            F1 --> F2["Sub-Task 2: Aggregation Logic <br/> - Use Routed Prompt: Chain {extraction_outputs} <br/> - Rules: Flag Low-Confidence (&lt;0.8) <br/> - Cross-Reference Docs (e.g., Match Txns) <br/> - Output: Unified Dataset per Doc/Batch"]
            F2 --> F3["Sub-Task 3: Scalability Check <br/> - Distributed Processing (Cloud Nodes) <br/> - Monitor: Logs/Audits via QA Agent <br/> - Handle Volume: ~2GB Batch"]
            F3 --> OR
        end
        
        subgraph "Reasoning Agent Task"
            OR --> G1["Sub-Task 1: Fetch SOR Fields <br/> - API Call: /get-sor-fields?batch_id=XYZ&amp;txn_ids=[list] <br/> - Pull from CORE Portal/DB (e.g., Txn ID, Amount, Date) <br/> - Use Batch ID + Extracted Txn IDs for Verification <br/> - Fallback: Cached S3 Snapshots <br/> - Output: SOR JSON Dataset"]
            G1 --> G2["Sub-Task 2: Verify Transactions <br/> - Use Routed Prompt: Chain {aggregated_data} + SOR <br/> - Compare: Extracted vs. SOR (e.g., Amount Match?) <br/> - Reason: Flag Mismatches w/ Confidence/Reasons <br/> - Infer: Missing Fields from Patterns <br/> - Output: Verified JSON w/ Flags (e.g., 98% Match)"]
            G2 --> G3["Sub-Task 3: Holistic Reasoning <br/> - Check Business Rules (e.g., Date Consistency) <br/> - Dispute Flags: ~1.6% Rate <br/> - Prepare for HITL: Summary Metrics"]
            G3 --> OR
        end
        
        OR --> H["HITL Review in QAA App <br/> - Dashboard: View Flags/Disputes <br/> - Human Validates/Resolves (Quality of Reasoning) <br/> - Escalate High-Risk to Supervisory <br/> - Due to Restrictions: Kept in QAA"]
        
        H --> I["Target System: SOR Final Validation <br/> - Cross-Check Completed Review <br/> - Store in DB (e.g., Mongo) <br/> - Generate Report: Pass/Fail Metrics <br/> - Only for SOR Validation (No HITL Here)"]
        
        I --> J["Supervisory Review - TA Agent <br/> - Senior Check: Resolved Disputes <br/> - Approve Final Dataset <br/> - Update: Audit Trail"]
    end
    
    subgraph "Prompt Management Overlay"
        B -.->|"Pass All 3 Prompts w/ Batch ID (JSON)"| D
        D -.->|"QAA Routes to Agents (Cache Fetch)"| OR
    end
    
    G1 --> SOR["SOR API <br/> - Endpoint: /get-sor-fields <br/> - Filters: Batch ID + Txn IDs <br/> - Response: JSON Transaction Data <br/> - Auth: API Keys/OAuth"]
    
    style A fill:#d9f7be,stroke:#68d391
    style B fill:#d9f7be,stroke:#68d391
    style C fill:#d9f7be,stroke:#68d391
    style K fill:#d9f7be,stroke:#68d391
    style D fill:#bee3f8,stroke:#4299e1
    style OR fill:#bee3f8,stroke:#4299e1
    style E1 fill:#bee3f8,stroke:#4299e1
    style E2 fill:#bee3f8,stroke:#4299e1
    style F1 fill:#bee3f8,stroke:#4299e1
    style F2 fill:#bee3f8,stroke:#4299e1
    style F3 fill:#bee3f8,stroke:#4299e1
    style G1 fill:#bee3f8,stroke:#4299e1
    style G2 fill:#bee3f8,stroke:#4299e1
    style G3 fill:#bee3f8,stroke:#4299e1
    style H fill:#bee3f8,stroke:#4299e1
    style I fill:#bee3f8,stroke:#4299e1
    style J fill:#bee3f8,stroke:#4299e1
    style SOR fill:#fed7aa,stroke:#f97316

5. Prompt Management (Detailed)
5.1 Overview
Prompts are user-configurable templates passed from TTA to QAA with Batch ID, enabling customization without code changes. The Orchestrator routes them to agents via cache (e.g., Redis by Batch ID). This supports chaining (e.g., {prev_output}) and versioning.
5.2 Prompt Structure and Examples

Serialization: Single JSON object from TTA UI.
Routing: Orchestrator fetches by Batch ID; injects runtime data (e.g., chunks, SOR).
Best Practices: Use few-shot (2-3 examples) for accuracy; group fields to avoid token overload; chain for context; log usage for audits.

Example JSON Payload:
json{
  "batch_id": "20251002",
  "extraction_prompt": "Extract ONLY these fields as JSON from the chunk text. Use examples for format. Ignore unrelated text. Examples: Input: 'Borrower: John Doe, Address: 123 Main St, Phone: 555-1234' → {'name': 'John Doe', 'address': '123 Main St', 'phone': '555-1234'}. Chain: {prev_chunk_results}. Chunk: {chunk_text}. Fields: name, address, phone, SSN, DOB.",
  "aggregation_prompt": "Aggregate these chunk extractions into one JSON per doc. Dedupe (e.g., same name=keep first), normalize (e.g., phone format ###-###-####), flag low-confidence (<0.8). Examples: Chunks: [{'name': 'John Doe'}, {'name': 'john doe'}] → {'name': 'John Doe', 'confidence': 0.95}. Chain: {extraction_outputs}. Output: Unified fields for batch.",
  "reasoning_prompt": "Reason over aggregated fields: Verify vs. SOR (compare amounts, dates), infer missing, flag disputes. Output JSON with reasons. Examples: Aggregated: {'name': 'John Doe', 'amount': 5000} + SOR: {'amount': 5000} → {'amount': 5000, 'reason': 'Exact match', 'dispute': false}. Chain: {aggregated_data} + SOR."
}
