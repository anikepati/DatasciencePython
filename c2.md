Here is the complete implementation package. This includes the PostgreSQL schema, the FastAPI server with the atomic locking logic, and the specialized Python client.

### 1. Database Setup (PostgreSQL)

Run these SQL commands in your PostgreSQL database. I have added the `project_type` column to handle your specific requirements (Smart vs. Indigo).

```sql
-- 1. Table to track your workers (100+ clients)
CREATE TABLE workers (
    client_id VARCHAR(50) PRIMARY KEY,
    hostname VARCHAR(100),
    preferred_project_type VARCHAR(50), -- 'smart' or 'indigo'
    status VARCHAR(20) DEFAULT 'OFFLINE', -- 'IDLE', 'BUSY', 'OFFLINE'
    last_heartbeat TIMESTAMP DEFAULT NOW()
);

-- 2. Table to hold the work items
CREATE TABLE tasks (
    id SERIAL PRIMARY KEY,
    project_type VARCHAR(50) NOT NULL, -- 'smart', 'indigo', etc.
    status VARCHAR(20) DEFAULT 'PENDING', -- 'PENDING', 'PROCESSING', 'COMPLETED', 'FAILED'
    payload JSONB NOT NULL, -- The actual data (e.g., {"url": "..."})
    assigned_to VARCHAR(50) REFERENCES workers(client_id),
    created_at TIMESTAMP DEFAULT NOW(),
    started_at TIMESTAMP,
    completed_at TIMESTAMP
);

-- 3. Create an index to make finding PENDING tasks instant
CREATE INDEX idx_tasks_pending ON tasks(status, project_type) WHERE status = 'PENDING';

-- 4. Insert some dummy data for testing
INSERT INTO tasks (project_type, payload) VALUES 
('smart', '{"job": "Analyze Smart Data 1"}'),
('smart', '{"job": "Analyze Smart Data 2"}'),
('indigo', '{"job": "Process Indigo Batch A"}'),
('indigo', '{"job": "Process Indigo Batch B"}');

```

---

### 2. Server Side: FastAPI (`main.py`)

This server handles the persistent WebSocket connections and implements the critical **Atomic Task Locking**.

**Dependencies:**
`pip install fastapi uvicorn websockets sqlalchemy psycopg2-binary`

```python
import asyncio
import json
from datetime import datetime
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from sqlalchemy import create_engine, text

# --- Configuration ---
DATABASE_URL = "postgresql://user:password@localhost/dbname"
engine = create_engine(DATABASE_URL)

app = FastAPI()

class ConnectionManager:
    def __init__(self):
        # We keep track of active sockets to handle disconnects
        self.active_connections = {}

    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections[client_id] = websocket
        print(f"[SERVER] Client {client_id} connected.")

    def disconnect(self, client_id: str):
        if client_id in self.active_connections:
            del self.active_connections[client_id]
        print(f"[SERVER] Client {client_id} disconnected.")

manager = ConnectionManager()

# --- The "Atomic" Task Fetcher ---
def get_next_task(client_id: str, project_type: str):
    """
    Finds the next PENDING task for the specific project_type.
    Uses 'FOR UPDATE SKIP LOCKED' to ensure no two clients get the same task.
    """
    with engine.begin() as conn: # Transaction starts here
        # 1. Select and Lock
        sql_select = text("""
            SELECT id, payload 
            FROM tasks 
            WHERE status = 'PENDING' AND project_type = :ptype
            ORDER BY id ASC 
            LIMIT 1 
            FOR UPDATE SKIP LOCKED
        """)
        result = conn.execute(sql_select, {"ptype": project_type}).fetchone()

        if result:
            task_id, payload = result
            
            # 2. Update to PROCESSING
            sql_update = text("""
                UPDATE tasks 
                SET status = 'PROCESSING', 
                    assigned_to = :cid, 
                    started_at = NOW() 
                WHERE id = :tid
            """)
            conn.execute(sql_update, {"cid": client_id, "tid": task_id})
            
            print(f"[DB] Assigned Task {task_id} ({project_type}) to {client_id}")
            return {"id": task_id, "payload": payload}
        
        return None

def mark_task_complete(task_id: int):
    with engine.begin() as conn:
        conn.execute(text("UPDATE tasks SET status = 'COMPLETED', completed_at = NOW() WHERE id = :tid"), {"tid": task_id})
        print(f"[DB] Task {task_id} marked COMPLETED")

# --- WebSocket Endpoint ---
@app.websocket("/ws/{client_id}/{project_type}")
async def websocket_endpoint(websocket: WebSocket, client_id: str, project_type: str):
    await manager.connect(websocket, client_id)
    
    try:
        # 1. Update Worker DB status to IDLE/ONLINE
        with engine.begin() as conn:
            conn.execute(text("""
                INSERT INTO workers (client_id, preferred_project_type, status, last_heartbeat)
                VALUES (:cid, :ptype, 'IDLE', NOW())
                ON CONFLICT (client_id) DO UPDATE 
                SET status = 'IDLE', last_heartbeat = NOW(), preferred_project_type = :ptype
            """), {"cid": client_id, "ptype": project_type})

        while True:
            # 2. Wait for message from Client
            data_text = await websocket.receive_text()
            data = json.loads(data_text)
            
            msg_type = data.get("type")

            if msg_type == "HEARTBEAT":
                # Update 'last_heartbeat' in DB (could be optimized to not run every single time)
                pass 

            elif msg_type == "REQUEST_WORK":
                # Client is asking for a job matching their project_type
                task = get_next_task(client_id, project_type)
                
                if task:
                    await websocket.send_json({
                        "type": "DISPATCH", 
                        "task_id": task["id"], 
                        "payload": task["payload"]
                    })
                else:
                    await websocket.send_json({"type": "WAIT"})

            elif msg_type == "JOB_DONE":
                task_id = data.get("task_id")
                mark_task_complete(task_id)
                # Immediately try to give them another task?
                # or wait for them to ask REQUEST_WORK again.
                await websocket.send_json({"type": "ACK_DONE"})

    except WebSocketDisconnect:
        manager.disconnect(client_id)
        # Ideally, reset any tasks stuck in 'PROCESSING' for this client here

```

---

### 3. Client Side: Python Worker (`client.py`)

This client connects, identifies what "type" of work it wants (e.g., 'smart'), sends heartbeats, and processes tasks.

**Dependencies:**
`pip install websockets asyncio`

```python
import asyncio
import websockets
import json
import random
import sys

# --- Settings ---
CLIENT_ID = f"PC-{random.randint(1000,9999)}"
# CHANGE THIS based on the client needs: 'smart' or 'indigo'
MY_PROJECT_TYPE = "smart" 
SERVER_URL = f"ws://localhost:8000/ws/{CLIENT_ID}/{MY_PROJECT_TYPE}"

async def perform_job(task_payload):
    """Simulate the heavy work"""
    print(f">>> WORKING on: {task_payload}")
    await asyncio.sleep(3) # Simulate processing time
    print(">>> DONE")

async def start_worker():
    print(f"[{CLIENT_ID}] Connecting to Grid requesting '{MY_PROJECT_TYPE}' tasks...")
    
    async with websockets.connect(SERVER_URL) as websocket:
        print(f"[{CLIENT_ID}] Connected.")

        # Start the heartbeat loop in the background
        asyncio.create_task(send_heartbeat(websocket))

        while True:
            # 1. Ask for work immediately
            await websocket.send(json.dumps({"type": "REQUEST_WORK"}))
            
            # 2. Listen for response
            response = await websocket.recv()
            data = json.loads(response)
            
            if data["type"] == "DISPATCH":
                task_id = data["task_id"]
                print(f"[{CLIENT_ID}] Received Task #{task_id}")
                
                # Do the work
                await perform_job(data["payload"])
                
                # Report completion
                await websocket.send(json.dumps({
                    "type": "JOB_DONE", 
                    "task_id": task_id,
                    "result": "Success"
                }))
                
                # Wait for ACK before asking for next (optional)
                ack = await websocket.recv() 

            elif data["type"] == "WAIT":
                print(f"[{CLIENT_ID}] No '{MY_PROJECT_TYPE}' tasks available. Sleeping 5s...")
                await asyncio.sleep(5)

async def send_heartbeat(websocket):
    """Sends a ping every 10 seconds to keep connection alive"""
    while True:
        try:
            await websocket.send(json.dumps({"type": "HEARTBEAT"}))
            await asyncio.sleep(10)
        except Exception:
            break

if __name__ == "__main__":
    try:
        asyncio.run(start_worker())
    except KeyboardInterrupt:
        print("Worker stopped.")

```

### 4. Integration Test: How to verify "Atomic Locking"

To verify that two clients never get the same task:

1. **Start the Server:**
`uvicorn main:app --reload`
2. **Reset DB Tasks:**
Ensure you have 2 'smart' tasks in the DB (`status='PENDING'`).
3. **Start Client A:**
Run `python client.py` (ensure `MY_PROJECT_TYPE = "smart"`).
*Observation:* It will pick up Task #1.
4. **Start Client B:**
Immediately run `python client.py` in a separate terminal.
*Observation:*
* If locking works: Client B will skip Task #1 (because it's locked/processing) and pick up Task #2.
* If locking fails: Client B might try to grab Task #1 and cause an error or duplicate work.
* Since we used `FOR UPDATE SKIP LOCKED`, Client B will effectively "not see" Task #1 and instantly grab Task #2.



### Why `SKIP LOCKED` is critical here:

Without `SKIP LOCKED`, Client B would try to select Task #1, see it is locked by Client A, and **wait (freeze)** until Client A finishes. That destroys performance.
With `SKIP LOCKED`, Client B sees Task #1 is busy, skips it, and grabs Task #2 immediately.
