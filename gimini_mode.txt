#tools.py
import time
import pandas as pd
from opentelemetry import trace

# Import the observer to get the singleton instance
from gen_ai_observer import GenAIObserver

# Get the existing instance of the observer. Because it's a singleton,
# this will be the exact same object that was created in main_app.py.
observer = GenAIObserver()

class ToolMock:
    @observer.trace_tool_call
    def search_internet(self, query: str) -> str:
        """Mocks an internet search and updates its span with the query."""
        print(f"    ‚û°Ô∏è  Tool call: Searching for '{query}'...")
        span = trace.get_current_span()
        if span.is_recording():
            display_query = (query[:40] + '...') if len(query) > 40 else query
            span.update_name(f"tool.search_internet: '{display_query}'")
            span.set_attribute("tool.query", query)
        time.sleep(0.5)
        return "It's sunny and 75 degrees Fahrenheit in Antioch."

    @observer.trace_tool_call
    def create_support_ticket(self, issue: str, customer_id: str) -> str:
        """Mocks creating a support ticket and updates its span with key details."""
        print(f"    ‚û°Ô∏è  Tool call: Creating ticket for customer '{customer_id}' with issue: '{issue}'...")
        span = trace.get_current_span()
        if span.is_recording():
            span.update_name(f"tool.create_support_ticket: {customer_id}")
            span.set_attribute("tool.issue_description", issue)
            span.set_attribute("tool.customer_id", customer_id)
        
        time.sleep(0.5)
        ticket_id = f"TICKET-{pd.Timestamp.now().microsecond}"
        print(f"    ‚úÖ  Successfully created ticket: {ticket_id}")
        return f"Support ticket {ticket_id} has been created successfully."

#main_app.py
import os
import openai
import time
import pandas as pd
import json
from typing import Dict, Optional
from dotenv import load_dotenv
from opentelemetry import trace

# Import the observer and evaluators from your module
from gen_ai_observer import (
    GenAIObserver,
    EvaluationManager,
    RogueEvaluator,
    QAEvaluator, # We'll use this for the offline demo
)
# Import the ToolMock class from our new tools.py file
from tools import ToolMock

# --- SETUP ---
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY environment variable not set!")

# --- CRITICAL INITIALIZATION STEP ---
observer = GenAIObserver(
    project_name="gen-ai-interactive-demo",
    use_ax_mode=False 
)
eval_manager = EvaluationManager(observer)

# --- AGENT AND TOOLS (Business Logic) ---
class GenAIAgent:
    def __init__(self):
        self.client = openai.OpenAI(api_key=api_key)
        self.tool_mock = ToolMock()
        # Define the list of available tools for the OpenAI API
        self.tools = [
            { "type": "function", "function": { "name": "search_internet", "description": "Get the current weather in a given location.", "parameters": { "type": "object", "properties": { "query": { "type": "string" } }, "required": ["query"], }, }, },
            { "type": "function", "function": { "name": "create_support_ticket", "description": "Create a new support ticket for a customer.", "parameters": { "type": "object", "properties": { "issue": { "type": "string", "description": "A detailed description of the customer's issue." }, "customer_id": { "type": "string", "description": "The customer's unique identifier." }, }, "required": ["issue", "customer_id"], }, }, }
        ]

    @observer.trace_function
    def process_request(self, prompt: str) -> str:
        """
        Processes a request by calling an LLM, which may decide to use one or more tools in parallel.
        """
        print(f"  ü§ñ Agent processing request: '{prompt}'")

        messages = [{"role": "user", "content": prompt}]
        # Step 1: Call the model with tools
        response = self.client.chat.completions.create(
            model="gpt-4o-mini", messages=messages, tools=self.tools, tool_choice="auto",
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # Step 2: Check if the model wants to call tools
        if tool_calls:
            print(f"    ü§ñ LLM decided to use {len(tool_calls)} tool(s).")
            messages.append(response_message)  # Add assistant's reply to history

            # Step 3: Execute all tool calls
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                if function_name == "search_internet":
                    function_response = self.tool_mock.search_internet(query=function_args.get("query"))
                elif function_name == "create_support_ticket":
                    function_response = self.tool_mock.create_support_ticket(
                        issue=function_args.get("issue"),
                        customer_id=function_args.get("customer_id")
                    )
                else:
                    function_response = f"Error: Unknown tool '{function_name}'"
                
                # Append the result of each tool call to the history
                messages.append({
                    "tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": function_response,
                })
            
            # Step 4: Call the model again with tool results to get a final summary
            print("    ü§ñ Sending tool results back to LLM for final response...")
            second_response = self.client.chat.completions.create(
                model="gpt-4o-mini", messages=messages,
            )
            return second_response.choices[0].message.content
        else:
            print("    ü§ñ LLM responded directly.")
            return response_message.content
    
    def generate_llm_response(self, prompt: str) -> str:
        """Generates a direct response from the LLM without considering tools."""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}],
        )
        return response.choices[0].message.content

# --- WORKFLOW DEFINITIONS ---
@observer.trace_workflow
def run_workflow(prompt: str, user_feedback: Optional[str] = None) -> Dict:
    agent = GenAIAgent()
    current_span = trace.get_current_span()
    print(f"\nüöÄ Starting workflow for prompt: '{prompt}'")
    response = agent.process_request(prompt=prompt)
    print(f"  ‚úÖ Workflow completed. Final Response: '{response}'")

    if current_span.is_recording() and user_feedback:
        print("    üìù Adding user feedback as span attributes (annotations)...")
        current_span.set_attribute("feedback.text", user_feedback)
        current_span.set_attribute("feedback.rating", "positive")
        
    return {}

# --- DEMO FUNCTIONS FOR MENU ---
def demo_simple_llm_trace():
    print("\n--- 1. Simple LLM Trace Demo ---")
    run_workflow(prompt="What is the capital of France?")
    print("\nTrace sent. Check the Phoenix UI.")

def demo_llm_with_weather_tool():
    print("\n--- 2a. LLM + Weather Tool Demo ---")
    run_workflow(prompt="What is the weather like today?")
    print("\nTrace sent. The trace should show a parent LLM span and a child 'tool.search_internet' span.")

def demo_llm_with_multi_tool_call():
    print("\n--- 2b. LLM + Multi-Tool Demo ---")
    run_workflow(prompt="My account CUST-12345 is locked after too many failed login attempts, can you create a ticket for me? Also, what's the weather like in Antioch, CA?")
    print("\nTrace sent. The trace should show a parent LLM span and two parallel tool call spans.")

def demo_llm_with_annotations():
    """Shows how to add user feedback as span attributes."""
    print("\n--- 3. LLM with Annotations Demo ---")
    run_workflow(prompt="What are the main benefits of Python?", user_feedback="Helpful and concise!")
    print("\nTrace and feedback sent. Inspect the attributes of the root span in the Phoenix UI.")

def demo_offline_evals():
    """Runs an offline evaluation by first generating responses and then scoring them."""
    print("\n--- 4. Offline Evaluations Demo ---")
    sample_dataset = eval_manager.create_sample_dataset()
    print("Created sample dataset for evaluation...")
    
    print("Generating model responses for the dataset...")
    agent = GenAIAgent()
    sample_dataset["output"] = sample_dataset["input"].apply(agent.generate_llm_response)
    print("Model responses generated.")

    eval_manager.run_offline_experiment(
        dataset=sample_dataset, evaluators=[QAEvaluator], eval_model="gpt-4o-mini", eval_name="Demo-Offline-Quality-Test"
    )
    print("\nOffline experiment finished and results logged to Phoenix.")
    print("Check the 'Evaluations' tab in the Phoenix UI.")

def display_menu():
    print("\n" + "="*50)
    print(" GenAI Observer Interactive Demo ".center(50, "="))
    print("="*50)
    print("1.  Run Simple LLM Trace")
    print("2a. Run LLM Trace with Weather Tool")
    print("2b. Run LLM Trace with Multi-Tool Call")
    print("3.  Run LLM Trace with Annotations")
    print("4.  Run Offline Evaluations")
    print("Q.  Quit and Save Traces")
    print("-"*50)
    return input("Enter your choice: ").strip().lower()

def main():
    while True:
        choice = display_menu()
        if choice == '1':
            demo_simple_llm_trace()
        elif choice == '2a':
            demo_llm_with_weather_tool()
        elif choice == '2b':
            demo_llm_with_multi_tool_call()
        elif choice == '3':
            demo_llm_with_annotations()
        elif choice == '4':
            demo_offline_evals()
        elif choice == 'q':
            # The shutdown is handled explicitly for user confirmation
            print("Shutting down and flushing traces...")
            observer.shutdown()
            print("Exiting.")
            break
        else:
            print("Invalid choice. Please try again.")
        time.sleep(2)

if __name__ == "__main__":
    main()


#gen_ai_observer.py
import os
import logging
import time
import json
import uuid
import pandas as pd
import atexit
from typing import Any, Dict, Optional, List, Callable
from functools import wraps
from retry import retry
import phoenix as px
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.trace import SpanKind, StatusCode, get_current_span
from openinference.instrumentation.openai import OpenAIInstrumentor
from rouge_score import rouge_scorer
import inspect

# Import the high-level register function from your original working code.
from phoenix.otel import register

from phoenix.trace import SpanEvaluations
from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
)

# Structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
)
logger = logging.getLogger(__name__)


def _safe_serialize(obj: Any) -> str:
    """Safely serialize an object to a JSON string, handling unserializable types."""
    try:
        return json.dumps(obj, default=lambda o: f"<unserializable type: {type(o).__name__}>")
    except Exception:
        return f"<serialization error: {str(obj)}>"


class RogueEvaluator:
    """Custom evaluator for ROUGE score that aligns with Phoenix's conventions."""
    def __init__(self, model: Optional[str] = None):
        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    def evaluate(self, df: pd.DataFrame) -> pd.DataFrame:
        results = []
        for _, row in df.iterrows():
            # Use 'reference' to align with Phoenix conventions
            if 'response' in row and 'reference' in row:
                scores = self.scorer.score(str(row['reference']), str(row['response']))
                avg_score = (scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3
                result = {
                    'rouge1': scores['rouge1'].fmeasure,
                    'rouge2': scores['rouge2'].fmeasure,
                    'rougeL': scores['rougeL'].fmeasure,
                    'score': avg_score,
                    'label': 'pass' if avg_score > 0.5 else 'fail',
                }
                results.append(result)
        return df.assign(**pd.DataFrame(results))


class GenAIObserver:
    _instance = None
    _instrumented = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.__initialized = False
        return cls._instance

    def __init__(
        self,
        project_name: str = "gen-ai-observability",
        endpoint: Optional[str] = None,
        use_ax_mode: bool = False,
    ):
        if self.__initialized:
            return

        self.project_name = project_name
        self.endpoint = endpoint or os.getenv("PHOENIX_COLLECTOR_ENDPOINT", "http://localhost:6006")
        self.use_ax_mode = use_ax_mode
        self.tracer = trace.get_tracer(__name__)
        self.tracer_provider: Optional[TracerProvider] = None

        self._setup()
        self.__initialized = True
        
        atexit.register(self.shutdown)

    def _setup(self):
        """
        Initializes Phoenix using the stable `register` function.
        """
        if GenAIObserver._instrumented:
            return

        try:
            os.environ["PHOENIX_PROJECT_NAME"] = self.project_name
            os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = self.endpoint

            if self.use_ax_mode:
                # Configuration for enterprise mode
                api_key = os.getenv("ARIZE_API_KEY")
                space_key = os.getenv("ARIZE_SPACE_KEY")
                if not api_key or not space_key:
                    raise ValueError("AX Mode requires ARIZE_API_KEY and ARIZE_SPACE_KEY.")
                os.environ["ARIZE_API_KEY"] = api_key
                os.environ["ARIZE_SPACE_KEY"] = space_key
                logger.info("Enterprise AX Mode configured.")
            else:
                logger.info(f"Local Phoenix Mode configured for project '{self.project_name}'.")

            # Use the high-level register function to initialize tracing
            self.tracer_provider = register(project_name=self.project_name)
            trace.set_tracer_provider(self.tracer_provider)
            
            OpenAIInstrumentor().instrument()
            GenAIObserver._instrumented = True
            logger.info("‚úÖ Phoenix client configured and OpenAI instrumented successfully.")

        except Exception as e:
            logger.error(f"‚ùå Failed to configure Phoenix tracer: {e}")

    def shutdown(self):
        if getattr(self, '_shutdown_called', False):
            return
        self._shutdown_called = True
        try:
            if self.tracer_provider and hasattr(self.tracer_provider, 'shutdown'):
                logger.info("Shutting down OpenTelemetry tracer provider (flushing spans)...")
                self.tracer_provider.shutdown()
                logger.info("‚úÖ Tracer provider shut down successfully.")
        except Exception as e:
            logger.error(f"‚ùå Error during shutdown: {e}")

    def trace_workflow(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            session_id = str(uuid.uuid4())
            with self.tracer.start_as_current_span(
                func.__name__,
                attributes={
                    "openinference.span.kind": "workflow",
                    "session.id": session_id,
                }
            ) as span:
                logger.info(f"Starting workflow '{func.__name__}' with session_id: {session_id}")
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise
        return wrapper

    def _trace_common(self, span_name: str, span_kind: SpanKind, attributes: Dict[str, Any]):
        """Helper to create a child span that inherits the session.id."""
        parent_span = get_current_span()
        session_id = parent_span.attributes.get("session.id")
        if session_id:
            attributes["session.id"] = session_id
        return self.tracer.start_as_current_span(span_name, kind=span_kind, attributes=attributes)

    def trace_function(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            with self._trace_common(func.__name__, SpanKind.INTERNAL, {}) as span:
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise
        return wrapper

    def trace_tool_call(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            attributes = {
                "openinference.span.kind": "tool",
                "tool.name": func.__name__,
            }
            with self._trace_common(f"tool.{func.__name__}", SpanKind.CLIENT, attributes) as span:
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise
        return wrapper


class EvaluationManager:
    def __init__(self, observer: GenAIObserver):
        self.observer = observer

    def log_evaluations(self, eval_name: str, eval_df: pd.DataFrame):
        try:
            px.Client(endpoint=self.observer.endpoint).log_evaluations(
                SpanEvaluations(eval_name=eval_name, dataframe=eval_df)
            )
            logger.info(f"üìä Successfully logged evaluations for '{eval_name}' to Phoenix.")
        except Exception as e:
            logger.error(f"Failed to log evaluations for '{eval_name}' to Phoenix: {e}")

    def create_sample_dataset(self) -> pd.DataFrame:
        """Creates a sample DataFrame using Phoenix-conventional column names."""
        data = {
            # RENAMED: 'prompt' -> 'input'
            'input': [ "What is the capital of France?", "Explain machine learning."],
            # 'response' is not needed here as the evaluator will generate it
            # RENAMED: 'ground_truth' -> 'reference'
            'reference': ["Paris is the capital of France.", "Machine learning is a subfield of AI where systems learn from data."]
        }
        return pd.DataFrame(data)

    def run_offline_experiment(
        self,
        dataset: pd.DataFrame,
        evaluators: List[Callable],
        eval_model: str,
        eval_name: str = "Offline Experiment",
    ) -> Dict[str, pd.DataFrame]:
        tracer = trace.get_tracer(__name__)
        with tracer.start_as_current_span(f"offline_experiment.{eval_name}") as span:
            span.set_attribute("experiment.name", eval_name)
            
            results = {}
            for evaluator_cls in evaluators:
                evaluator_name = evaluator_cls.__name__
                with tracer.start_as_current_span(f"eval.{evaluator_name}") as eval_span:
                    try:
                        # QAEvaluator and others from phoenix.evals are designed to
                        # run the model, generate spans, and return a compatible DataFrame.
                        evaluator_instance = evaluator_cls(model=eval_model)
                        eval_df = evaluator_instance.evaluate(dataset)
                        results[evaluator_name] = eval_df
                        # This log call will now work because eval_df contains the necessary
                        # 'context.span_id' and 'context.trace_id' columns.
                        self.log_evaluations(f"{eval_name} - {evaluator_name}", eval_df)
                        eval_span.set_status(StatusCode.OK)
                    except Exception as e:
                        eval_span.set_status(StatusCode.ERROR, description=str(e))
                        eval_span.record_exception(e)
                        logger.error(f"Failed to run offline eval {evaluator_name}: {e}")
            return results

#requirements.txt
# requirements.txt
python-dotenv
openai
arize-phoenix
arize-phoenix-client
arize-phoenix-otel
arize-phoenix-evals
openinference-instrumentation-openai
opentelemetry-sdk
requests
pandas
retry
certifi
rouge-score
pydantic

# requirements.txt
# python-dotenv
# openai
# arize-phoenix[evals,otel,client]
# openinference-instrumentation-openai
# requests
# pandas
# retry
# certifi
# rouge-score
