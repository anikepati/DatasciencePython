import os
import openai
import time
import pandas as pd
import json
from typing import Dict, Optional
from dotenv import load_dotenv
from opentelemetry import trace

# Import the observer and evaluators from your module
from gen_ai_observer import (
    GenAIObserver,
    EvaluationManager,
    RogueEvaluator,
    QAEvaluator, # We'll use this for the offline demo
)

# --- SETUP ---
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY environment variable not set!")

# --- CRITICAL INITIALIZATION STEP ---
observer = GenAIObserver(
    project_name="gen-ai-interactive-demo",
    use_ax_mode=False 
)
eval_manager = EvaluationManager(observer)

# --- AGENT AND TOOLS (Business Logic) ---
class ToolMock:
    @observer.trace_tool_call
    def search_internet(self, query: str) -> str:
        print(f"    ➡️  Tool call: Searching for '{query}'...")
        time.sleep(0.5)
        return "It's sunny and 75 degrees Fahrenheit in Antioch."

class GenAIAgent:
    def __init__(self):
        self.client = openai.OpenAI(api_key=api_key)
        self.tool_mock = ToolMock()
        # Define the tool for the OpenAI API
        self.tools = [
            {
                "type": "function",
                "function": {
                    "name": "search_internet",
                    "description": "Get the current weather in a given location",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "The city and state, e.g. San Francisco, CA, or a question about the weather.",
                            },
                        },
                        "required": ["query"],
                    },
                },
            }
        ]

    @observer.trace_function
    def process_request(self, prompt: str) -> str:
        """
        Processes a request by first calling the LLM to decide whether to
        respond directly or use a tool. This ensures an LLM span is always created.
        """
        print(f"  🤖 Agent processing request: '{prompt}'")

        # Step 1: Call the LLM with the prompt and available tools.
        messages = [{"role": "user", "content": prompt}]
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            tools=self.tools,
            tool_choice="auto",
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # Step 2: Check if the model wants to call a tool.
        if tool_calls:
            print("    🤖 LLM decided to use a tool.")
            tool_call = tool_calls[0]
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)

            if function_name == "search_internet":
                # This call is traced by the @observer.trace_tool_call decorator
                return self.tool_mock.search_internet(
                    query=function_args.get("query")
                )
            else:
                return f"Error: Unknown tool '{function_name}'"
        else:
            print("    🤖 LLM responded directly.")
            return response_message.content
    
    def generate_llm_response(self, prompt: str) -> str:
        """Generates a direct response from the LLM without considering tools."""
        # This method is used for the offline eval to generate outputs directly.
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
        )
        return response.choices[0].message.content

# --- WORKFLOW DEFINITIONS ---
@observer.trace_workflow
def run_workflow(prompt: str, user_feedback: Optional[str] = None) -> Dict:
    agent = GenAIAgent()
    current_span = trace.get_current_span()
    print(f"\n🚀 Starting workflow for prompt: '{prompt}'")
    response = agent.process_request(prompt=prompt)
    print(f"  ✅ Workflow completed. Final Response: '{response}'")

    if current_span.is_recording() and user_feedback:
        # Log feedback as an "online evaluation" against the current span.
        # This is the Phoenix-native way to add annotations that appear correctly.
        span_id = str(current_span.context.span_id)
        trace_id = str(current_span.context.trace_id)
        feedback_df = pd.DataFrame([{
            "context.span_id": span_id,
            "context.trace_id": trace_id,
            "feedback": user_feedback,
            "rating": "positive" # Add a dummy rating for demonstration
        }])
        eval_manager.log_evaluations(eval_name="User Feedback", eval_df=feedback_df)
        print("    📝 User feedback logged as an annotation.")
    return {}

# --- DEMO FUNCTIONS FOR MENU ---
def demo_simple_llm_trace():
    print("\n--- 1. Simple LLM Trace Demo ---")
    run_workflow(prompt="What is the capital of France?")
    print("\nTrace sent. Check the Phoenix UI.")

def demo_llm_with_tools():
    print("\n--- 2. LLM + Tools Demo ---")
    run_workflow(prompt="What is the weather like today?")
    print("\nTrace sent. The trace should now show a parent LLM span and a child 'tool.search_internet' span.")

def demo_llm_with_annotations():
    """Shows how to add user feedback as a logged evaluation."""
    print("\n--- 3. LLM with Annotations Demo ---")
    print("This workflow adds user feedback as an evaluation, which appears as an annotation.")
    run_workflow(prompt="What are the main benefits of Python?", user_feedback="Helpful and concise!")
    print("\nTrace and feedback sent. Inspect the trace in the Phoenix UI for an evaluation.")

def demo_offline_evals():
    """Runs an offline evaluation by first generating responses and then scoring them."""
    print("\n--- 4. Offline Evaluations Demo ---")
    # Step 1: Create the dataset with inputs and ground truth references.
    sample_dataset = eval_manager.create_sample_dataset()
    print("Created sample dataset for evaluation...")
    
    # Step 2: Generate model responses (outputs) for each input.
    # This simulates a real-world scenario where you have a batch of model outputs to evaluate.
    print("Generating model responses for the dataset...")
    agent = GenAIAgent()
    # The 'output' column is what the QAEvaluator expects for the model's response.
    # We use a simple generator here; process_request could also be used.
    sample_dataset["output"] = sample_dataset["input"].apply(agent.generate_llm_response)
    print("Model responses generated.")

    # Step 3: Run the experiment on the completed dataset.
    # The evaluator will now find the 'input', 'output', and 'reference' columns it needs.
    evaluators_to_run = [QAEvaluator]
    
    eval_manager.run_offline_experiment(
        dataset=sample_dataset,
        evaluators=evaluators_to_run,
        eval_model="gpt-4o-mini", # This model is used by the evaluator for its internal scoring logic.
        eval_name="Demo-Offline-Quality-Test"
    )
    print("\nOffline experiment finished and results logged to Phoenix.")
    print("Check the 'Evaluations' tab in the Phoenix UI.")

def display_menu():
    print("\n" + "="*50)
    print(" GenAI Observer Interactive Demo ".center(50, "="))
    print("="*50)
    print("1. Run Simple LLM Trace")
    print("2. Run LLM Trace with Tool Call")
    print("3. Run LLM Trace with Annotations")
    print("4. Run Offline Evaluations")
    print("Q. Quit and Save Traces")
    print("-"*50)
    return input("Enter your choice: ").strip().lower()

def main():
    while True:
        choice = display_menu()
        if choice == '1':
            demo_simple_llm_trace()
        elif choice == '2':
            demo_llm_with_tools()
        elif choice == '3':
            demo_llm_with_annotations()
        elif choice == '4':
            demo_offline_evals()
        elif choice == 'q':
            print("Exiting. The shutdown hook will flush traces automatically.")
            break
        else:
            print("Invalid choice. Please try again.")
        time.sleep(2)

if __name__ == "__main__":
    main()

-------------------
import os
import logging
import time
import json
import uuid
import pandas as pd
import atexit
from typing import Any, Dict, Optional, List, Callable
from functools import wraps
from retry import retry
import phoenix as px
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.trace import SpanKind, StatusCode, get_current_span
from openinference.instrumentation.openai import OpenAIInstrumentor
from rouge_score import rouge_scorer
import inspect

# Import the high-level register function from your original working code.
from phoenix.otel import register

from phoenix.trace import SpanEvaluations
from phoenix.evals import (
    HallucinationEvaluator,
    QAEvaluator,
    RelevanceEvaluator,
)

# Structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
)
logger = logging.getLogger(__name__)


def _safe_serialize(obj: Any) -> str:
    """Safely serialize an object to a JSON string, handling unserializable types."""
    try:
        return json.dumps(obj, default=lambda o: f"<unserializable type: {type(o).__name__}>")
    except Exception:
        return f"<serialization error: {str(obj)}>"


class RogueEvaluator:
    """Custom evaluator for ROUGE score that aligns with Phoenix's conventions."""
    def __init__(self, model: Optional[str] = None):
        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    def evaluate(self, df: pd.DataFrame) -> pd.DataFrame:
        results = []
        for _, row in df.iterrows():
            # Use 'reference' to align with Phoenix conventions
            if 'response' in row and 'reference' in row:
                scores = self.scorer.score(str(row['reference']), str(row['response']))
                avg_score = (scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3
                result = {
                    'rouge1': scores['rouge1'].fmeasure,
                    'rouge2': scores['rouge2'].fmeasure,
                    'rougeL': scores['rougeL'].fmeasure,
                    'score': avg_score,
                    'label': 'pass' if avg_score > 0.5 else 'fail',
                }
                results.append(result)
        return df.assign(**pd.DataFrame(results))


class GenAIObserver:
    _instance = None
    _instrumented = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.__initialized = False
        return cls._instance

    def __init__(
        self,
        project_name: str = "gen-ai-observability",
        endpoint: Optional[str] = None,
        use_ax_mode: bool = False,
    ):
        if self.__initialized:
            return

        self.project_name = project_name
        self.endpoint = endpoint or os.getenv("PHOENIX_COLLECTOR_ENDPOINT", "http://localhost:6006")
        self.use_ax_mode = use_ax_mode
        self.tracer = trace.get_tracer(__name__)
        self.tracer_provider: Optional[TracerProvider] = None

        self._setup()
        self.__initialized = True
        
        atexit.register(self.shutdown)

    def _setup(self):
        """
        Initializes Phoenix using the stable `register` function.
        """
        if GenAIObserver._instrumented:
            return

        try:
            os.environ["PHOENIX_PROJECT_NAME"] = self.project_name
            os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = self.endpoint

            if self.use_ax_mode:
                # Configuration for enterprise mode
                api_key = os.getenv("ARIZE_API_KEY")
                space_key = os.getenv("ARIZE_SPACE_KEY")
                if not api_key or not space_key:
                    raise ValueError("AX Mode requires ARIZE_API_KEY and ARIZE_SPACE_KEY.")
                os.environ["ARIZE_API_KEY"] = api_key
                os.environ["ARIZE_SPACE_KEY"] = space_key
                logger.info("Enterprise AX Mode configured.")
            else:
                logger.info(f"Local Phoenix Mode configured for project '{self.project_name}'.")

            # Use the high-level register function to initialize tracing
            self.tracer_provider = register(project_name=self.project_name)
            trace.set_tracer_provider(self.tracer_provider)
            
            OpenAIInstrumentor().instrument()
            GenAIObserver._instrumented = True
            logger.info("✅ Phoenix client configured and OpenAI instrumented successfully.")

        except Exception as e:
            logger.error(f"❌ Failed to configure Phoenix tracer: {e}")

    def shutdown(self):
        if getattr(self, '_shutdown_called', False):
            return
        self._shutdown_called = True
        try:
            if self.tracer_provider and hasattr(self.tracer_provider, 'shutdown'):
                logger.info("Shutting down OpenTelemetry tracer provider (flushing spans)...")
                self.tracer_provider.shutdown()
                logger.info("✅ Tracer provider shut down successfully.")
        except Exception as e:
            logger.error(f"❌ Error during shutdown: {e}")

    def trace_workflow(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            session_id = str(uuid.uuid4())
            with self.tracer.start_as_current_span(
                func.__name__,
                attributes={
                    "openinference.span.kind": "workflow",
                    "session.id": session_id,
                }
            ) as span:
                logger.info(f"Starting workflow '{func.__name__}' with session_id: {session_id}")
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise
        return wrapper

    def _trace_common(self, span_name: str, span_kind: SpanKind, attributes: Dict[str, Any]):
        """Helper to create a child span that inherits the session.id."""
        parent_span = get_current_span()
        session_id = parent_span.attributes.get("session.id")
        if session_id:
            attributes["session.id"] = session_id
        return self.tracer.start_as_current_span(span_name, kind=span_kind, attributes=attributes)

    def trace_function(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            with self._trace_common(func.__name__, SpanKind.INTERNAL, {}) as span:
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise
        return wrapper

    def trace_tool_call(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            attributes = {
                "openinference.span.kind": "tool",
                "tool.name": func.__name__,
            }
            with self._trace_common(f"tool.{func.__name__}", SpanKind.CLIENT, attributes) as span:
                try:
                    result = func(*args, **kwargs)
                    span.set_status(StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    raise
        return wrapper


class EvaluationManager:
    def __init__(self, observer: GenAIObserver):
        self.observer = observer

    def log_evaluations(self, eval_name: str, eval_df: pd.DataFrame):
        try:
            px.Client(endpoint=self.observer.endpoint).log_evaluations(
                SpanEvaluations(eval_name=eval_name, dataframe=eval_df)
            )
            logger.info(f"📊 Successfully logged evaluations for '{eval_name}' to Phoenix.")
        except Exception as e:
            logger.error(f"Failed to log evaluations for '{eval_name}' to Phoenix: {e}")

    def create_sample_dataset(self) -> pd.DataFrame:
        """Creates a sample DataFrame using Phoenix-conventional column names."""
        data = {
            # RENAMED: 'prompt' -> 'input'
            'input': [ "What is the capital of France?", "Explain machine learning."],
            # 'response' is not needed here as the evaluator will generate it
            # RENAMED: 'ground_truth' -> 'reference'
            'reference': ["Paris is the capital of France.", "Machine learning is a subfield of AI where systems learn from data."]
        }
        return pd.DataFrame(data)

    def run_offline_experiment(
        self,
        dataset: pd.DataFrame,
        evaluators: List[Callable],
        eval_model: str,
        eval_name: str = "Offline Experiment",
    ) -> Dict[str, pd.DataFrame]:
        tracer = trace.get_tracer(__name__)
        with tracer.start_as_current_span(f"offline_experiment.{eval_name}") as span:
            span.set_attribute("experiment.name", eval_name)
            
            results = {}
            for evaluator_cls in evaluators:
                evaluator_name = evaluator_cls.__name__
                with tracer.start_as_current_span(f"eval.{evaluator_name}") as eval_span:
                    try:
                        # QAEvaluator and others from phoenix.evals are designed to
                        # run the model, generate spans, and return a compatible DataFrame.
                        evaluator_instance = evaluator_cls(model=eval_model)
                        eval_df = evaluator_instance.evaluate(dataset)
                        results[evaluator_name] = eval_df
                        # This log call will now work because eval_df contains the necessary
                        # 'context.span_id' and 'context.trace_id' columns.
                        self.log_evaluations(f"{eval_name} - {evaluator_name}", eval_df)
                        eval_span.set_status(StatusCode.OK)
                    except Exception as e:
                        eval_span.set_status(StatusCode.ERROR, description=str(e))
                        eval_span.record_exception(e)
                        logger.error(f"Failed to run offline eval {evaluator_name}: {e}")
            return results


-------------------
# requirements.txt
python-dotenv
openai
arize-phoenix
arize-phoenix-client
arize-phoenix-otel
arize-phoenix-evals
openinference-instrumentation-openai
opentelemetry-sdk
requests
pandas
retry
certifi
rouge-score
pydantic

# requirements.txt
# python-dotenv
# openai
# arize-phoenix[evals,otel,client]
# openinference-instrumentation-openai
# requests
# pandas
# retry
# certifi
# rouge-score
