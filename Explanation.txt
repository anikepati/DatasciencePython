Full Features Explanation
The enterprise_llm_observer package is designed as an enterprise-grade tool for observability in generative AI applications, supporting both Arize Phoenix (LLM-focused) and Arize AX (general ML). It uses a singleton pattern for efficiency in multi-request environments (e.g., FastAPI), Enums for type-safe provider selection, and integrates tracing, logging, and evaluations. For enterprise use, it prioritizes scalability (batch logging, sampling), security (Apigee OAuth with renewal/retries, PII redaction), and flexibility (flag for Phoenix/AX mode, JWT for OpenAI proxy).
Thinking deeply for enterprise level:

Scalability: Singleton avoids reinitialization per request; batching reduces API calls; sampling controls eval costs in high-volume systems (e.g., 100K+ requests/day).
Security/Compliance: PII redaction prevents sensitive data leaks; separate OAuth for Arize logging and JWT for OpenAI proxy; env var config for secure CI/CD.
Integration: FastAPI-compatible; works with Kubernetes (env vars); supports offline evals for batch analysis in data warehouses.
Performance: Async evals minimize latency (e.g., <50ms added for sampled evals); retries handle transient failures in distributed systems.
Monitoring: Structured logs for Splunk/ELK; Arize dashboards for trace/eval visualization; alerts on low eval scores.
Cost Optimization: Sampling (10% default) and batching save on eval API calls; hybrid online/offline evals balance real-time insights with batch savings.
Extensibility: Enum for providers; customizable evaluators; flag for Phoenix/AX to fit LLM vs. general ML workflows.
Best Practices: Use in FastAPI startup for initialization; shutdown in cleanup; monitor token renewal logs for Apigee health.

Full Package Code
setup.py
pythonfrom setuptools import setup, find_packages

setup(
    name="enterprise_llm_observer",
    version="0.3.4",  # Updated for Arize AX/Phoenix flag
    packages=find_packages(),
    install_requires=[
        "arize-phoenix>=4.0.0",
        "arize>=7.0.0",  # For Arize AX
        "arize-otel>=0.1.0",
        "openinference-instrumentation-openai>=0.1.3",
        "openinference-instrumentation-anthropic>=0.1.3",
        "openinference-instrumentation-vertexai>=0.1.4",
        "opentelemetry-exporter-otlp-proto-http>=1.22.0",
        "retry>=0.9.2",
        "requests>=2.28.0",
        "google-cloud-aiplatform>=1.38.0",
        "presidio-analyzer>=2.2.33",
        "pandas>=2.0.0",
    ],
    extras_require={
        "adk": ["google-adk"],
    },
    description="Enterprise-grade LLM/agent observability with tracing, spans, evaluations (hallucination, relevance, QA, toxicity), Apigee OAuth/JWT, Arize Phoenix/AX flag, Vertex AI/ADK support, and singleton",
    author="Your Name",
    author_email="your.email@example.com",
    url="https://your-repo-url",
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.8",
)
observer.py (Full Code)
pythonimport os
import logging
import time
import random
import asyncio
from enum import Enum
from typing import Any, Dict, Optional, List, Callable, AsyncGenerator
from functools import wraps
from retry import retry
import requests
import pandas as pd
import phoenix as px
from phoenix.evaluations import (
    HallucinationEvaluator,
    RelevanceEvaluator,
    QaEvaluator,
    ToxicityEvaluator,
)
from arize.utils.types import ModelTypes, Environments, Metrics
from arize.public_pb2 import Record
from arize.api import Client as ArizeClient
from arize.otel import register, Endpoints
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.resources import Resource
from openinference.instrumentation.openai import OpenAIInstrumentor
from openinference.instrumentation.anthropic import AnthropicInstrumentor
from openinference.instrumentation.vertexai import VertexAIInstrumentor
from presidio_analyzer import AnalyzerEngine
try:
    from google.adk.agents import Agent
except ImportError:
    Agent = None

# Structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
)
logger = logging.getLogger(__name__)

class LLMProvider(Enum):
    VERTEX_AI = "vertexai"
    OPENAI = "openai"
    GROK = "grok"
    ANTHROPIC = "anthropic"

class EnterpriseLLMObserver:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.__initialized = False
        return cls._instance

    def __init__(
        self,
        space_id: Optional[str] = None,
        api_key: Optional[str] = None,
        project_name: str = "enterprise-llm-observability",
        endpoint: Optional[str] = None,
        environment: str = "production",
        oauth_client_id: Optional[str] = None,
        oauth_client_secret: Optional[str] = None,
        oauth_token_url: Optional[str] = None,
        token_buffer_seconds: int = 60,
        default_sample_rate: float = 0.1,
        openai_proxy_url: Optional[str] = None,
        openai_jwt_token: Optional[str] = None,
        use_phoenix: bool = True,  # Flag for Phoenix (True) or Arize AX (False)
    ):
        if self.__initialized:
            logger.info("Observer instance already initialized; reusing.")
            return
        self.space_id = space_id or os.getenv("PHOENIX_SPACE_ID")
        self.api_key = api_key or os.getenv("PHOENIX_API_KEY")
        self.project_name = project_name
        self.endpoint = endpoint or Endpoints.ARIZE_OTLP_HTTP
        self.environment = environment
        self.oauth_client_id = oauth_client_id
        self.oauth_client_secret = oauth_client_secret
        self.oauth_token_url = oauth_token_url
        self.token_buffer_seconds = token_buffer_seconds
        self.default_sample_rate = default_sample_rate
        self.openai_proxy_url = openai_proxy_url or os.getenv("OPENAI_PROXY_URL")
        self.openai_jwt_token = openai_jwt_token or os.getenv("OPENAI_JWT_TOKEN")
        self.use_phoenix = use_phoenix
        self.access_token: Optional[str] = None
        self.token_expiration: float = 0.0
        self.tracer_provider: Optional[TracerProvider] = None
        self.session = None
        self.arize_client = None if use_phoenix else ArizeClient(api_key=self.api_key, space_id=self.space_id)
        self.batch_buffer: List[Dict[str, Any]] = []
        self.eval_buffer: List[pd.DataFrame] = []
        self.pii_analyzer = AnalyzerEngine()
        self._setup()
        self.__initialized = True

    def _setup(self):
        if not self.space_id or not self.api_key:
            raise ValueError("Space ID and API Key required.")

        if self.oauth_client_id and self.oauth_client_secret and self.oauth_token_url:
            self._fetch_oauth_token()

        os.environ["PHOENIX_SPACE_ID"] = self.space_id
        os.environ["PHOENIX_API_KEY"] = self.api_key

        if self.use_phoenix:
            try:
                self.session = px.launch_app(project_name=self.project_name)
                logger.info(f"Phoenix session initialized for {self.project_name}")
            except Exception as e:
                logger.error(f"Failed to initialize Phoenix: {str(e)}")
                raise
        else:
            logger.info("Using Arize AX mode for enterprise ML observability.")

        resource = Resource(attributes={"environment": self.environment, "service.name": "llm-observability"})
        self.tracer_provider = register(
            space_id=self.space_id,
            api_key=self.api_key,
            project_name=self.project_name,
            endpoint=self.endpoint,
            resource=resource,
        )
        trace.set_tracer_provider(self.tracer_provider)
        logger.info("OpenTelemetry initialized.")

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def _fetch_oauth_token(self):
        try:
            response = requests.post(
                self.oauth_token_url,
                data={
                    "grant_type": "client_credentials",
                    "client_id": self.oauth_client_id,
                    "client_secret": self.oauth_client_secret,
                },
            )
            response.raise_for_status()
            data = response.json()
            self.access_token = data["access_token"]
            expires_in = data.get("expires_in", 3600)
            self.token_expiration = time.time() + expires_in - self.token_buffer_seconds
            os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Bearer {self.access_token}"
            logger.info("OAuth token fetched/renewed.")
        except Exception as e:
            logger.error(f"Failed to fetch OAuth token: {str(e)}")
            raise

    def _ensure_token_valid(self):
        if self.oauth_token_url and time.time() >= self.token_expiration:
            logger.info("OAuth token near expiration; renewing.")
            self._fetch_oauth_token()

    def _anonymize_data(self, text: str) -> str:
        try:
            result = self.pii_analyzer.analyze(text=text, language="en")
            return result.redact()
        except Exception as e:
            logger.error(f"Failed to anonymize data: {str(e)}")
            return text

    def instrument_llm(self, provider: LLMProvider):
        self._ensure_token_valid()
        try:
            if provider in [LLMProvider.OPENAI, LLMProvider.GROK]:
                if self.openai_proxy_url and self.openai_jwt_token:
                    config = {
                        "base_url": self.openai_proxy_url,
                        "extra_headers": {"Authorization": f"Bearer {self.openai_jwt_token}"},
                    }
                    OpenAIInstrumentor().instrument(tracer_provider=self.tracer_provider, client_config=config)
                else:
                    OpenAIInstrumentor().instrument(tracer_provider=self.tracer_provider)
                logger.info(f"{provider.name} instrumented with proxy: {bool(self.openai_proxy_url)}")
            elif provider == LLMProvider.ANTHROPIC:
                AnthropicInstrumentor().instrument(tracer_provider=self.tracer_provider)
                logger.info("Anthropic instrumented.")
            elif provider == LLMProvider.VERTEX_AI:
                VertexAIInstrumentor().instrument(tracer_provider=self.tracer_provider)
                logger.info("Vertex AI (Gemini) instrumented.")
        except Exception as e:
            logger.error(f"Failed to instrument {provider.name}: {str(e)}")
            raise

    def instrument_adk(self):
        self._ensure_token_valid()
        if not Agent:
            raise ImportError("Google ADK not installed.")
        original_run_async = Agent.run_async

        @wraps(original_run_async)
        async def traced_run_async(self, context, *args, **kwargs) -> AsyncGenerator[Any, None]:
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span(f"adk_{self.name}_run_async") as span:
                span.set_attribute("agent_name", self.name)
                span.set_attribute("agent_model", self.model)
                span.set_attribute("agent_description", self.description)
                start_time = time.time()
                try:
                    async for event in original_run_async(self, context, *args, **kwargs):
                        span.set_attribute("event_type", type(event).__name__)
                        yield event
                    span.set_attribute("execution_time_ms", (time.time() - start_time) * 1000)
                    span.set_status(trace.StatusCode.OK)
                except Exception as e:
                    span.set_status(trace.StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    logger.error(f"Error in ADK agent {self.name}: {str(e)}")
                    raise

        Agent.run_async = traced_run_async
        logger.info("Google ADK instrumented.")

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def log_interaction(
        self,
        prompt: str,
        response: str,
        model: str,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        self._ensure_token_valid()
        anonymized_prompt = self._anonymize_data(prompt)
        anonymized_response = self._anonymize_data(response)
        if self.use_phoenix:
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span("llm_interaction") as span:
                span.set_attribute("input", anonymized_prompt)
                span.set_attribute("output", anonymized_response)
                span.set_attribute("model", model)
                for key, value in (metadata or {}).items():
                    span.set_attribute(key, str(value))
                logger.info(f"Logged interaction for {model} in Phoenix mode.")
        else:
            # Arize AX mode
            record = Record(
                prediction_id=str(time.time()),
                prediction_label=anonymized_response,
                features={"model": model, "prompt": anonymized_prompt},
                tags=metadata or {},
            )
            self.arize_client.log(record=record, model_id="my-model", model_type=ModelTypes.GENERATIVE_LLM, environment=Environments.PRODUCTION, metric=Metrics.GENERATIVE_LLM)
            logger.info(f"Logged interaction for {model} in Arize AX mode.")

    def log_batch(self, interactions: List[Dict[str, Any]]):
        self._ensure_token_valid()
        self.batch_buffer.extend(interactions)
        if len(self.batch_buffer) >= 100:
            self._flush_batch()
        logger.debug(f"Added {len(interactions)} to batch (total: {len(self.batch_buffer)}).")

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def _flush_batch(self):
        self._ensure_token_valid()
        if not self.batch_buffer:
            return
        try:
            for interaction in self.batch_buffer:
                self.log_interaction(
                    prompt=interaction["prompt"],
                    response=interaction["response"],
                    model=interaction["model"],
                    metadata=interaction.get("metadata"),
                )
            logger.info(f"Flushed {len(self.batch_buffer)} interactions.")
            self.batch_buffer.clear()
        except Exception as e:
            logger.error(f"Failed to flush batch: {str(e)}")
            raise

    async def evaluate_and_log_async(
        self,
        trace_data: Dict[str, Any],
        evaluators: List[Callable],
        sample_rate: Optional[float] = None,
        eval_model: str = "gemini-1.5-pro",
    ):
        self._ensure_token_valid()
        sample_rate = sample_rate or self.default_sample_rate
        if random.random() > sample_rate:
            logger.debug("Skipped evaluation due to sampling.")
            return

        trace_data["prompt"] = self._anonymize_data(trace_data.get("prompt", ""))
        trace_data["response"] = self._anonymize_data(trace_data.get("response", ""))

        df = pd.DataFrame([trace_data])
        results = []
        for evaluator_cls in evaluators:
            evaluator = evaluator_cls(model=eval_model)
            try:
                result = await asyncio.to_thread(evaluator.evaluate, df)
                results.append((result, evaluator_cls.__name__))
            except Exception as e:
                logger.error(f"Failed evaluation with {evaluator_cls.__name__}: {str(e)}")

        self.eval_buffer.extend(results)
        if len(self.eval_buffer) >= 10:
            self._flush_eval_buffer()

    @retry(tries=3, delay=1, backoff=2, logger=logger)
    def _flush_eval_buffer(self):
        self._ensure_token_valid()
        if not self.eval_buffer:
            return
        try:
            for result, eval_name in self.eval_buffer:
                if self.use_phoenix:
                    px.log_evaluations(result, eval_name=eval_name)
                else:
                    for index, row in result.iterrows():
                        record = Record(
                            prediction_id=str(time.time()),
                            prediction_label=row.get("output", ""),
                            features={"eval_name": eval_name},
                            tags={"eval_score": row.get("score", 0.0), "eval_label": row.get("label", "unknown")},
                        )
                        self.arize_client.log(record=record, model_id="my-model", model_type=ModelTypes.GENERATIVE_LLM, environment=Environments.PRODUCTION, metric=Metrics.GENERATIVE_LLM)
            logger.info(f"Flushed {len(self.eval_buffer)} evaluation results in {'Phoenix' if self.use_phoenix else 'Arize AX'} mode.")
            self.eval_buffer.clear()
        except Exception as e:
            logger.error(f"Failed to flush eval buffer: {str(e)}")
            raise

    def trace_function(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            self._ensure_token_valid()
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span(func.__name__) as span:
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("execution_time_ms", (time.time() - start_time) * 1000)
                    span.set_status(trace.StatusCode.OK)
                    return result
                except Exception as e:
                    span.set_status(trace.StatusCode.ERROR, description=str(e))
                    span.record_exception(e)
                    logger.error(f"Error in {func.__name__}: {str(e)}")
                    raise
        return wrapper

    def shutdown(self):
        try:
            if self.batch_buffer:
                self._flush_batch()
            if self.eval_buffer:
                self._flush_eval_buffer()
            if self.tracer_provider:
                self.tracer_provider.shutdown()
            if self.session:
                self.session.close()
            logger.info("Shutdown completed.")
        except Exception as e:
            logger.error(f"Shutdown error: {str(e)}")
            raise
Usage Examples
1. FastAPI with Phoenix Mode (Grok via OpenAI Proxy)
pythonfrom fastapi import FastAPI, Depends
from enterprise_llm_observer import EnterpriseLLMObserver, LLMProvider
from openai import OpenAI
from phoenix.evaluations import HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator

app = FastAPI()

observer = EnterpriseLLMObserver(
    space_id="your-space-id",
    api_key="your-api-key",
    endpoint="https://your-apigee-proxy/arize-otlp",
    oauth_client_id="your-client-id",
    oauth_client_secret="your-client-secret",
    oauth_token_url="https://your-oauth-token-url/oauth/token",
    openai_proxy_url="https://your-apigee-proxy/openai",
    openai_jwt_token="your-jwt-token",
    default_sample_rate=0.1,
    use_phoenix=True,
)

observer.instrument_llm(LLMProvider.GROK)

def get_observer():
    return observer

@app.post("/query")
async def query_endpoint(observer: EnterpriseLLMObserver = Depends(get_observer)):
    client = OpenAI(
        base_url=observer.openai_proxy_url or "https://api.x.ai/v1",
        api_key="your-xai-api-key",
    )
    response = client.chat.completions.create(
        model="grok-4",
        messages=[{"role": "user", "content": "Explain AI."}],
    )

    observer.log_interaction(
        prompt="Explain AI.",
        response=response.choices[0].message.content,
        model="grok-4",
    )

    trace_data = {"prompt": "Explain AI.", "response": response.choices[0].message.content, "model": "grok-4"}
    evaluators = [HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator]
    await observer.evaluate_and_log_async(trace_data, evaluators, eval_model="gemini-1.5-pro")
    return {"response": response.choices[0].message.content}
2. Arize AX Mode with Vertex AI
pythonfrom enterprise_llm_observer import EnterpriseLLMObserver, LLMProvider
import vertexai
from vertexai.generative_models import GenerativeModel
from phoenix.evaluations import HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator

observer = EnterpriseLLMObserver(
    space_id="your-space-id",
    api_key="your-api-key",
    endpoint="https://your-apigee-proxy/arize-otlp",
    oauth_client_id="your-client-id",
    oauth_client_secret="your-client-secret",
    oauth_token_url="https://your-oauth-token-url/oauth/token",
    default_sample_rate=0.1,
    use_phoenix=False,  # Arize AX mode
)

observer.instrument_llm(LLMProvider.VERTEX_AI)

vertexai.init(project="your-gcp-project", location="us-central1")
model = GenerativeModel("gemini-1.5-pro")
response = model.generate_content("Explain AI.")

observer.log_interaction(
    prompt="Explain AI.",
    response=response.text,
    model="gemini-1.5-pro",
)

trace_data = {"prompt": "Explain AI.", "response": response.text, "model": "gemini-1.5-pro"}
evaluators = [HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator]
asyncio.run(observer.evaluate_and_log_async(trace_data, evaluators, eval_model="gemini-1.5-pro"))

observer.shutdown()
3. Google ADK in Phoenix Mode
pythonfrom enterprise_llm_observer import EnterpriseLLMObserver, LLMProvider
from google.adk.agents import Agent
from phoenix.evaluations import HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator

observer = EnterpriseLLMObserver(
    space_id="your-space-id",
    api_key="your-api-key",
    endpoint="https://your-apigee-proxy/arize-otlp",
    oauth_client_id="your-client-id",
    oauth_client_secret="your-client-secret",
    oauth_token_url="https://your-oauth-token-url/oauth/token",
    default_sample_rate=0.1,
    use_phoenix=True,
)

observer.instrument_adk()

agent = Agent(
    name="test_agent",
    model="gemini-1.5-pro",
    description="Test agent",
)

import asyncio
async def run_agent():
    context = {}
    async for event in agent.run_async(context):
        print(event)

asyncio.run(run_agent())

observer.log_interaction(
    prompt="Agent prompt",
    response="Agent response",
    model="gemini-1.5-pro",
)

trace_data = {"prompt": "Agent prompt", "response": "Agent response", "model": "gemini-1.5-pro"}
evaluators = [HallucinationEvaluator, RelevanceEvaluator, QaEvaluator, ToxicityEvaluator]
asyncio.run(observer.evaluate_and_log_async(trace_data, evaluators, eval_model="gemini-1.5-pro"))

observer.shutdown()
4. Offline Evaluations in AX Mode
python# Assuming traces are exported or collected in a DF
df = pd.DataFrame([{"prompt": "Query", "response": "Response", "model": "gemini-1.5-pro"}])
evaluator = HallucinationEvaluator(model="gemini-1.5-pro")
results = evaluator.evaluate(df)

# In AX mode, log as tags (as in _flush_eval_buffer)
